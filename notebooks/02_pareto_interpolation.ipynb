{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicola-Ibrahim/Pareto-Optimization/blob/main/notebooks/02_pareto_interpolation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder Formulation for Pareto Front Analysis\n"
      ],
      "metadata": {
        "id": "iojL3z_52E-y"
      },
      "id": "iojL3z_52E-y"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "nqxWtmAw9qmU"
      },
      "id": "nqxWtmAw9qmU",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data"
      ],
      "metadata": {
        "id": "Hr8T5ZsM-nCz"
      },
      "id": "Hr8T5ZsM-nCz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Read npz\n",
        "pareto_front = np.load('./pareto_front.npz')['pareto_front']\n",
        "print(pareto_front.shape)"
      ],
      "metadata": {
        "id": "qw-bLw06-nVa",
        "outputId": "55a0f5f9-e52e-4d7f-b34b-c06868c88a9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "id": "qw-bLw06-nVa",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './pareto_front.npz'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-50aca6db4376>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read npz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpareto_front\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./pareto_front.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pareto_front'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpareto_front\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './pareto_front.npz'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Data Preparation\n",
        "**Input Data Structure**:\n",
        "- Let $X = \\{\\mathbf{x}_i\\}_{i=1}^N \\subset \\mathbb{R}^2$ be the set of Pareto-optimal solutions\n",
        "- Each solution $\\mathbf{x}_i = (f_1^{(i)}, f_2^{(i)})$ represents a trade-off between:\n",
        "  - $f_1$: Travel time (minutes)\n",
        "  - $f_2$: Energy consumption (kWh)\n",
        "\n",
        "\n",
        "**Normalization**: Scale objectives to $[0,1]$ range for training stability:\n",
        "$$\n",
        "\\hat{f}_1 = \\frac{f_1 - f_{1}^{min}}{f_{1}^{max} - f_{1}^{min}}, \\quad\n",
        "\\hat{f}_2 = \\frac{f_2 - f_{2}^{min}}{f_{2}^{max} - f_{2}^{min}}\n",
        "$$\n",
        "\n",
        "**Standardization**:\n",
        "$$\n",
        "\\hat{f}_k^{(i)} = \\frac{f_k^{(i)} - \\mu_k}{\\sigma_k} \\quad \\text{for } k=1,2\n",
        "$$\n",
        "where $\\mu_k$, $\\sigma_k$ are the mean and standard deviation of each objective.\n"
      ],
      "metadata": {
        "id": "T7CaJUAe4drS"
      },
      "id": "T7CaJUAe4drS"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bc9ba7a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "bc9ba7a8",
        "outputId": "e16e9a8e-5e90-4c9e-de61-1a09640f8c4d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pareto_front' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-3c0a5ddb2363>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Normalize to [0, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpareto_front\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Split data (80% train, 20% validation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pareto_front' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Normalize to [0, 1]\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = scaler.fit_transform(pareto_front)\n",
        "\n",
        "# Split data (80% train, 20% validation)\n",
        "X_train, X_val = train_test_split(X_normalized, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Architecture Specification\n",
        "\n",
        "### Encoder Network (Compression)\n",
        "$$\n",
        "\\mathbf{z} = g_\\phi(\\mathbf{x}) = \\text{LeakyReLU}(\\mathbf{W}_2 \\cdot \\text{ELU}(\\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1) + \\mathbf{b}_2)\n",
        "$$\n",
        "\n",
        "### Decoder Network (Reconstruction)\n",
        "$$\n",
        "\\hat{\\mathbf{x}} = f_\\theta(\\mathbf{z}) = \\text{Sigmoid}(\\mathbf{W}_4 \\cdot \\text{ELU}(\\mathbf{W}_3\\mathbf{z} + \\mathbf{b}_3) + \\mathbf{b}_4)\n",
        "$$\n",
        "\n",
        "**Dimensionality**:\n",
        "- Input/Output: $\\mathbb{R}^2$ (normalized objectives)\n",
        "- Latent space: $\\mathbb{R}^1$ (bottleneck)\n",
        "- Hidden layers: 32 neurons with ELU activation\n",
        "\n",
        "\n",
        "## Architecture\n",
        "\n",
        "### Encoder\n",
        "Maps 2D Pareto solutions to a 1D latent space:\n",
        "$$\n",
        "\\mathbf{z} = \\text{Encoder}(\\mathbf{x}) = \\sigma(\\mathbf{W}_2 \\cdot \\text{ReLU}(\\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1) + \\mathbf{b}_2)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\mathbf{W}_1 \\in \\mathbb{R}^{h \\times 2}$, $\\mathbf{W}_2 \\in \\mathbb{R}^{1 \\times h}$ are weight matrices\n",
        "- $\\mathbf{b}_1 \\in \\mathbb{R}^h$, $\\mathbf{b}_2 \\in \\mathbb{R}^1$ are bias terms\n",
        "- $h$ is hidden layer size\n",
        "- $\\sigma$ is sigmoid activation\n",
        "\n",
        "### Decoder\n",
        "Reconstructs solutions from latent space:\n",
        "$$\n",
        "\\hat{\\mathbf{x}} = \\text{Decoder}(\\mathbf{z}) = \\sigma(\\mathbf{W}_4 \\cdot \\text{ReLU}(\\mathbf{W}_3\\mathbf{z} + \\mathbf{b}_3) + \\mathbf{b}_4)\n",
        "$$\n",
        "\n",
        "With:\n",
        "- $\\mathbf{W}_3 \\in \\mathbb{R}^{h \\times 1}$, $\\mathbf{W}_4 \\in \\mathbb{R}^{2 \\times h}$\n",
        "- $\\mathbf{b}_3 \\in \\mathbb{R}^h$, $\\mathbf{b}_4 \\in \\mathbb{R}^2$\n",
        "\n",
        "### Loss Function\n",
        "Mean Squared Error (MSE) reconstruction loss:\n",
        "$$\n",
        "\\mathcal{L}_{recon} = \\frac{1}{N}\\sum_{i=1}^N \\|\\mathbf{x}_i - \\hat{\\mathbf{x}}_i\\|^2_2\n",
        "$$"
      ],
      "metadata": {
        "id": "HGrqXafj-W1n"
      },
      "id": "HGrqXafj-W1n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1431b8e8",
      "metadata": {
        "id": "1431b8e8"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.norm = LayerNormalization()\n",
        "        self.dense = Dense(embed_dim, activation='gelu')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn_output = self.attention(inputs, inputs)\n",
        "        x = self.norm(inputs + attn_output)\n",
        "        return self.dense(x)\n",
        "\n",
        "class TransformerDecoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.norm = LayerNormalization()\n",
        "        self.dense = Dense(embed_dim, activation='gelu')\n",
        "\n",
        "    def call(self, inputs, context):\n",
        "        attn_output = self.attention(inputs, context)\n",
        "        x = self.norm(inputs + attn_output)\n",
        "        return self.dense(x)\n",
        "\n",
        "\n",
        "# Transformer Autoencoder\n",
        "input_dim = 2\n",
        "latent_dim = 1\n",
        "embed_dim = 32\n",
        "num_heads = 2\n",
        "\n",
        "# Encoder\n",
        "inputs = Input(shape=(input_dim,))\n",
        "x = Dense(embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, num_heads)(x)\n",
        "latent = Dense(latent_dim, name='latent')(x)\n",
        "\n",
        "# Decoder\n",
        "decoder_input = Input(shape=(latent_dim,))\n",
        "x = Dense(embed_dim)(decoder_input)\n",
        "x = TransformerDecoder(embed_dim, num_heads)(x, x)\n",
        "outputs = Dense(input_dim, activation='sigmoid')(x)\n",
        "\n",
        "# Models\n",
        "encoder = Model(inputs, latent)\n",
        "decoder = Model(decoder_input, outputs)\n",
        "autoencoder = Model(inputs, decoder(encoder(inputs)))\n",
        "\n",
        "# Custom loss with reconstruction and latent regularization\n",
        "def total_loss(y_true, y_pred):\n",
        "    recon_loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
        "    latent_reg = tf.reduce_mean(tf.square(latent))\n",
        "    return recon_loss + 0.1 * latent_reg\n",
        "\n",
        "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=total_loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "history = autoencoder.fit(\n",
        "    X_train, X_train,\n",
        "    epochs=500,\n",
        "    batch_size=16,\n",
        "    validation_data=(X_val, X_val),\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "KG2S6vOU98qu"
      },
      "id": "KG2S6vOU98qu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpolation in Latent Space\n",
        "\n",
        "### Linear Interpolation\n",
        "For solutions $\\mathbf{x}_A$, $\\mathbf{x}_B$:\n",
        "1. Encode two solutions:\n",
        "   $$\n",
        "   \\mathbf{z}_A = \\text{Encoder}(\\mathbf{x}_A), \\quad \\mathbf{z}_B = \\text{Encoder}(\\mathbf{x}_B)\n",
        "   $$\n",
        "\n",
        "2. Linear interpolation:\n",
        "   $$\n",
        "   \\mathbf{z}_{new} = \\alpha\\mathbf{z}_A + (1-\\alpha)\\mathbf{z}_B, \\quad \\alpha \\in [0,1]\n",
        "   $$\n",
        "\n",
        "3. Decode to generate new solution:\n",
        "   $$\n",
        "   \\mathbf{x}_{new} = \\text{Decoder}(\\mathbf{z}_{new})\n",
        "   $$\n",
        "\n",
        "\n",
        "### Geodesic Interpolation\n",
        "For solutions $\\mathbf{x}_A$, $\\mathbf{x}_B$:\n",
        "1. Encode: $\\mathbf{z}_A = g_\\phi(\\mathbf{x}_A)$, $\\mathbf{z}_B = g_\\phi(\\mathbf{x}_B)$\n",
        "2. Spherical interpolation:\n",
        "   $$\n",
        "   \\mathbf{z}_{\\text{new}} = \\frac{\\sin[(1-\\alpha)\\Omega]}{\\sin\\Omega}\\mathbf{z}_A + \\frac{\\sin[\\alpha\\Omega]}{\\sin\\Omega}\\mathbf{z}_B\n",
        "   $$\n",
        "   where $\\Omega = \\arccos(\\mathbf{z}_A^\\top \\mathbf{z}_B)$\n",
        "\n",
        "3. Decode: $\\mathbf{x}_{\\text{new}} = f_\\theta(\\mathbf{z}_{\\text{new}})$\n"
      ],
      "metadata": {
        "id": "A8km8d2E-ANE"
      },
      "id": "A8km8d2E-ANE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Interpolation with geodesic sampling\n",
        "def geodesic_interpolate(z1, z2, alpha):\n",
        "    omega = np.arccos(np.clip(np.dot(z1.T, z2)[0,0], -1, 1))\n",
        "    return (np.sin((1-alpha)*omega)/np.sin(omega))*z1 + (np.sin(alpha*omega)/np.sin(omega))*z2\n",
        "\n",
        "z_A = encoder.predict(X_train[0:1])\n",
        "z_B = encoder.predict(X_train[1:2])\n",
        "\n",
        "for alpha in np.linspace(0, 1, 5):\n",
        "    z_new = geodesic_interpolate(z_A, z_B, alpha)\n",
        "    J_new = decoder.predict(z_new)\n",
        "    J_original = scaler.inverse_transform(J_new)\n",
        "    print(f\"Î±={alpha:.1f}: Time={J_original[0,0]:.2f}s, Energy={J_original[0,1]:.2f}kWh\")\n"
      ],
      "metadata": {
        "id": "jntdH5Hj-CLo"
      },
      "id": "jntdH5Hj-CLo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b16d24cc",
      "metadata": {
        "id": "b16d24cc"
      },
      "outputs": [],
      "source": [
        "# Interpolation with linear sampling\n",
        "\n",
        "# Encode two Pareto solutions\n",
        "z_A = encoder.predict(X_train[0:1])  # Solution A\n",
        "z_B = encoder.predict(X_train[1:2])  # Solution B\n",
        "\n",
        "# Linear interpolation\n",
        "alpha = 0.5\n",
        "z_new = alpha * z_A + (1 - alpha) * z_B\n",
        "\n",
        "# Decode to generate new solution\n",
        "J_new = decoder.predict(z_new)\n",
        "\n",
        "# Denormalize\n",
        "J_new_original = scaler.inverse_transform(J_new)\n",
        "print(f\"Interpolated Solution: Time = {J_new_original[0, 0]:.2f} s, Energy = {J_new_original[0, 1]:.2f} kWh\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution Validation Protocol\n",
        "\n",
        "### Dominance Verification\n",
        "$$\n",
        "\\mathbf{x}_{\\text{new}} \\text{ is non-dominated iff } \\nexists \\mathbf{x}_i \\in X :\n",
        "\\begin{cases}\n",
        "f_1^{(i)} \\leq f_1^{\\text{(new)}} \\\\\n",
        "f_2^{(i)} \\leq f_2^{\\text{(new)}} \\\\\n",
        "\\|\\mathbf{x}_i - \\mathbf{x}_{\\text{new}}\\|_2 > \\delta\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "### Feasibility Check\n",
        "$$\n",
        "\\mathbf{x}_{\\text{new}} \\in \\mathcal{F} \\iff\n",
        "\\begin{cases}\n",
        "g_1(\\mathbf{x}_{\\text{new}}) \\leq 0 \\\\\n",
        "g_2(\\mathbf{x}_{\\text{new}}) \\leq 0 \\\\\n",
        "\\vdots \\\\\n",
        "g_k(\\mathbf{x}_{\\text{new}}) \\leq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Ensure:\n",
        "$$\n",
        "f_1^{new} \\geq 0, \\quad f_2^{new} \\geq 0\n",
        "$$\n",
        "And any problem-specific constraints (e.g., $g(\\mathbf{x}_{new}) \\leq 0$)\n",
        "\n",
        "\n",
        "## Implementation Notes\n",
        "\n",
        "1. **Normalization**: Essential for stable training\n",
        "2. **Bottleneck Size**: 1D latent space enables linear interpolation\n",
        "3. **Activation**: Sigmoid ensures outputs stay in normalized $[0,1]$ range\n",
        "4. **Regularization**: Consider adding KL divergence for variational AE"
      ],
      "metadata": {
        "id": "GaxxlDqA-d_q"
      },
      "id": "GaxxlDqA-d_q"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2qlcpfob9yai"
      },
      "id": "2qlcpfob9yai",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}