{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicola-Ibrahim/Pareto-Optimization/blob/main/notebooks/02_pareto_interpolation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder Formulation for Pareto Front Analysis\n"
      ],
      "metadata": {
        "id": "iojL3z_52E-y"
      },
      "id": "iojL3z_52E-y"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LayerNormalization, MultiHeadAttention\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "nqxWtmAw9qmU"
      },
      "id": "nqxWtmAw9qmU",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data"
      ],
      "metadata": {
        "id": "Hr8T5ZsM-nCz"
      },
      "id": "Hr8T5ZsM-nCz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Read npz\n",
        "pareto_front = np.load('./pareto_data.npz')['X']\n",
        "print(pareto_front.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw-bLw06-nVa",
        "outputId": "a6d21bbd-03dc-4e67-e64a-0ac399645a25"
      },
      "id": "qw-bLw06-nVa",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Data Preparation\n",
        "**Input Data Structure**:\n",
        "- Let $X = \\{\\mathbf{x}_i\\}_{i=1}^N \\subset \\mathbb{R}^2$ be the set of Pareto-optimal solutions\n",
        "- Each solution $\\mathbf{x}_i = (f_1^{(i)}, f_2^{(i)})$ represents a trade-off between:\n",
        "  - $f_1$: Travel time (minutes)\n",
        "  - $f_2$: Energy consumption (kWh)\n",
        "\n",
        "\n",
        "**Normalization**: Scale objectives to $[0,1]$ range for training stability:\n",
        "$$\n",
        "\\hat{f}_1 = \\frac{f_1 - f_{1}^{min}}{f_{1}^{max} - f_{1}^{min}}, \\quad\n",
        "\\hat{f}_2 = \\frac{f_2 - f_{2}^{min}}{f_{2}^{max} - f_{2}^{min}}\n",
        "$$\n",
        "\n",
        "**Standardization**:\n",
        "$$\n",
        "\\hat{f}_k^{(i)} = \\frac{f_k^{(i)} - \\mu_k}{\\sigma_k} \\quad \\text{for } k=1,2\n",
        "$$\n",
        "where $\\mu_k$, $\\sigma_k$ are the mean and standard deviation of each objective.\n"
      ],
      "metadata": {
        "id": "T7CaJUAe4drS"
      },
      "id": "T7CaJUAe4drS"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "bc9ba7a8",
      "metadata": {
        "id": "bc9ba7a8"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Normalize to [0, 1]\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = scaler.fit_transform(pareto_front)\n",
        "\n",
        "# Split data (80% train, 20% validation)\n",
        "X_train, X_val = train_test_split(X_normalized, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Architecture Specification\n",
        "\n",
        "### Encoder Network (Compression)\n",
        "$$\n",
        "\\mathbf{z} = g_\\phi(\\mathbf{x}) = \\text{LeakyReLU}(\\mathbf{W}_2 \\cdot \\text{ELU}(\\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1) + \\mathbf{b}_2)\n",
        "$$\n",
        "\n",
        "### Decoder Network (Reconstruction)\n",
        "$$\n",
        "\\hat{\\mathbf{x}} = f_\\theta(\\mathbf{z}) = \\text{Sigmoid}(\\mathbf{W}_4 \\cdot \\text{ELU}(\\mathbf{W}_3\\mathbf{z} + \\mathbf{b}_3) + \\mathbf{b}_4)\n",
        "$$\n",
        "\n",
        "**Dimensionality**:\n",
        "- Input/Output: $\\mathbb{R}^2$ (normalized objectives)\n",
        "- Latent space: $\\mathbb{R}^1$ (bottleneck)\n",
        "- Hidden layers: 32 neurons with ELU activation\n",
        "\n",
        "\n",
        "## Architecture\n",
        "\n",
        "### Encoder\n",
        "Maps 2D Pareto solutions to a 1D latent space:\n",
        "$$\n",
        "\\mathbf{z} = \\text{Encoder}(\\mathbf{x}) = \\sigma(\\mathbf{W}_2 \\cdot \\text{ReLU}(\\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1) + \\mathbf{b}_2)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\mathbf{W}_1 \\in \\mathbb{R}^{h \\times 2}$, $\\mathbf{W}_2 \\in \\mathbb{R}^{1 \\times h}$ are weight matrices\n",
        "- $\\mathbf{b}_1 \\in \\mathbb{R}^h$, $\\mathbf{b}_2 \\in \\mathbb{R}^1$ are bias terms\n",
        "- $h$ is hidden layer size\n",
        "- $\\sigma$ is sigmoid activation\n",
        "\n",
        "### Decoder\n",
        "Reconstructs solutions from latent space:\n",
        "$$\n",
        "\\hat{\\mathbf{x}} = \\text{Decoder}(\\mathbf{z}) = \\sigma(\\mathbf{W}_4 \\cdot \\text{ReLU}(\\mathbf{W}_3\\mathbf{z} + \\mathbf{b}_3) + \\mathbf{b}_4)\n",
        "$$\n",
        "\n",
        "With:\n",
        "- $\\mathbf{W}_3 \\in \\mathbb{R}^{h \\times 1}$, $\\mathbf{W}_4 \\in \\mathbb{R}^{2 \\times h}$\n",
        "- $\\mathbf{b}_3 \\in \\mathbb{R}^h$, $\\mathbf{b}_4 \\in \\mathbb{R}^2$\n",
        "\n",
        "### Loss Function\n",
        "Mean Squared Error (MSE) reconstruction loss:\n",
        "$$\n",
        "\\mathcal{L}_{recon} = \\frac{1}{N}\\sum_{i=1}^N \\|\\mathbf{x}_i - \\hat{\\mathbf{x}}_i\\|^2_2\n",
        "$$"
      ],
      "metadata": {
        "id": "HGrqXafj-W1n"
      },
      "id": "HGrqXafj-W1n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1431b8e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "id": "1431b8e8",
        "outputId": "0b2ad478-aeaf-47db-a0b2-37e28f56c900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:1387: UserWarning: Layer 'transformer_encoder' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
            "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
            "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
            "Exception encountered: ''Exception encountered when calling Softmax.call().\n",
            "\n",
            "\u001b[1mtuple index out of range\u001b[0m\n",
            "\n",
            "Arguments received by Softmax.call():\n",
            "  • inputs=tf.Tensor(shape=(None, 2), dtype=float32)\n",
            "  • mask=None''\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'transformer_encoder', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "Exception encountered when calling TransformerEncoder.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'transformer_encoder' (of type TransformerEncoder). Either the `TransformerEncoder.call()` method is incorrect, or you need to implement the `TransformerEncoder.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nException encountered when calling Softmax.call().\n\n\u001b[1mtuple index out of range\u001b[0m\n\nArguments received by Softmax.call():\n  • inputs=tf.Tensor(shape=(None, 2), dtype=float32)\n  • mask=None\u001b[0m\n\nArguments received by TransformerEncoder.call():\n  • args=('<KerasTensor shape=(None, 32), dtype=float32, sparse=False, name=keras_tensor_1>',)\n  • kwargs=<class 'inspect._empty'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-1d471b47dfc0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mlatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-1d471b47dfc0>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Exception encountered when calling TransformerEncoder.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'transformer_encoder' (of type TransformerEncoder). Either the `TransformerEncoder.call()` method is incorrect, or you need to implement the `TransformerEncoder.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nException encountered when calling Softmax.call().\n\n\u001b[1mtuple index out of range\u001b[0m\n\nArguments received by Softmax.call():\n  • inputs=tf.Tensor(shape=(None, 2), dtype=float32)\n  • mask=None\u001b[0m\n\nArguments received by TransformerEncoder.call():\n  • args=('<KerasTensor shape=(None, 32), dtype=float32, sparse=False, name=keras_tensor_1>',)\n  • kwargs=<class 'inspect._empty'>"
          ]
        }
      ],
      "source": [
        "class TransformerEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.norm = LayerNormalization()\n",
        "        self.dense = Dense(embed_dim, activation='gelu')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn_output = self.attention(inputs, inputs)\n",
        "        x = self.norm(inputs + attn_output)\n",
        "        return self.dense(x)\n",
        "\n",
        "class TransformerDecoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.norm = LayerNormalization()\n",
        "        self.dense = Dense(embed_dim, activation='gelu')\n",
        "\n",
        "    def call(self, inputs, context):\n",
        "        attn_output = self.attention(inputs, context)\n",
        "        x = self.norm(inputs + attn_output)\n",
        "        return self.dense(x)\n",
        "\n",
        "\n",
        "# Transformer Autoencoder\n",
        "input_dim = 2\n",
        "latent_dim = 1\n",
        "embed_dim = 32\n",
        "num_heads = 2\n",
        "\n",
        "# Encoder\n",
        "inputs = Input(shape=(input_dim,))\n",
        "x = Dense(embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, num_heads)(x)\n",
        "latent = Dense(latent_dim, name='latent')(x)\n",
        "\n",
        "# Decoder\n",
        "decoder_input = Input(shape=(latent_dim,))\n",
        "x = Dense(embed_dim)(decoder_input)\n",
        "x = TransformerDecoder(embed_dim, num_heads)(x, x)\n",
        "outputs = Dense(input_dim, activation='sigmoid')(x)\n",
        "\n",
        "# Models\n",
        "encoder = Model(inputs, latent)\n",
        "decoder = Model(decoder_input, outputs)\n",
        "autoencoder = Model(inputs, decoder(encoder(inputs)))\n",
        "\n",
        "# Custom loss with reconstruction and latent regularization\n",
        "def total_loss(y_true, y_pred):\n",
        "    recon_loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
        "    latent_reg = tf.reduce_mean(tf.square(latent))\n",
        "    return recon_loss + 0.1 * latent_reg\n",
        "\n",
        "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=total_loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "history = autoencoder.fit(\n",
        "    X_train, X_train,\n",
        "    epochs=500,\n",
        "    batch_size=16,\n",
        "    validation_data=(X_val, X_val),\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "KG2S6vOU98qu"
      },
      "id": "KG2S6vOU98qu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpolation in Latent Space\n",
        "\n",
        "### Linear Interpolation\n",
        "For solutions $\\mathbf{x}_A$, $\\mathbf{x}_B$:\n",
        "1. Encode two solutions:\n",
        "   $$\n",
        "   \\mathbf{z}_A = \\text{Encoder}(\\mathbf{x}_A), \\quad \\mathbf{z}_B = \\text{Encoder}(\\mathbf{x}_B)\n",
        "   $$\n",
        "\n",
        "2. Linear interpolation:\n",
        "   $$\n",
        "   \\mathbf{z}_{new} = \\alpha\\mathbf{z}_A + (1-\\alpha)\\mathbf{z}_B, \\quad \\alpha \\in [0,1]\n",
        "   $$\n",
        "\n",
        "3. Decode to generate new solution:\n",
        "   $$\n",
        "   \\mathbf{x}_{new} = \\text{Decoder}(\\mathbf{z}_{new})\n",
        "   $$\n",
        "\n",
        "\n",
        "### Geodesic Interpolation\n",
        "For solutions $\\mathbf{x}_A$, $\\mathbf{x}_B$:\n",
        "1. Encode: $\\mathbf{z}_A = g_\\phi(\\mathbf{x}_A)$, $\\mathbf{z}_B = g_\\phi(\\mathbf{x}_B)$\n",
        "2. Spherical interpolation:\n",
        "   $$\n",
        "   \\mathbf{z}_{\\text{new}} = \\frac{\\sin[(1-\\alpha)\\Omega]}{\\sin\\Omega}\\mathbf{z}_A + \\frac{\\sin[\\alpha\\Omega]}{\\sin\\Omega}\\mathbf{z}_B\n",
        "   $$\n",
        "   where $\\Omega = \\arccos(\\mathbf{z}_A^\\top \\mathbf{z}_B)$\n",
        "\n",
        "3. Decode: $\\mathbf{x}_{\\text{new}} = f_\\theta(\\mathbf{z}_{\\text{new}})$\n"
      ],
      "metadata": {
        "id": "A8km8d2E-ANE"
      },
      "id": "A8km8d2E-ANE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Interpolation with geodesic sampling\n",
        "def geodesic_interpolate(z1, z2, alpha):\n",
        "    omega = np.arccos(np.clip(np.dot(z1.T, z2)[0,0], -1, 1))\n",
        "    return (np.sin((1-alpha)*omega)/np.sin(omega))*z1 + (np.sin(alpha*omega)/np.sin(omega))*z2\n",
        "\n",
        "z_A = encoder.predict(X_train[0:1])\n",
        "z_B = encoder.predict(X_train[1:2])\n",
        "\n",
        "for alpha in np.linspace(0, 1, 5):\n",
        "    z_new = geodesic_interpolate(z_A, z_B, alpha)\n",
        "    J_new = decoder.predict(z_new)\n",
        "    J_original = scaler.inverse_transform(J_new)\n",
        "    print(f\"α={alpha:.1f}: Time={J_original[0,0]:.2f}s, Energy={J_original[0,1]:.2f}kWh\")\n"
      ],
      "metadata": {
        "id": "jntdH5Hj-CLo"
      },
      "id": "jntdH5Hj-CLo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b16d24cc",
      "metadata": {
        "id": "b16d24cc"
      },
      "outputs": [],
      "source": [
        "# Interpolation with linear sampling\n",
        "\n",
        "# Encode two Pareto solutions\n",
        "z_A = encoder.predict(X_train[0:1])  # Solution A\n",
        "z_B = encoder.predict(X_train[1:2])  # Solution B\n",
        "\n",
        "# Linear interpolation\n",
        "alpha = 0.5\n",
        "z_new = alpha * z_A + (1 - alpha) * z_B\n",
        "\n",
        "# Decode to generate new solution\n",
        "J_new = decoder.predict(z_new)\n",
        "\n",
        "# Denormalize\n",
        "J_new_original = scaler.inverse_transform(J_new)\n",
        "print(f\"Interpolated Solution: Time = {J_new_original[0, 0]:.2f} s, Energy = {J_new_original[0, 1]:.2f} kWh\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution Validation Protocol\n",
        "\n",
        "### Dominance Verification\n",
        "$$\n",
        "\\mathbf{x}_{\\text{new}} \\text{ is non-dominated iff } \\nexists \\mathbf{x}_i \\in X :\n",
        "\\begin{cases}\n",
        "f_1^{(i)} \\leq f_1^{\\text{(new)}} \\\\\n",
        "f_2^{(i)} \\leq f_2^{\\text{(new)}} \\\\\n",
        "\\|\\mathbf{x}_i - \\mathbf{x}_{\\text{new}}\\|_2 > \\delta\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "### Feasibility Check\n",
        "$$\n",
        "\\mathbf{x}_{\\text{new}} \\in \\mathcal{F} \\iff\n",
        "\\begin{cases}\n",
        "g_1(\\mathbf{x}_{\\text{new}}) \\leq 0 \\\\\n",
        "g_2(\\mathbf{x}_{\\text{new}}) \\leq 0 \\\\\n",
        "\\vdots \\\\\n",
        "g_k(\\mathbf{x}_{\\text{new}}) \\leq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Ensure:\n",
        "$$\n",
        "f_1^{new} \\geq 0, \\quad f_2^{new} \\geq 0\n",
        "$$\n",
        "And any problem-specific constraints (e.g., $g(\\mathbf{x}_{new}) \\leq 0$)\n",
        "\n",
        "\n",
        "## Implementation Notes\n",
        "\n",
        "1. **Normalization**: Essential for stable training\n",
        "2. **Bottleneck Size**: 1D latent space enables linear interpolation\n",
        "3. **Activation**: Sigmoid ensures outputs stay in normalized $[0,1]$ range\n",
        "4. **Regularization**: Consider adding KL divergence for variational AE"
      ],
      "metadata": {
        "id": "GaxxlDqA-d_q"
      },
      "id": "GaxxlDqA-d_q"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2qlcpfob9yai"
      },
      "id": "2qlcpfob9yai",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}