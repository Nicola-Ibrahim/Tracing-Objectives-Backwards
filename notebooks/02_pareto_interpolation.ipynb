{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nicola-Ibrahim/Pareto-Optimization/blob/main/notebooks/02_pareto_interpolation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder Formulation for Pareto Front Analysis\n",
        "\n",
        "## Data Preparation\n",
        "**Input**: A set of Pareto-optimal solutions  \n",
        "$X = \\{\\mathbf{x}_i\\} \\in \\mathbb{R}^2$, where each $\\mathbf{x}_i = [f_1, f_2]$ represents a trade-off between two objectives.\n",
        "\n",
        "**Normalization**: Scale objectives to $[0,1]$ range for training stability:\n",
        "$$\n",
        "\\hat{f}_1 = \\frac{f_1 - f_{1}^{min}}{f_{1}^{max} - f_{1}^{min}}, \\quad\n",
        "\\hat{f}_2 = \\frac{f_2 - f_{2}^{min}}{f_{2}^{max} - f_{2}^{min}}\n",
        "$$\n",
        "\n",
        "## Architecture\n",
        "\n",
        "### Encoder\n",
        "Maps 2D Pareto solutions to a 1D latent space:\n",
        "$$\n",
        "\\mathbf{z} = \\text{Encoder}(\\mathbf{x}) = \\sigma(\\mathbf{W}_2 \\cdot \\text{ReLU}(\\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1) + \\mathbf{b}_2)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\mathbf{W}_1 \\in \\mathbb{R}^{h \\times 2}$, $\\mathbf{W}_2 \\in \\mathbb{R}^{1 \\times h}$ are weight matrices\n",
        "- $\\mathbf{b}_1 \\in \\mathbb{R}^h$, $\\mathbf{b}_2 \\in \\mathbb{R}^1$ are bias terms\n",
        "- $h$ is hidden layer size\n",
        "- $\\sigma$ is sigmoid activation\n",
        "\n",
        "### Decoder\n",
        "Reconstructs solutions from latent space:\n",
        "$$\n",
        "\\hat{\\mathbf{x}} = \\text{Decoder}(\\mathbf{z}) = \\sigma(\\mathbf{W}_4 \\cdot \\text{ReLU}(\\mathbf{W}_3\\mathbf{z} + \\mathbf{b}_3) + \\mathbf{b}_4)\n",
        "$$\n",
        "\n",
        "With:\n",
        "- $\\mathbf{W}_3 \\in \\mathbb{R}^{h \\times 1}$, $\\mathbf{W}_4 \\in \\mathbb{R}^{2 \\times h}$\n",
        "- $\\mathbf{b}_3 \\in \\mathbb{R}^h$, $\\mathbf{b}_4 \\in \\mathbb{R}^2$\n",
        "\n",
        "### Loss Function\n",
        "Mean Squared Error (MSE) reconstruction loss:\n",
        "$$\n",
        "\\mathcal{L}_{recon} = \\frac{1}{N}\\sum_{i=1}^N \\|\\mathbf{x}_i - \\hat{\\mathbf{x}}_i\\|^2_2\n",
        "$$\n",
        "\n",
        "## Interpolation in Latent Space\n",
        "\n",
        "1. Encode two solutions:\n",
        "   $$\n",
        "   \\mathbf{z}_A = \\text{Encoder}(\\mathbf{x}_A), \\quad \\mathbf{z}_B = \\text{Encoder}(\\mathbf{x}_B)\n",
        "   $$\n",
        "\n",
        "2. Linear interpolation:\n",
        "   $$\n",
        "   \\mathbf{z}_{new} = \\alpha\\mathbf{z}_A + (1-\\alpha)\\mathbf{z}_B, \\quad \\alpha \\in [0,1]\n",
        "   $$\n",
        "\n",
        "3. Decode to generate new solution:\n",
        "   $$\n",
        "   \\mathbf{x}_{new} = \\text{Decoder}(\\mathbf{z}_{new})\n",
        "   $$\n",
        "\n",
        "## Solution Validation\n",
        "\n",
        "### Dominance Check\n",
        "For a new solution $\\mathbf{x}_{new} = [f_1^{new}, f_2^{new}]$, verify:\n",
        "$$\n",
        "\\nexists \\mathbf{x}_i \\in X \\text{ such that }\n",
        "\\begin{cases}\n",
        "f_1^i \\leq f_1^{new} \\\\\n",
        "f_2^i \\leq f_2^{new} \\\\\n",
        "\\text{with at least one strict inequality}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "### Feasibility Check\n",
        "Ensure:\n",
        "$$\n",
        "f_1^{new} \\geq 0, \\quad f_2^{new} \\geq 0\n",
        "$$\n",
        "And any problem-specific constraints (e.g., $g(\\mathbf{x}_{new}) \\leq 0$)\n",
        "\n",
        "## Implementation Notes\n",
        "\n",
        "1. **Normalization**: Essential for stable training\n",
        "2. **Bottleneck Size**: 1D latent space enables linear interpolation\n",
        "3. **Activation**: Sigmoid ensures outputs stay in normalized $[0,1]$ range\n",
        "4. **Regularization**: Consider adding KL divergence for variational AE"
      ],
      "metadata": {
        "id": "iojL3z_52E-y"
      },
      "id": "iojL3z_52E-y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc9ba7a8",
      "metadata": {
        "id": "bc9ba7a8"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Normalize to [0, 1]\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = scaler.fit_transform(pareto_front)\n",
        "\n",
        "# Split data (80% train, 20% validation)\n",
        "X_train, X_val = train_test_split(X_normalized, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1431b8e8",
      "metadata": {
        "id": "1431b8e8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define autoencoder architecture\n",
        "input_dim = 2\n",
        "latent_dim = 1  # 1D latent space for simplicity\n",
        "\n",
        "# Encoder\n",
        "inputs = Input(shape=(input_dim,))\n",
        "encoded = Dense(32, activation='relu')(inputs)\n",
        "encoded = Dense(latent_dim, activation='linear')(encoded)\n",
        "\n",
        "# Decoder\n",
        "decoded = Dense(32, activation='relu')(encoded)\n",
        "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "\n",
        "# Compile\n",
        "autoencoder = Model(inputs, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train\n",
        "history = autoencoder.fit(\n",
        "    X_train, X_train,\n",
        "    epochs=500,\n",
        "    batch_size=16,\n",
        "    validation_data=(X_val, X_val),\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b16d24cc",
      "metadata": {
        "id": "b16d24cc"
      },
      "outputs": [],
      "source": [
        "# Encode two Pareto solutions\n",
        "z_A = encoder.predict(X_train[0:1])  # Solution A\n",
        "z_B = encoder.predict(X_train[1:2])  # Solution B\n",
        "\n",
        "# Linear interpolation\n",
        "alpha = 0.5\n",
        "z_new = alpha * z_A + (1 - alpha) * z_B\n",
        "\n",
        "# Decode to generate new solution\n",
        "J_new = decoder.predict(z_new)\n",
        "\n",
        "# Denormalize\n",
        "J_new_original = scaler.inverse_transform(J_new)\n",
        "print(f\"Interpolated Solution: Time = {J_new_original[0, 0]:.2f} s, Energy = {J_new_original[0, 1]:.2f} kWh\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}