{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder Formulation for Pareto Front Analysis\n",
        "\n",
        "## Data Preparation\n",
        "**Input Data Structure**:\n",
        "- Let $X = \\{\\mathbf{x}_i\\}_{i=1}^N \\subset \\mathbb{R}^2$ be the set of Pareto-optimal solutions\n",
        "- Each solution $\\mathbf{x}_i = (f_1^{(i)}, f_2^{(i)})$ represents a trade-off between:\n",
        "  - $f_1$: Travel time (minutes)\n",
        "  - $f_2$: Energy consumption (kWh)\n",
        "\n",
        "\n",
        "**Normalization**: Scale objectives to $[0,1]$ range for training stability:\n",
        "$$\n",
        "\\hat{f}_1 = \\frac{f_1 - f_{1}^{min}}{f_{1}^{max} - f_{1}^{min}}, \\quad\n",
        "\\hat{f}_2 = \\frac{f_2 - f_{2}^{min}}{f_{2}^{max} - f_{2}^{min}}\n",
        "$$\n",
        "\n",
        "**Standardization**:\n",
        "$$\n",
        "\\hat{f}_k^{(i)} = \\frac{f_k^{(i)} - \\mu_k}{\\sigma_k} \\quad \\text{for } k=1,2\n",
        "$$\n",
        "where $\\mu_k$, $\\sigma_k$ are the mean and standard deviation of each objective.\n",
        "\n",
        "## Neural Architecture Specification\n",
        "\n",
        "### Encoder Network (Compression)\n",
        "$$\n",
        "\\mathbf{z} = g_\\phi(\\mathbf{x}) = \\text{LeakyReLU}(\\mathbf{W}_2 \\cdot \\text{ELU}(\\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1) + \\mathbf{b}_2)\n",
        "$$\n",
        "\n",
        "### Decoder Network (Reconstruction)\n",
        "$$\n",
        "\\hat{\\mathbf{x}} = f_\\theta(\\mathbf{z}) = \\text{Sigmoid}(\\mathbf{W}_4 \\cdot \\text{ELU}(\\mathbf{W}_3\\mathbf{z} + \\mathbf{b}_3) + \\mathbf{b}_4)\n",
        "$$\n",
        "\n",
        "**Dimensionality**:\n",
        "- Input/Output: $\\mathbb{R}^2$ (normalized objectives)\n",
        "- Latent space: $\\mathbb{R}^1$ (bottleneck)\n",
        "- Hidden layers: 32 neurons with ELU activation\n",
        "\n",
        "\n",
        "## Architecture\n",
        "\n",
        "### Encoder\n",
        "Maps 2D Pareto solutions to a 1D latent space:\n",
        "$$\n",
        "\\mathbf{z} = \\text{Encoder}(\\mathbf{x}) = \\sigma(\\mathbf{W}_2 \\cdot \\text{ReLU}(\\mathbf{W}_1\\mathbf{x} + \\mathbf{b}_1) + \\mathbf{b}_2)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\mathbf{W}_1 \\in \\mathbb{R}^{h \\times 2}$, $\\mathbf{W}_2 \\in \\mathbb{R}^{1 \\times h}$ are weight matrices\n",
        "- $\\mathbf{b}_1 \\in \\mathbb{R}^h$, $\\mathbf{b}_2 \\in \\mathbb{R}^1$ are bias terms\n",
        "- $h$ is hidden layer size\n",
        "- $\\sigma$ is sigmoid activation\n",
        "\n",
        "### Decoder\n",
        "Reconstructs solutions from latent space:\n",
        "$$\n",
        "\\hat{\\mathbf{x}} = \\text{Decoder}(\\mathbf{z}) = \\sigma(\\mathbf{W}_4 \\cdot \\text{ReLU}(\\mathbf{W}_3\\mathbf{z} + \\mathbf{b}_3) + \\mathbf{b}_4)\n",
        "$$\n",
        "\n",
        "With:\n",
        "- $\\mathbf{W}_3 \\in \\mathbb{R}^{h \\times 1}$, $\\mathbf{W}_4 \\in \\mathbb{R}^{2 \\times h}$\n",
        "- $\\mathbf{b}_3 \\in \\mathbb{R}^h$, $\\mathbf{b}_4 \\in \\mathbb{R}^2$\n",
        "\n",
        "### Loss Function\n",
        "Mean Squared Error (MSE) reconstruction loss:\n",
        "$$\n",
        "\\mathcal{L}_{recon} = \\frac{1}{N}\\sum_{i=1}^N \\|\\mathbf{x}_i - \\hat{\\mathbf{x}}_i\\|^2_2\n",
        "$$\n",
        "\n",
        "## Interpolation in Latent Space\n",
        "\n",
        "### Linear Interpolation\n",
        "For solutions $\\mathbf{x}_A$, $\\mathbf{x}_B$:\n",
        "1. Encode two solutions:\n",
        "   $$\n",
        "   \\mathbf{z}_A = \\text{Encoder}(\\mathbf{x}_A), \\quad \\mathbf{z}_B = \\text{Encoder}(\\mathbf{x}_B)\n",
        "   $$\n",
        "\n",
        "2. Linear interpolation:\n",
        "   $$\n",
        "   \\mathbf{z}_{new} = \\alpha\\mathbf{z}_A + (1-\\alpha)\\mathbf{z}_B, \\quad \\alpha \\in [0,1]\n",
        "   $$\n",
        "\n",
        "3. Decode to generate new solution:\n",
        "   $$\n",
        "   \\mathbf{x}_{new} = \\text{Decoder}(\\mathbf{z}_{new})\n",
        "   $$\n",
        "\n",
        "\n",
        "### Geodesic Interpolation\n",
        "For solutions $\\mathbf{x}_A$, $\\mathbf{x}_B$:\n",
        "1. Encode: $\\mathbf{z}_A = g_\\phi(\\mathbf{x}_A)$, $\\mathbf{z}_B = g_\\phi(\\mathbf{x}_B)$\n",
        "2. Spherical interpolation:\n",
        "   $$\n",
        "   \\mathbf{z}_{\\text{new}} = \\frac{\\sin[(1-\\alpha)\\Omega]}{\\sin\\Omega}\\mathbf{z}_A + \\frac{\\sin[\\alpha\\Omega]}{\\sin\\Omega}\\mathbf{z}_B\n",
        "   $$\n",
        "   where $\\Omega = \\arccos(\\mathbf{z}_A^\\top \\mathbf{z}_B)$\n",
        "\n",
        "3. Decode: $\\mathbf{x}_{\\text{new}} = f_\\theta(\\mathbf{z}_{\\text{new}})$\n",
        "\n",
        "\n",
        "## Solution Validation Protocol\n",
        "\n",
        "### Dominance Verification\n",
        "$$\n",
        "\\mathbf{x}_{\\text{new}} \\text{ is non-dominated iff } \\nexists \\mathbf{x}_i \\in X :\n",
        "\\begin{cases}\n",
        "f_1^{(i)} \\leq f_1^{\\text{(new)}} \\\\\n",
        "f_2^{(i)} \\leq f_2^{\\text{(new)}} \\\\\n",
        "\\|\\mathbf{x}_i - \\mathbf{x}_{\\text{new}}\\|_2 > \\delta\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "### Feasibility Check\n",
        "$$\n",
        "\\mathbf{x}_{\\text{new}} \\in \\mathcal{F} \\iff\n",
        "\\begin{cases}\n",
        "g_1(\\mathbf{x}_{\\text{new}}) \\leq 0 \\\\\n",
        "g_2(\\mathbf{x}_{\\text{new}}) \\leq 0 \\\\\n",
        "\\vdots \\\\\n",
        "g_k(\\mathbf{x}_{\\text{new}}) \\leq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Ensure:\n",
        "$$\n",
        "f_1^{new} \\geq 0, \\quad f_2^{new} \\geq 0\n",
        "$$\n",
        "And any problem-specific constraints (e.g., $g(\\mathbf{x}_{new}) \\leq 0$)\n",
        "\n",
        "\n",
        "## Implementation Notes\n",
        "\n",
        "1. **Normalization**: Essential for stable training\n",
        "2. **Bottleneck Size**: 1D latent space enables linear interpolation\n",
        "3. **Activation**: Sigmoid ensures outputs stay in normalized $[0,1]$ range\n",
        "4. **Regularization**: Consider adding KL divergence for variational AE"
      ],
      "metadata": {
        "id": "iojL3z_52E-y"
      },
      "id": "iojL3z_52E-y"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T7CaJUAe4drS"
      },
      "id": "T7CaJUAe4drS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc9ba7a8",
      "metadata": {
        "id": "bc9ba7a8"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Normalize to [0, 1]\n",
        "scaler = MinMaxScaler()\n",
        "X_normalized = scaler.fit_transform(pareto_front)\n",
        "\n",
        "# Split data (80% train, 20% validation)\n",
        "X_train, X_val = train_test_split(X_normalized, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1431b8e8",
      "metadata": {
        "id": "1431b8e8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define autoencoder architecture\n",
        "input_dim = 2\n",
        "latent_dim = 1  # 1D latent space for simplicity\n",
        "\n",
        "# Encoder\n",
        "inputs = Input(shape=(input_dim,))\n",
        "encoded = Dense(32, activation='relu')(inputs)\n",
        "encoded = Dense(latent_dim, activation='linear')(encoded)\n",
        "\n",
        "# Decoder\n",
        "decoded = Dense(32, activation='relu')(encoded)\n",
        "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
        "\n",
        "# Compile\n",
        "autoencoder = Model(inputs, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train\n",
        "history = autoencoder.fit(\n",
        "    X_train, X_train,\n",
        "    epochs=500,\n",
        "    batch_size=16,\n",
        "    validation_data=(X_val, X_val),\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b16d24cc",
      "metadata": {
        "id": "b16d24cc"
      },
      "outputs": [],
      "source": [
        "# Encode two Pareto solutions\n",
        "z_A = encoder.predict(X_train[0:1])  # Solution A\n",
        "z_B = encoder.predict(X_train[1:2])  # Solution B\n",
        "\n",
        "# Linear interpolation\n",
        "alpha = 0.5\n",
        "z_new = alpha * z_A + (1 - alpha) * z_B\n",
        "\n",
        "# Decode to generate new solution\n",
        "J_new = decoder.predict(z_new)\n",
        "\n",
        "# Denormalize\n",
        "J_new_original = scaler.inverse_transform(J_new)\n",
        "print(f\"Interpolated Solution: Time = {J_new_original[0, 0]:.2f} s, Energy = {J_new_original[0, 1]:.2f} kWh\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}