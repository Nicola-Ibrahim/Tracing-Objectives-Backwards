%-------------------------------------------------------------------------------
% Implementation
%-------------------------------------------------------------------------------

\chapter{Implementation}\label{chap:implementation}

This chapter documents the practical implementation of the proposed
inverse decision mapping framework.  We describe the software
infrastructure, the benchmark problems used for testing, the training
pipelines for the inverse and forward models and the user interface for
interactive exploration.

\section{Software Infrastructure}

The implementation is based on the Python programming language (version
3.10) and relies on several open‑source libraries:

\begin{itemize}
  \item \textbf{\texttt{pymoo}} for multi‑objective optimisation.  We use
    its implementations of NSGA‑II, MOEA/D and other evolutionary
    algorithms to generate Pareto samples.  Pymoo provides convenient
    interfaces for defining benchmark problems, populations and
    performance indicators.
  \item \textbf{\texttt{scikit‑learn}} for regression modelling.  Gaussian
    processes, radial basis function interpolators, random forests and
    feed‑forward neural networks are implemented using the
    \texttt{GaussianProcessRegressor}, \texttt{RBFInterpolator},
    \texttt{RandomForestRegressor} and \texttt{MLPRegressor} classes.
    Cross‑validation and hyperparameter search are handled by
    \texttt{GridSearchCV} and \texttt{RandomizedSearchCV}.
  \item \textbf{\texttt{NumPy}} and \texttt{SciPy} for numerical
    operations, random sampling and distance computations.
  \item \textbf{\texttt{Dash}} for the web‑based user interface.  Dash
    facilitates building interactive dashboards with sliders, drop‑down
    menus and plots rendered via Plotly.
  \item \textbf{\texttt{Matplotlib}} and \texttt{Plotly} for plotting
    Pareto fronts, target objectives and candidate solutions.
\end{itemize}

The code base follows a modular architecture.  A \texttt{problem}
module defines benchmark functions and their parameters; a
\texttt{optimisation} module wraps calls to evolutionary algorithms and
performs sampling; a \texttt{models} module contains classes for
training and predicting with different inverse and forward surrogates; and
a \texttt{ui} module implements the interactive dashboard.  Unit tests
ensure correctness of core functions, and scripts are provided to
reproduce all experiments.

\section{Benchmark Problems}

To evaluate the inverse mapping framework we experiment on well‑known
synthetic MOPs.  These problems differ in dimensionality, Pareto front
shape and modality and therefore test different aspects of the methods.
Specifically, we consider:

\begin{description}
  \item[ZDT suite] The six ZDT problems (ZDT1–ZDT6) are two‑objective
  problems with 30 decision variables and various front shapes (convex,
  non‑convex, discontinuous).  They are popular benchmarks for testing
  evolutionary algorithms and surrogate models.
  \item[DTLZ suite] The DTLZ problems (DTLZ1–DTLZ7) can be scaled to
  arbitrary numbers of objectives and decision variables.  We use
  DTLZ2, DTLZ5 and DTLZ7 with three objectives to assess inverse
  modelling in higher dimensions.
  \item[Real‑world case study] A simplified engineering design problem
  inspired by structural optimisation is used as a proof of concept.  It
  involves minimising weight and deflection of a beam subject to stress
  constraints.  Objective functions are computed via a finite
  element model.  Because the simulation is expensive, the benefit of
  inverse mapping is accentuated.
\end{description}

For each benchmark problem we generate Pareto samples by running NSGA‑II
with a population size of 100, crossover probability 0.9, mutation
probability 1/\(n\) (where \(n\) is the number of decision variables) and
100 generations.  For DTLZ problems we use a population size of 300 and
200 generations.  The union of the final non‑dominated sets over five
independent runs forms the dataset \(\mathcal{D}\).  When transfer
learning is explored, samples from multiple problems (e.g. ZDT1 and
ZDT2) are pooled together.

\section{Training Pipeline}

Given a dataset \(\mathcal{D}\) we perform the following steps for each
problem:

\begin{enumerate}
  \item \textbf{Normalisation.}  Compute the minimum and maximum of each
    objective and decision dimension and scale the data to \([0,1]\).
  \item \textbf{Train–validation split.}  Use an 80/20 split for
    training and validation.  When data is scarce we employ a leave‑one‑out
    scheme.
  \item \textbf{Hyperparameter search.}  For each inverse model type
    (GPR, RBF, RF, FFNN) perform 50 random trials over predefined
    hyperparameter ranges.  For GPR we search over squared exponential
    kernels with length scales in \([0.1,10]\); for RBF we vary the
    number of centres between 10 and 200; for RF we vary the number of
    trees (50–300) and maximum depth (3–20); for FFNN we vary the number
    of hidden layers (1–3) and neurons (50–200).
  \item \textbf{Model training.}  Fit each candidate model on the
    training data and evaluate performance on the validation set using
    RMSE and generational distance.  Select the best hyperparameter
    configuration.
  \item \textbf{Forward surrogate.}  Train a Gaussian process regression
    model on the same training data to act as the forward surrogate.  A
    Matérn kernel is used with noise estimated via maximum likelihood.
  \item \textbf{Testing.}  Evaluate the selected inverse and forward
    models on the held‑out test data.  Report RMSE, \(R^2\) and the
    proportion of predictions within a tolerance of the true decision
    values.
\end{enumerate}

