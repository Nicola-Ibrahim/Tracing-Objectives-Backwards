%-------------------------------------------------------------------------------
% Problem statement
%-------------------------------------------------------------------------------

\chapter{Problem Statement}\label{chap:problem}

This chapter formalises the inverse decision mapping problem studied in
this thesis.  We begin by recalling the multi‑objective optimisation
formulation and then define the inverse mapping task.  We discuss
conditions under which a target objective vector is admissible and propose
criteria for evaluating the quality of candidate decision vectors.

\section{Inverse Mapping in Multi‑Objective Optimisation}

Consider a multi‑objective optimisation problem (MOP) defined by a vector
of decision variables \(\vect{x}\in\calX\subset\R^n\) and objective
functions \(\vect{f}\colon\calX\to\calY\subset\R^m\).  The goal of
traditional MOP solvers is to approximate the Pareto set \(\text{PS}\)
and its image, the Pareto front \(\text{PF}\).  In the forward direction
one evaluates \(\vect{f}(\vect{x})\) for candidate \(\vect{x}\) and selects
non‑dominated solutions.  Inverse mapping poses the complementary task:
given a desired objective vector \(\vect{y}^\star\in\calY\), find one or
more decision vectors \(\vect{x}^\star\in\calX\) such that
\(\vect{f}(\vect{x}^\star)\approx \vect{y}^\star\).  Denote by
\(g\colon \calY\to \calX\) the (generally multi‑valued) inverse map,
\(\vect{x}=g(\vect{y})\).  Since the forward map is often
non‑injective—multiple decisions may yield identical objective values—and
non‑surjective—some objectives may be unattainable—the inverse problem
is inherently ill‑posed.  That is, a desired target \(\vect{y}^\star\)
  may correspond to zero, one or many feasible decisions~\cite{Tan2023}.

In practice we do not have access to the exact forward map but only a
finite set of sample pairs \(\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^N\) obtained
by running a multi‑objective optimisation algorithm.  These pairs sample
the Pareto front and possibly its vicinity.  The inverse decision
mapping problem considered here is:

\begin{quote}
\emph{Given a target objective vector \(\vect{y}^\star\in\calY\) and a
dataset \(\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^N\) of previously
evaluated Pareto‑optimal or near‑optimal solutions, estimate one or more
candidate decision vectors \(\hat{g}(\vect{y}^\star)\) that are likely to
produce \(\vect{y}^\star\) when evaluated with the true objective
functions.}
\end{quote}

Here \(\hat{g}\) denotes an approximate inverse model trained from the
data.  Because the map may be non‑unique, the output of \(\hat{g}\)
comprises a set of candidates or a distribution over decisions.

\section{Feasibility and Admissibility}

Not every desired objective vector is physically or logically attainable.
We therefore distinguish between three cases:

\begin{enumerate}
  \item \textbf{Feasible Pareto region:}  \(\vect{y}^\star\) lies on or close
  to the approximated Pareto front.  In this case there is at least one
  \(\vect{x}\in\mathcal{D}\) with \(\vect{f}(\vect{x})\approx\vect{y}^\star\),
  and an inverse model should produce similar decisions.
  \item \textbf{Feasible dominated region:} \(\vect{y}^\star\) lies in the
  dominated region, i.e. there exists \(\vect{y}\in\text{PF}\) such that
  \(\vect{y}\preceq\vect{y}^\star\).  Although such objectives are
  feasible, they do not represent efficient trade‑offs.  The inverse
  model may choose to project \(\vect{y}^\star\) onto the Pareto front or
  propose a dominated solution and warn the user.
  \item \textbf{Infeasible region:}  \(\vect{y}^\star\) is outside the
  attainable region in objective space.  There is no decision vector
  achieving \(\vect{y}^\star\), and any candidate produced by the inverse
  model must necessarily violate constraints or objectives.  A
  plausibility check should detect this case and inform the user.
\end{enumerate}

Determining which case applies requires a notion of distance from
\(\vect{y}^\star\) to the sampled Pareto front.  In this thesis we use
distance measures such as the Euclidean distance to the nearest sampled
Pareto point and statistical tests to estimate whether \(\vect{y}^\star\) is
within a tolerance of the front.  Confidence thresholds can be set
differently depending on the application.

\section{Quality Criteria for Inverse Solutions}

Having generated candidate decisions \(\hat{\vect{x}}\) via the inverse
model, we need criteria to evaluate their quality.  Several measures are
pertinent:

\begin{description}
  \item[Forward error] \(e_f=\|\vect{f}(\hat{\vect{x}})-\vect{y}^\star\|\)
  quantifies how close the true objectives of the candidate are to the
  target.  Because \(\vect{f}\) may be expensive to evaluate, a
  surrogate forward model can provide an approximate assessment.
  \item[Distance to Pareto front] For Pareto‑optimal targets the candidate
  should lie near the front.  We compute the generational distance
  between \(\hat{\vect{x}}\) and the sampled Pareto set as described in
  the literature on performance indicators for multi‑objective optimisation~\cite{Blank2020}.
  \item[Diversity and multiplicity] When multiple candidates are returned,
  diversity encourages exploration of alternative designs.  We measure
  pairwise distances among candidates in decision space and select
  diversified subsets for user presentation.
  \item[Plausibility score] A scalar between 0 and 1 indicates the degree
  of confidence that \(\vect{y}^\star\) is attainable given the training
  data.  The score is derived from the distance to the Pareto front and
  the uncertainty estimates of the inverse model.  A low score warns
  users that the prediction may be unreliable.
\end{description}

\section{Research Questions}

The overarching goal of this thesis is to develop methods for
interactive inverse mapping in multi‑objective systems.  The following
research questions guide our investigation:

\begin{enumerate}
  \item How can a user specify a target objective vector \(\vect{y}^\star\)
  and obtain a set of candidate decisions \(\hat{\vect{x}}\) without
  rerunning the entire optimisation process?
  \item Which modelling strategies—Gaussian processes, radial basis
  functions, random forests, neural networks or transfer learning—are
  most effective for learning the inverse mapping from a finite set of
  Pareto samples?
  \item How can we assess whether a target \(\vect{y}^\star\) lies within
  the attainable region and communicate this information to the user in
  an intuitive way?
  \item How do the quality and quantity of training data influence the
  accuracy, diversity and robustness of the inverse model?  What is the
  trade‑off between model complexity and generalisation when sample
  sizes are small?
\end{enumerate}

The remainder of this thesis proposes a modular methodology and
experimental evaluation to address these questions.