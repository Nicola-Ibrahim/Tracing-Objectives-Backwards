%-------------------------------------------------------------------------------
% Background and Motivation
%-------------------------------------------------------------------------------

\chapter{Background and Motivation}\label{chap:motivation}

Inverse decision mapping builds on ideas from multi-objective optimisation (MOO)
and inverse problems. This chapter introduces (i) a compact view of MOO and
trade-off sets, (ii) the forward–inverse perspective and why inverse problems
are often ill-posed, and (iii) practical ways to obtain representative
\((\vect{x},\vect{y})\) pairs for learning an inverse map. We keep the
discussion agnostic to any specific optimiser; Pareto-based tools are one
useful option among several. For broad MOO fundamentals and assessment, see
Deb~\cite{Deb2011Intro} and the tutorial by Emmerich \& Deutz~\cite{Emmerich2018Tutorial}.

\section{Multi-Objective Optimisation}

A multi-objective optimisation problem (MOP) is defined by a vector of decision variables \(\vect{x}=(x_1,\dots,x_n)\in\calX\subset \R^n\) and a vector of objective functions \(\vect{f}=(f_1,\dots,f_m)\colon \calX\to \calY\subset \R^m\). Without loss of generality we consider all objectives to be minimised. A MOP can be written compactly as

\begin{align} 
\min_{\vect{x}\in\calX}\; \vect{f}(\vect{x}) &= \big(f_1(\vect{x}),\dots,f_m(\vect{x})\big),\\ \text{s.t.}\; g_j(\vect{x}) &\le 0,\quad j=1,\dots,J,\\ h_k(\vect{x}) &= 0,\quad k=1,\dots,K, 
\label{eq:mop}
\end{align}


where \(g_j\) and \(h_k\) denote inequality and equality constraints, respectively. The feasible region \(\calX\) is assumed to be a compact, connected subset of \(\R^n\). Each objective \(f_i\) may be non-convex, nonlinear and expensive to evaluate. Because objectives typically conflict, the goal is a \emph{set of trade-off
designs} rather than a single optimum. A standard partial order uses
\emph{Pareto dominance}: for \(\vect{u},\vect{v}\in\R^m\),
\(\vect{u}\preceq\vect{v}\) iff \(u_i\le v_i\ \forall i\) and \(u_j<v_j\) for
some \(j\). A feasible \(\vect{x}^\star\) is Pareto-optimal if no other
\(\vect{x}\) satisfies \(\vect{f}(\vect{x})\preceq \vect{f}(\vect{x}^\star)\).
See the references above for a concise survey of algorithmic families and
assessment practices.
MOPs arise in a broad spectrum of application domains, including engineering, economics, logistics, manufacturing and science~\cite{Tan2023}. In practice, objective functions often conflict: improving one performance metric deteriorates another. 



\section{Forward and Inverse Viewpoints}

The \emph{forward} process evaluates how decisions produce outcomes:
\begin{equation}
  \vect{y} \;=\; \vect{f}(\vect{x}) \;+\; \boldsymbol{\varepsilon},
  \qquad \vect{x}\in\calX,\ \vect{y}\in\calY,
  \label{eq:forward}
\end{equation}
where \(\boldsymbol{\varepsilon}\) represents measurement/model error. Inverse
exploration asks for decisions that realise a desired outcome pattern
\(\vect{y}^{\star}\): find \(\vect{x}\) with \(\vect{f}(\vect{x})\approx
\vect{y}^{\star}\). Inverse problems often fail Hadamard’s well-posedness
criteria (existence, uniqueness, stability) because many distinct decisions
can map to similar outcomes and small output errors can induce large changes
in the inferred input—i.e., they are \emph{ill-posed}. \cite{KabanikhinSurvey,Stuart2010}

Two complementary formalisms are widely used to make the inverse task
meaningful and robust:

\paragraph{Regularised optimisation.}
Seek \(\vect{x}\) by solving
\begin{equation}
  \widehat{\vect{x}}
  \;\in\; \arg\min_{\vect{x}\in\calX}
  \underbrace{\mathcal{L}\big(\vect{f}(\vect{x}),\,\vect{y}^{\star}\big)}_{\text{data-fit}}
  \;+\;
  \lambda\,\underbrace{R(\vect{x})}_{\text{regulariser}},
  \label{eq:tikhonov}
\end{equation}
where \(\mathcal{L}\) measures mismatch (e.g., squared or Mahalanobis distance)
and \(R\) encodes prior structure (e.g., smoothness, sparsity, bounded
complexity). Regularisation restores stability to ill-posed inverses by
penalising implausible solutions. \cite{BenningBurger2018}

\paragraph{Probabilistic/Bayesian formulation.}
Treat \(\vect{x}\) as random and infer the posterior
\begin{equation}
  p(\vect{x}\mid \vect{y}^{\star})
  \;\propto\;
  \underbrace{p(\vect{y}^{\star}\mid \vect{x})}_{\text{likelihood}}
  \;\underbrace{p(\vect{x})}_{\text{prior}},
  \label{eq:posterior}
\end{equation}
which quantifies uncertainty and captures multi-valued relationships between
decisions and outcomes. Point estimates (MAP, posterior mean) and samples
provide candidate designs and uncertainty summaries. \cite{Stuart2010}

In this thesis we adopt both \emph{deterministic} (single-solution, typically
regularised) and \emph{probabilistic} (distributional) models as complementary
tools for inverse exploration, in line with standard inverse-problems practice.
\cite{BenningBurger2018,Stuart2010}

\section{DataBase}

Learning a data-driven inverse map \(\hat{g}:\calY\!\to\!\calX\) requires a set
\(\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^{N}\) with \(\vect{y}_i=
\vect{f}(\vect{x}_i)\). In practice, \(\mathcal{D}\) can be built from:
\begin{itemize}
  \item Historical runs and prior designs (e.g., archives, logs).
  \item Simulation sweeps/DoE that span \(\calX\) and reveal trade-offs.
  \item Exploratory optimisation, including evolutionary, decomposition-
        based, or surrogate-assisted methods that generate diverse samples.
\end{itemize}
Our approach is \emph{agnostic} to how \(\mathcal{D}\) is obtained; it consumes
input–output examples without relying on a single optimisation family. For
surveys covering these algorithmic paradigms and assessment, see
Deb; Emmerich \& Deutz. \cite{Deb2011Intro,Emmerich2018Tutorial}


\section{Probabilistic View}\label{sec:inverse_data_eval}
talk about the data and Bayesian view, and how to utilise this concept to generate more candidates and model the data via distribution instead of point-wise prediction. 

Also talk about the distribution and how they model the data and start sampling from it.

\section{Towards Inverse Design}\label{sec:inverse_data_eval}

Most materials workflows are built for the \emph{forward} question $\,\vect{x}\!\mapsto\!\vect{y}$—given a candidate, what will it do? Design asks the converse: specify a target response $\vect{y}^{\star}$ and identify admissible decisions $\vect{x}$ that realise it under physical and process constraints. Because many $\vect{x}$ can yield similar $\vect{y}$, the inverse task is generically ill-posed. We therefore pose it explicitly as a regularised/Bayesian inverse problem (Eqs.~\ref{eq:tikhonov}–\ref{eq:posterior}), which affords (i) fast, feasible point proposals consistent with prior knowledge and constraints, and (ii) calibrated distributions over plausible designs that make non-uniqueness and uncertainty first-class citizens~\cite{BenningBurger2018,Stuart2010}.

An inverse model proposes decisions $\vect{x}\in\mathcal{X}$ that are likely to realise a desired outcome pattern $\vect{y}^{\star}\in\mathcal{Y}$. Rather than tying learning to a specific data-generation protocol (e.g., forward Pareto sweeps), we treat \emph{inverse exploration} as a data-driven task that leverages any representative set $\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^{N}$—from historical logs, simulation sweeps, or exploratory optimisation—together with principled modelling (deterministic or probabilistic) to handle ill-posedness (non-uniqueness, instability) and quantify uncertainty where appropriate. This view aligns with modern treatments of multi-objective optimisation and inverse problems~\cite{Emmerich2018Tutorial,Miettinen1999,Stuart2010,BenningBurger2018}.

For clarity, we group assumptions into: (i) \emph{structural} assumptions on the inverse map $g:\mathcal{Y}\!\to\!\mathcal{X}$ (e.g., smoothness, sparsity, Lipschitz bounds), (ii) \emph{feasibility} constraints $g_j(\vect{x})\!\le\!0$, $h_k(\vect{x})\!=\!0$ inherited from the design domain (cf.\ Eq.~\eqref{eq:mop}), and (iii) \emph{data} assumptions (coverage, noise models). When many $\vect{x}$ produce similar $\vect{y}$, the inverse is non-unique; well-posedness is recovered either by regularisation or by adopting a probabilistic posterior/predictive view that represents distributions over $\vect{x}$ conditional on $\vect{y}$.\footnote{See \cite{Stuart2010,KaipioSomersalo2005,Tarantola2005} for detailed treatments of existence/uniqueness/stability and statistical formulations.} In practice, training and query distributions may differ (dataset/covariate shift), motivating validation protocols and uncertainty reporting that are explicit about the deployment setting~\cite{QuinoneroCandela2009}.

Taken together, this framing turns the inverse task into \emph{guided exploration} of a constrained, often multi-modal design space, where priors, regularisation, and uncertainty are first-class citizens. The next section organises this exploration into a small set of complementary strategy families.


\subsection{Inverse-design strategies}

Inverse design starts from a target response and searches the (typically vast and constrained) space of compositions, structures, and processes for feasible solutions. Compared with forward modeling, inverse problems are often ill-posed (one-to-many mappings) and data-limited, amplifying concerns around generalization, uncertainty, and reproducibility in machine-learning (ML) workflows~\cite{Butler2018_Nature,Wei2019_InfoMat,Draxl2019_JPhysMater}. In practice, three complementary strategy families are prevalent: \textbf{exploration-based} (agent- or heuristic-driven search with minimal labels), \textbf{model-based} (learning bidirectional or generative structure–property surrogates), and \textbf{optimization-based} (treating inverse design as an explicit optimization over design variables). Below we outline their core ideas, typical algorithms, representative materials applications, and practical trade-offs.

\subsubsection{Exploration-based inverse design:}
Exploration-first approaches iteratively probe unknown design regions to discover candidates that satisfy performance constraints under sparse supervision. Reinforcement learning (RL) agents optimize sequences of design decisions by interacting with a simulator or learned environment; Monte Carlo Tree Search (MCTS) supplies strong look-ahead priors; and particle-swarm methods provide robust heuristic global search. For photonic device geometry, Hwang \emph{et~al.} combined an Advantage Actor–Critic (A2C) agent (IDEA) with a critic-value branch tree (CVBT) to diversify high-scoring candidate designs~\cite{Hwang2022_ASOC}. CASTING extends MCTS to continuous actions for materials discovery, improving exploration efficiency in high-dimensional spaces~\cite{Banik2023_NPJCM}, while MCTS has also accelerated polymer sequence design under astronomical combinatorics~\cite{Patra2020_Nanoscale}. When physics solvers are expensive or gradients are unavailable, population heuristics such as Particle Swarm Optimization (PSO) remain effective global explorers; notably, PSO coupled with self-consistent field theory (SCFT) has realized bulk copolymer morphologies meeting target patterns~\cite{Khadilkar2017_Macromolecules,Wang2018_SoftComput}. RL has likewise guided combinatorial chemistry to reach molecules with extreme property targets that confound distribution-learning models~\cite{Kim2024_ChemSci}. Strengths of exploration methods include label frugality, no need for bijective mappings, and natural diversity; typical limitations are high compute budgets (many simulator calls), reward shaping sensitivity, and difficulty enforcing hard constraints without additional mechanism design.

\subsubsection{Model-based inverse design:}
Model-based strategies learn data-driven or physics-informed maps that enable inference from desired properties back to candidate designs. A pragmatic pattern is \emph{forward–inverse coupling}: Liu \emph{et~al.} introduced a tandem neural architecture that mitigates non-uniqueness by training an inverse network cascaded with a forward predictor, so only property-consistent designs survive~\cite{Liu2018_ACSPhotonics}. Industrial alloy workflows have operationalized this idea: the ML Design System (MLDS) generates property$\to$composition (P2C) proposals that are validated by a more reliable composition$\to$property (C2P) forward model; iterations continue until target errors are met, enabling copper and aluminum alloy discoveries~\cite{Wang2019_NPJCM,Jiang2022_JMST}. 
Surrogate modeling is the other mainstay: random-forest or neural surrogates approximate expensive simulators and are inverted with classical optimizers. In Li-ion cathodes, a learned surrogate over synthesis descriptors enabled inverse ``retrosynthesis'' of processing conditions achieving high discharge capacity, validated experimentally~\cite{Liow2022_NanoEnergy}. Transfer learning further bolsters small-data surrogates; e.g., TLOpt leverages pretraining and then couples the surrogate to genetic and Bayesian optimizers to match target spectra in optical materials~\cite{Dong2021_CMS}. 
Finally, \emph{generative models} (VAEs, GANs, autoregressive flows) map low-dimensional latent variables to design manifolds, providing smooth spaces for search and conditional sampling. VAEs have been used to traverse continuous microstructure spaces to optimize mechanical response~\cite{Kim2021_MatDes}, and generative pipelines have supported inverse design of high-entropy refractory alloys~\cite{Debnath2021_JMI}. GAN-based inverse mappers have also produced morphing composite beams that realize prescribed shapes under actuation~\cite{Brzin2024_EAAI}. Model-based methods excel in sample efficiency and amortized inference once trained, but must explicitly manage non-uniqueness (e.g., via forward validators or conditional priors), distribution shift, and uncertainty quantification.

\subsubsection{Optimization-based inverse design:}
Here the inverse task is posed as optimizing one or more objectives (e.g., error to a target spectrum) over design variables, subject to constraints. \emph{Bayesian Optimization} (BO) offers strong sample efficiency by balancing exploration and exploitation with acquisition functions; in combination with graph deep-learning energy models, BO has led to experimentally realized superhard compounds and efficient exploration of hypothetical crystals~\cite{Zuo2021_MaterialsToday}. BO/active-learning loops have also delivered high-performance Mg--Mn alloys with few experiments~\cite{Mi2024_JMagAlloys}. Gaussian/Bayesian hybrids can tackle metamaterial inverse design with uncertainty-aware updates~\cite{Zheng2020_JAP}, and deep-learning Bayesian frameworks support attribute-driven molecular design under priors and constraints~\cite{Tagade2019_NPJCM}. 
\emph{Genetic algorithms} (GA) remain a strong baseline for discrete or constrained spaces and can be hybridized with surrogates; e.g., GA+NN co-optimization achieved efficient photonic device design with reduced data needs~\cite{Ren2021_PhotonicsResearch}. When full-wave or continuum physics is differentiable, \emph{topology optimization} (adjoint gradients) provides a powerful PDE-constrained inverse design tool widely adopted in photonics~\cite{Christiansen2021_JOSAB,Jensen2011_LPR}. Optimization-based schemes provide principled multi-objective trade-offs, constraint handling, and sample-efficient search, but depend on accurate surrogates/acquisitions, careful problem parameterization, and (for gradient methods) differentiable solvers and single-valued adjoints.

In practice, hybrid pipelines perform best: exploration agents query model-based surrogates (\emph{RL $\leftrightarrow$ surrogate}); generative latents are optimized with BO/GA (\emph{gen. model $\rightarrow$ optimizer}); and forward–inverse couplings screen P2C proposals through trusted C2P evaluators before simulation or experiment. Which path to prefer depends on data regime, simulator cost, constraints, and whether gradients are available. Table~\ref{tab:inv_compare} summarizes the trade-space.

\begin{table}[H]
\centering
\caption{Compact comparison of inverse-design strategies.}
\label{tab:inv_compare}
\setlength{\tabcolsep}{4pt}           % tighten horizontal padding
\renewcommand{\arraystretch}{1.12}    % subtle vertical compaction
\footnotesize                         % or \scriptsize if still wide
\begin{tabularx}{\linewidth}{
  >{\raggedright\arraybackslash}p{2.6cm}
  >{\raggedright\arraybackslash}X
  >{\raggedright\arraybackslash}X
  >{\raggedright\arraybackslash}X}
\toprule
\textbf{Strategy} & \textbf{Typical algorithms} & \textbf{Strengths} & \textbf{Common caveats} \\
\midrule
\emph{Exploration-based}
& RL (A2C/PPO + simulators); MCTS (discrete/continuous); PSO and other swarm heuristics
& Label-frugal; discovers diverse solutions; robust to one-to-many mappings
& Many simulator calls; reward-shaping sensitivity; constraint enforcement can be nontrivial \\
\addlinespace[2pt]
\emph{Model-based}
& Forward--inverse (tandem) nets; supervised surrogates (RF/NN); transfer learning; VAEs/GANs
& Sample-efficient; amortized inference; possible uncertainty handling via calibrated models
& Non-uniqueness requires forward validation; distribution shift; data curation and retraining loops \\
\addlinespace[2pt]
\emph{Optimization-based}
& Bayesian optimization / active learning; genetic algorithms; adjoint (topology) optimization
& Strong sample-efficiency (BO); handles constraints and multi-objective trade-offs; scalable with gradients
& Surrogate bias; local minima; need differentiable solvers/valid adjoints; parameterization sensitivity \\
\bottomrule
\end{tabularx}
\end{table}




\section{Modeling Background}\label{sec:modelling_background}

Inverse exploration relies on models as its inner loop: given candidate decisions $\vect{x}$, we must anticipate outcomes $\vect{y}$, and—crucially for design—quantify how confident we are in those predictions. Two complementary perspectives are standard. A \emph{deterministic} view learns a single mapping $f:\mathcal{X}\!\to\!\mathcal{Y}$ that returns point estimates (useful for speed and direct optimization). A \emph{probabilistic} view learns conditional distributions $p(\vect{y}\mid\vect{x})$ (and, when needed, posteriors over parameters or designs), making uncertainty an explicit object of reasoning. Both sit naturally within risk-minimization and decision-theoretic principles and will be used throughout our inverse workflows~\cite{Vapnik1998,Bishop2006,Murphy2012,HTF2009,Berger1985}.


\subsection{Learning as Risk Minimization}

Inverse design answers the query “which decision $\vect{x}$ is likely to realise the target outcome $\vect{y}^{\star}$?” We model an inverse map $g:\mathcal{Y}\!\to\!\mathcal{X}$ using data $\mathcal{D}=\{(\vect{y}_i,\vect{x}_i)\}_{i=1}^N$ and select $g$ by minimising expected loss in decision space,
\begin{equation}
  \mathcal{R}(g)
  \;=\;
  \mathbb{E}_{(\vect{y},\vect{x})}\!\left[\,
    \ell\!\big(g(\vect{y}),\,\vect{x}\big)
  \right],
  \label{eq:true_risk_new}
\end{equation}
where $\ell:\mathcal{X}\times\mathcal{X}\to\mathbb{R}_{\ge 0}$ measures how well the proposal $g(\vect{y})$ agrees with a known design $\vect{x}$ (e.g., an $\ell_2$ loss for continuous designs, or a structured loss if $\vect{x}$ includes categorical/process choices). Because $\mathcal{R}(g)$ depends on the unknown data-generating distribution, we use regularised empirical risk minimisation,
\begin{equation}
  \widehat{g}
  \;\in\;
  \arg\min_{g\in\mathcal{G}}
  \frac{1}{N}\sum_{i=1}^{N}\ell\!\left(g(\vect{y}_i),\,\vect{x}_i\right)
  \;+\;
  \lambda\,\Omega(g),
  \qquad \lambda\!\ge\!0,
  \label{eq:erm_new}
\end{equation}
with hypothesis class $\mathcal{G}$ (e.g., linear/kernels/NNs) and regulariser $\Omega$ (e.g., weight decay, sparsity, Lipschitz/Jacobian control) to improve generalisation and stabilise the inverse in the presence of non-uniqueness and noise.

A central challenge is that inverse maps are typically one-to-many: many $\vect{x}$ can induce the same $\vect{y}$. Purely supervising $g$ to reproduce a particular $\vect{x}_i$ for $\vect{y}_i$ can therefore penalise equally valid alternatives. When a forward model $\vect{f}:\mathcal{X}\!\to\!\mathcal{Y}$ is available (a simulator or a pretrained differentiable surrogate), we mitigate non-uniqueness by adding a forward-consistency term that rewards \emph{any} design whose forward response matches the target. A convenient hybrid objective is
\begin{equation}
  \widehat{g}
  \;\in\;
  \arg\min_{g\in\mathcal{G}}
  \frac{1}{N}\sum_{i=1}^{N}
  \Big[
    \alpha\,\ell\!\left(g(\vect{y}_i),\,\vect{x}_i\right)
    \;+\;
    (1-\alpha)\,\mathcal{L}\!\left(\vect{f}\!\big(g(\vect{y}_i)\big),\,\vect{y}_i\right)
  \Big]
  \;+\; \lambda\,\Omega(g),
  \qquad \alpha\in[0,1],
  \label{eq:tandem_new}
\end{equation}
where $\mathcal{L}:\mathcal{Y}\times\mathcal{Y}\to\mathbb{R}_{\ge 0}$ measures outcome mismatch (e.g., $\ell_2$ or a covariance-aware distance). Setting $\alpha\!\approx\!0$ yields a \emph{tandem/forward-validated} regime that trains $g$ through the forward model and accepts any valid preimage; choosing $\alpha\!>\!0$ additionally anchors $g$ near specific designs $\vect{x}_i$ (useful when particular recipes are preferred). If $\vect{f}$ is non-differentiable or expensive, one first trains a differentiable surrogate $\widehat{\vect{f}}$ and keeps it fixed while optimising $g$, replacing $\vect{f}$ by $\widehat{\vect{f}}$ in \eqref{eq:tandem_new}. For multi-objective outcomes with heterogeneous scales, $\mathcal{L}$ should be computed with normalisation or a metric that reflects task priorities to avoid dominance by any single objective.

\medskip
\emph{Illustrative use case (nanophotonic reflectance matching).}
Suppose $\vect{y}\in\mathbb{R}^{L}$ is a reflectance spectrum sampled at $L$ wavelengths and $\vect{x}\in\mathbb{R}^{n}$ are geometric/process parameters of a metasurface unit cell. We collect $\mathcal{D}=\{(\vect{y}_i,\vect{x}_i)\}$ via simulation. To learn $g(\vect{y})$, we take $\ell(\hat{\vect{x}},\vect{x})=\|\hat{\vect{x}}-\vect{x}\|_2^2$ and define $\mathcal{L}$ as the mean-squared spectrum error,
\[
  \mathcal{L}\!\left(\vect{f}(\hat{\vect{x}}),\,\vect{y}\right)
  \;=\;
  \frac{1}{L}\sum_{\ell=1}^{L}\big(f_\ell(\hat{\vect{x}})-y_\ell\big)^2,
\]
optionally after whitening or per-wavelength scaling if noise/importance varies across the band. We first train a forward surrogate $\widehat{\vect{f}}$ on $(\vect{x}_i,\vect{y}_i)$, freeze it, and then optimise $g$ with \eqref{eq:tandem_new} using $\widehat{\vect{f}}$ in place of $\vect{f}$. At inference, given a new target $\vect{y}^{\star}$, we compute $\hat{\vect{x}}=g(\vect{y}^{\star})$ as a fast, feasible proposal. If desired, we refine $\hat{\vect{x}}$ by a short local optimisation
\[
  \hat{\vect{x}}^{\,\text{ref}} \in \arg\min_{\vect{x}\in\mathcal{X}}
  \mathcal{L}\!\left(\widehat{\vect{f}}(\vect{x}),\,\vect{y}^{\star}\right)
  \quad \text{initialised at } \hat{\vect{x}},
\]
and verify it with the high-fidelity simulator. This workflow preserves speed (single forward pass through $g$), handles non-uniqueness (outcome-space fidelity), and retains control via regularisation and constraints in $\mathcal{X}$.

\subsection{Deterministic Modelling}\label{sec:deterministic_modelling}

In the deterministic setting, the inverse model $g:\mathcal{Y}\!\to\!\mathcal{X}$ returns a single design $\hat{\vect{x}}=g(\vect{y}^{\star})$ for any target $\vect{y}^{\star}$. Training uses data $\mathcal{D}=\{(\vect{y}_i,\vect{x}_i)\}_{i=1}^{N}$ and a regularised objective to promote stability and generalisation. A basic formulation treats inverse learning as empirical risk minimisation in decision space,
\begin{equation}
  \widehat{g}
  \;\in\;
  \arg\min_{g\in\mathcal{G}}
  \frac{1}{N}\sum_{i=1}^{N}\ell_{\mathcal{X}}\!\big(g(\vect{y}_i),\,\vect{x}_i\big)
  \;+\; \lambda\,\Omega(g),
  \label{eq:det_inv}
\end{equation}
where $\ell_{\mathcal{X}}$ penalises deviation from reference designs (e.g., $\|\hat{\vect{x}}-\vect{x}\|_2^2$ for continuous variables), $\Omega$ encodes inductive bias (e.g., weight decay, Jacobian control, early stopping), and $\lambda\!\ge\!0$ trades fit for complexity. This objective is effective when the mapping from $\vect{y}$ to a preferred $\vect{x}$ is essentially single-valued or when reproducing known recipes is desirable.

Inverse problems are often one-to-many, however, so it is natural to measure success in outcome space using a forward model $\vect{f}:\mathcal{X}\!\to\!\mathcal{Y}$ (a simulator or a frozen surrogate). A forward-validated or “tandem” objective trains $g$ so that its proposals \emph{produce} the targets:
\begin{equation}
  \widehat{g}
  \;\in\;
  \arg\min_{g\in\mathcal{G}}
  \frac{1}{N}\sum_{i=1}^{N}
  \ell_{\mathcal{Y}}\!\big(\,\vect{f}(g(\vect{y}_i))\,,\,\vect{y}_i\big)
  \;+\; \lambda\,\Omega(g),
  \label{eq:det_tandem}
\end{equation}
with $\ell_{\mathcal{Y}}$ an application-appropriate discrepancy in $\mathcal{Y}$ (e.g., scaled $\ell_2$, Mahalanobis, or a task metric). This objective naturally accommodates non-uniqueness by accepting any design whose forward response matches the target.

When one wishes to retain some preference for observed designs while enforcing outcome fidelity, a simple interpolation combines the two losses:
\begin{equation}
  \widehat{g}
  \;\in\;
  \arg\min_{g\in\mathcal{G}}
  \frac{1}{N}\sum_{i=1}^{N}
  \Big[
    \alpha\,\ell_{\mathcal{X}}\!\big(g(\vect{y}_i),\,\vect{x}_i\big)
    \;+\;
    (1-\alpha)\,\ell_{\mathcal{Y}}\!\big(\vect{f}(g(\vect{y}_i)),\,\vect{y}_i\big)
  \Big]
  \;+\; \lambda\,\Omega(g),
  \qquad \alpha\in[0,1].
  \label{eq:det_hybrid}
\end{equation}
Small $\alpha$ emphasises “any valid preimage” (useful under strong non-uniqueness); larger $\alpha$ softly anchors solutions near known designs (useful for manufacturability or established process windows). Hyperparameters (architecture, $\lambda$, and, if used, $\alpha$) are selected on validation data. At inference, a single forward pass yields $\hat{\vect{x}}=g(\vect{y}^{\star})$, which can be verified by evaluating $\vect{f}(\hat{\vect{x}})$ against $\vect{y}^{\star}$.

Deterministic training yields fast, reproducible proposals but collapses potentially multi-valued inverse relations to a point and does not natively quantify uncertainty or support diversity of candidates. In the broader framework of this thesis, these objectives are complemented by probabilistic models that learn conditional distributions (e.g., $p(\vect{x}\mid\vect{y})$ or $p(\vect{y}\mid\vect{x})$), enabling uncertainty estimates, calibrated scoring, and sampling-based exploration when multiple designs can realise the same target.


\subsection{MDN, CVAE, INN}
Write abou these estimators in details and how each one define the problem and what we are optimising, loss function, modeling distribution.

\subsection{Probabilistic Modelling}

The probabilistic view treats predictions as \emph{distributions} rather than single points, which makes uncertainty explicit and accommodates one-to-many inverse relationships. Inverse design can model either the forward conditional $p_\theta(\vect{y}\mid\vect{x})$ (often paired with a decision rule over $\vect{x}$) or the inverse conditional $p_\theta(\vect{x}\mid\vect{y})$ (directly proposing distributions over designs for a target). In both cases, parameters $\theta$ are learned by likelihood-based training with appropriate regularisation.

A forward conditional is fit by maximising the regularised conditional log-likelihood
\begin{equation}
  \widehat{\theta}
  \;\in\;
  \arg\max_{\theta}
  \bigg[
  \sum_{i=1}^{N}\log p_{\theta}\!\big(\vect{y}_i \mid \vect{x}_i\big)
  \;-\; \gamma\,\mathcal{R}(\theta)
  \bigg],
  \label{eq:prob_forward}
\end{equation}
after which inverse recommendations are obtained by searching or scoring designs against the predictive distribution (e.g., selecting $\tilde{\vect{x}}$ that maximises the probability of meeting a tolerance around a target $\vect{y}^{\star}$). An inverse conditional is fit analogously as
\begin{equation}
  \widehat{\theta}
  \;\in\;
  \arg\max_{\theta}
  \bigg[
  \sum_{i=1}^{N}\log p_{\theta}\!\big(\vect{x}_i \mid \vect{y}_i\big)
  \;-\; \gamma\,\mathcal{R}(\theta)
  \bigg],
  \label{eq:prob_inverse}
\end{equation}
and directly yields a distribution over feasible candidates for any query $\vect{y}^{\star}$.

Parametric density families include mixture models (e.g., mixture density networks that pair a neural network with a conditional Gaussian mixture), which offer closed-form likelihoods and capture multimodality in $p(\vect{x}\mid\vect{y})$. Latent-variable generative models (e.g., conditional variational autoencoders) express the conditional via an integral over a low-dimensional latent $\vect{z}$,
\begin{equation}
  p_\theta(\vect{x}\mid \vect{y})
  \;=\;
  \int p_\theta(\vect{x}\mid \vect{z},\vect{y})\,p_\theta(\vect{z}\mid \vect{y})\,\mathrm{d}\vect{z},
  \label{eq:cvae_factorisation}
\end{equation}
and are trained by maximising an evidence lower bound (ELBO) with an inference network $q_\phi(\vect{z}\mid \vect{x},\vect{y})$. Flow-based models use invertible transformations conditioned on $\vect{y}$ to obtain exact $\log$-likelihoods and flexible densities; diffusion-style models provide another route to sample diverse designs from a learned conditional score or noise process. These families trade off tractability (closed-form likelihoods) and flexibility (expressiveness of the conditional), and each can be regularised to encode smoothness or architectural priors.

Uncertainty is a first-class output under the probabilistic view. Aleatoric (data) uncertainty reflects intrinsic variability/noise in outcomes for a fixed design and is captured directly by the conditional variance. Epistemic (model) uncertainty reflects limited data or model misspecification; practical estimators include Bayesian neural networks and deep ensembles that approximate posterior predictive variability. For calibrated evaluation, strictly proper scoring rules such as negative log-likelihood (NLL) and the continuous ranked probability score (CRPS) simultaneously reward sharpness and calibration of predictive distributions.

Turning distributions into decisions can be done by sampling and ranking or by a Bayes decision rule. Given a target $\vect{y}^{\star}$ and an inverse conditional $p(\vect{x}\mid\vect{y}^{\star})$, draw candidates $\{\vect{x}^{(r)}\}_{r=1}^{R}$, evaluate their forward responses $\vect{f}(\vect{x}^{(r)})$, and select those with highest success probability within a task-specific tolerance, e.g.,
\[
\widehat{\vect{x}}
\in
\arg\max_{r\in\{1,\dots,R\}}
\Pr\!\Big[\,d\!\big(\vect{f}(\vect{x}^{(r)}),\vect{y}^{\star}\big)\le\epsilon\ \Big|\ \mathcal{D}\Big].
\]
Alternatively, define a utility $u(\tilde{\vect{x}},\vect{y};\,\vect{y}^{\star})$ that rewards target attainment (and optionally penalises constraint violation) and choose
\[
\tilde{\vect{x}}^{\star}
\in
\arg\max_{\tilde{\vect{x}}\in\mathcal{X}}
\mathbb{E}_{\vect{y}\sim p(\vect{y}\mid \tilde{\vect{x}},\mathcal{D})}\!\big[\,u(\tilde{\vect{x}},\vect{y};\,\vect{y}^{\star})\,\big],
\]
which connects probabilistic predictions to task-aligned, point-valued recommendations. In practice, the same pipeline supports diversity-aware design: draw multiple high-probability candidates, report their predictive success rates, and surface epistemic uncertainty to guide follow-up experiments or simulations where the model is least certain.


\paragraph{Aleatoric vs.\ epistemic uncertainty.}
Probabilistic models distinguish data (aleatoric) uncertainty from model (epistemic) uncertainty: the former reflects intrinsic variability/noise in $\vect{y}\mid\vect{x}$, the latter arises from limited data or misspecification. This distinction matters when ranking candidates and communicating confidence~\cite{Murphy2022PML1,Bishop2006PRML}.

\subsection{Inverse-centric evaluation metrics}

Evaluation reflects the inverse goal: \emph{propose decisions that realise a desired outcome $\vect{y}^{\star}$}. In this thesis, each method returns a single decision per target (e.g., any probabilistic output is collapsed to a point estimate such as a mean, median, or MAP), and we therefore assess the quality of that proposed decision, irrespective of how it was generated.

We first quantify how close a proposed decision $\widehat{\vect{x}}$ comes to the target by measuring a discrepancy in the outcome space with a task-appropriate distance $d$:
\begin{equation}
  \mathcal{E}_{\mathcal{Y}}(\widehat{\vect{x}},\vect{y}^{\star})
  \;=\;
  d\!\left(\,\vect{f}(\widehat{\vect{x}})\,,\,\vect{y}^{\star}\right),
  \label{eq:target_error_clean}
\end{equation}
where $\vect{f}$ denotes the forward map from decision to outcome. The choice of $d$ should match the semantics and scales of the outcomes. In many settings, the Euclidean distance (the $\ell_2$ norm) is a natural default, especially after standardising outcome dimensions; see, for example, \cite{BoydVandenberghe2004}. When correlations between outcome components matter, the Mahalanobis distance provides a principled alternative by incorporating the outcome covariance; see the original development in \cite{Mahalanobis1936} and a modern tutorial in \cite{DeMaesschalck2000}. In application-driven contexts, $d$ may be defined by an accepted domain metric; in all cases, the definition of $d$ and any normalisation or weighting should be stated to support interpretability and reproducibility.

To complement the continuous error, we also report a tolerance-based success measure. For each target, we record a binary success indicator that answers the question “did the single proposed decision meet the specification?”:
\begin{equation}
  \mathrm{succ}^{(m)}_\epsilon
  \;=\;
  \mathbb{I}\!\left[\,d\!\left(\vect{f}\!\big(\widehat{\vect{x}}^{(m)}\big),\,\vect{y}^{\star (m)}\right)\le \epsilon\,\right],
  \label{eq:succ_eps_single}
\end{equation}

The tolerance $\epsilon$ should reflect domain limits (e.g., engineering tolerances or user-acceptable error). When such limits differ by component, it is helpful to define $d$ using per-component scaling or to report $\mathrm{Succ}_\epsilon$ at several tolerances (e.g., tight and relaxed bands) for a more complete picture. Alongside aggregate success, summary statistics of $\mathcal{E}_{\mathcal{Y}}$ across targets (e.g., median and interquartile range) convey both typical performance and variability.


In sum, $\mathcal{E}_{\mathcal{Y}}$ captures \emph{how close} we get to the target, while $\mathrm{Succ}_\epsilon$ captures \emph{whether} we satisfy a practically meaningful tolerance. Together they provide a concise, implementation-agnostic summary of decision quality that is easy to interpret and straightforward to reproduce.
