%-------------------------------------------------------------------------------
% Discussion
%-------------------------------------------------------------------------------

\chapter{Discussion}\label{chap:discussion}

The experimental results in Chapter~\ref{chap:experiments} provide
evidence that inverse modelling can effectively map desired objectives to
plausible decision variables when trained on Pareto samples.  In this
chapter we reflect on these findings, analyse the strengths and
limitations of the proposed framework and highlight directions for
future research.

\section{Interpretation of Results}

The comparative evaluation demonstrates that Gaussian process regression
(GPR) consistently yields the most accurate inverse predictions across
diverse benchmark problems.  This aligns with the observation in
 inverse evolutionary algorithms that GPR captures both global structure
 and local irregularities of the inverse mapping~\cite{Farias2024}.
GPR’s ability to provide uncertainty estimates is particularly valuable
in the interactive setting: high predictive variance signals regions of
the objective space where the model has little support, complementing
the plausibility check.

Random forests (RF) emerge as a strong alternative.  They often produce
more diverse candidate sets and handle discontinuous or multi‑modal
inverse mappings better than smooth models.  The trade‑off is that RF
predictions can exhibit discontinuities and are less interpretable.
Radial basis functions (RBF) and neural networks (FFNN) underperform
when training data is limited but become competitive as the dataset
grows.  RBF networks are sensitive to hyperparameters such as the number
and placement of centres, while neural networks require careful
regularisation to avoid overfitting.

Data availability is a key factor.  As the training set size increases
from 100 to 500 samples, errors decrease substantially.  This suggests
that the cost of generating additional Pareto samples via forward
optimisation is amortised by the benefits of a more accurate inverse
model.  Transfer learning across tasks with common objectives further
 improves generalisation~\cite{Tan2023}, though caution is needed to
ensure that tasks are sufficiently related; otherwise negative transfer
may occur.

\section{Benefits and Applications}

The main advantage of the proposed framework is its separation of
training and exploration phases.  Computational effort is invested
offline to build the inverse and forward models; thereafter, users can
explore the objective space in real time without rerunning expensive
optimisation.  This capability is particularly beneficial in domains
where evaluating the forward map involves simulations or experiments
costing minutes or hours, such as structural design, aerodynamics,
computational chemistry or medical imaging.  Engineers and designers
gain greater insight into trade‑offs and can interactively assess how
changes in objectives affect the feasible design space.

The framework also supports educational and collaborative settings.  By
visualising the Pareto front and candidate decisions, stakeholders with
different expertise can discuss trade‑offs and converge on acceptable
solutions.  The plausibility check communicates uncertainty and
encourages critical thinking about whether a desired outcome is
realistic given the available data.

\section{Limitations}

Several limitations warrant discussion.  First, the inverse model is
only as good as the data it is trained on.  If the Pareto sample is
sparse or biased towards certain regions of the front, the model may
fail to capture other regions.  Active learning or adaptive sampling
could be used to focus optimisation runs on informative regions of the
objective space.

Second, the current implementation treats each decision variable
independently when training inverse models.  Although multioutput
regression could capture correlations among decision variables, it may
increase computational complexity.  Exploring structured output models
or deep generative approaches (e.g. variational autoencoders) could
yield more coherent predictions.

Third, our plausibility check is based on distance measures and model
uncertainty.  A more principled approach might involve conformal
 prediction or distributionally robust inverse optimisation~\cite{Dong2021},
which provide statistical guarantees on prediction validity.  Adapting
such methods to the multi‑objective setting is an interesting avenue.

Fourth, the current user interface is functional but simple.  It could
be enhanced with richer visualisations, such as 3‑D plots for
three‑objective problems, interactive trade‑off curves and sensitivity
analysis.  Integration with computer‑aided design software or immersive
environments could further improve usability.

Finally, the evaluation in this thesis relies on surrogate forward
models rather than ground‑truth simulations.  While surrogates provide
reasonable approximations, there is a risk that errors compound when
using an inverse model trained on approximate data and a forward model
for evaluation.  Future work could incorporate a budget of real
function evaluations to periodically validate predictions and update
models online.

\section{Future Directions}

The results invite several promising research directions:

\begin{itemize}
  \item \textbf{Hybrid inverse–forward training.}  Instead of training
    inverse and forward models separately, joint training could ensure
    consistency between predictions.  For example, inverse predictions
    could be fed into the forward model during training to minimise a
    cyclic reconstruction loss.
  \item \textbf{Active sampling.}  Incorporating active learning into
    the MOO phase could generate samples that maximise information gain
    for the inverse model.  Acquisition functions based on inverse model
    uncertainty or expected improvement could guide where to sample next.
  \item \textbf{Generative models.}  Deep generative models such as
    mixture density networks~\cite{Bishop1994}, tandem architectures for
    inverse design~\cite{Liu2018}, conditional variational autoencoders
    (e.g. MGCVAE~\cite{Lee2022MGCVAE}) and semi‑supervised generative models
    for metamaterials~\cite{Ma2019} could learn a joint distribution over
    decision and objective variables and allow sampling of diverse feasible
    solutions conditioned on target objectives.  Generative models have
    been successfully applied to design nano‑photonic structures and
    high‑entropy alloys【34486591917118†L8-L16】【492130352176691†L214-L223】 and
    offer a natural way to handle multimodal inverse mappings and
    uncertainty.
  \item \textbf{Human–in‑the‑loop optimisation.}  Integrating user
    preferences into the inverse mapping process could personalise
    recommendations.  Preference learning or Bayesian optimisation over
    subjective utility functions could be combined with the inverse
    surrogate to suggest designs that align with individual priorities.
  \item \textbf{Robustness and safety.}  Distributionally robust inverse
    optimisation frameworks~\cite{Dong2021} could be adapted to
    account for uncertainty in the objective functions, ensuring that
    recommended designs perform well even under perturbations or model
    misspecification.
\end{itemize}

Addressing these directions would further enhance the capability and
reliability of inverse decision mapping methods.