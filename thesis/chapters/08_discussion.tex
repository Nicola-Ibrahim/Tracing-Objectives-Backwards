%-------------------------------------------------------------------------------
% Discussion
%-------------------------------------------------------------------------------

\chapter{Discussion}\label{chap:discussion}

This chapter interprets the empirical results, states the overall conclusions, and outlines targeted recommendations for subsequent research and deployment.

\section{Interpreting the Findings}

Across the COCO/bbob-biobj benchmarks and the real-world signal-processing case, the central result is that an inverse mapping learned from representative $(\mathbf{x},\mathbf{y})$ pairs can reliably translate a user target $\mathbf{y}^{\ast}$ into a decision $\widehat{\mathbf{x}}$ whose forward evaluation $\mathbf{f}(\widehat{\mathbf{x}})$ closely tracks the target. This statement is supported by the primary indicators—outcome-space error $\mathcal{E}_{\mathcal{Y}}$ and tolerance success $\mathrm{Succ}_{\epsilon}$—reported per problem and per dataset split. The consistency of these indicators across instances suggests that inverse mapping is a viable route to target-conditioned design, provided the training archive is representative of the reachable region.

Three patterns recur. First, \emph{problem structure matters}: separable and gently conditioned biobjective problems tend to yield lower $\mathcal{E}_{\mathcal{Y}}$ and higher $\mathrm{Succ}_{\epsilon}$ than highly multimodal combinations, reflecting the increased complexity of the inverse relation. This aligns with established observations in the COCO suite regarding how separability, conditioning, and multimodality shape optimisation difficulty, which in turn affects any learned surrogate built from those samples. 

Second, \emph{data sufficiency is consequential}: increasing the number of training pairs reduces median error and narrows the IQR, indicating both improved accuracy and stability. This is consistent with expectations for supervised models whose bias–variance trade-off benefits from broader coverage of the objective space.

Third, \emph{metric choice influences interpretation}: when outcomes are measured on heterogeneous scales or exhibit correlation, Mahalanobis distance offers a more faithful notion of “closeness” than raw Euclidean distance, as it accounts for covariance structure estimated on held-out data; this is standard practice in multivariate assessment. 

Finally, the cross-problem ranking with nonparametric tests (Friedman with Nemenyi post-hoc) provides an aggregate view of which modelling choices are consistently strong versus problem-specific, following recommended procedures for multi-dataset comparisons. 

\section{Implications}

Methodologically, the results validate a \emph{target-attainment} evaluation stance for inverse design: reporting $\mathcal{E}_{\mathcal{Y}}$ (how close to the target) together with $\mathrm{Succ}_{\epsilon}$ (meets a meaningful tolerance) yields compact, decision-oriented evidence. Practically, this enables interactive workflows: users nominate $\mathbf{y}^{\ast}$, obtain one or several proposals, and immediately see whether the proposals land within pre-defined tolerances. For benchmarking, coupling inverse metrics with standard COCO post-processing (e.g., reference fronts, archive handling) makes results comparable and reproducible across instances and function groups. :contentReference[oaicite:3]{index=3}

\section{Limitations}

Three limitations deserve emphasis. (i) \emph{Archive representativeness}: if training pairs undersample parts of the reachable set, the learned inverse will underperform on targets drawn from those regions. (ii) \emph{Forward-model reliance}: when a surrogate $\widehat{\mathbf{f}}$ is used for verification, evaluation quality is bounded by surrogate fidelity; periodic spot-checks against the true simulator would mitigate compounding errors. (iii) \emph{Metric dependence}: conclusions can shift with the chosen distance and scaling; reporting both Euclidean and covariance-aware distances (and documenting the covariance estimate) reduces ambiguity. :contentReference[oaicite:4]{index=4}

\section{Future Work}

Building on these findings, we outline concrete, study-aligned directions:

\begin{itemize}
  \item \textbf{Coverage-aware reporting.} Augment $\mathcal{E}_{\mathcal{Y}}$ and $\mathrm{Succ}_{\epsilon}$ with conformal prediction sets in outcome space (split-conformal on residuals of $\mathbf{f}(g(\cdot))$), yielding finite-sample coverage guarantees for “achieved outcome within a calibrated ball around $\mathbf{y}^{\ast}$.” This adds a statistically principled reliability layer without re-training the inverse model. :contentReference[oaicite:5]{index=5}
  \item \textbf{Instance-generalisation on COCO.} Evaluate cross-instance transfer (train on a subset of instances, test on held-out instances) and cross-suite robustness (e.g., varying dimension), leveraging COCO’s standardised instance machinery and post-processing tools for rigorous comparison. :contentReference[oaicite:6]{index=6}
  \item \textbf{Distance calibration and scaling.} Systematise the choice between Euclidean and Mahalanobis distances via validated scaling protocols (e.g., covariance from a calibration split), and report sensitivity analyses so practitioners can match $d(\cdot,\cdot)$ to domain semantics. :contentReference[oaicite:7]{index=7}
  \item \textbf{Real-data validation loops.} For the signal-processing case, schedule small, periodic batches of ground-truth forward evaluations to (a) check surrogate drift, and (b) re-calibrate tolerances. This operationalises reliability without altering your core modelling approach.
  \item \textbf{Benchmarking alignment.} Complement inverse-specific indicators with COCO’s native performance summaries (e.g., archive-based reference sets) when comparing pipelines, improving external validity and reusability of results. :contentReference[oaicite:8]{index=8}
\end{itemize}

\section{Conclusion}

Learning an inverse map from outcomes to decisions is a practical and effective way to support target-driven design. On bbob-biobj functions and a real signal-processing dataset, the approach achieves low outcome-space errors and high tolerance success when trained on representative archives, with clear benefits from sufficient data and carefully chosen outcome metrics. The proposed reporting protocol---pairing $\mathcal{E}_{\mathcal{Y}}$ with $\mathrm{Succ}_{\epsilon}$ and documenting scaling---is simple to reproduce and to interpret. By adding coverage-aware reporting, testing cross-instance generalisation, and tightening metric calibration, future studies can make inverse decision mapping even more dependable and transferable.
