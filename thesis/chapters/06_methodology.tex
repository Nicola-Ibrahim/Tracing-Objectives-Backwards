%-------------------------------------------------------------------------------
% Methodology
%-------------------------------------------------------------------------------

\chapter{Methodology}\label{chap:method}

Inverse decision mapping becomes practically useful when it supports interactive design exploration: a user specifies a target outcome $\vect{y}^{\star}$ and the method proposes candidate decisions $\widehat{\vect{x}}\in\calX$ that plausibly realise that target. The Introduction framed this task as inherently ambiguous and sometimes infeasible, and posed research questions about (i) constructing an outcome $\!\to\!$ decision mechanism, (ii) defining attainment in a reproducible way, and (iii) understanding which modelling choices generalise to unseen targets. The methodology presented here turns those requirements into a concrete protocol centered on probabilistic candidate generation followed by explicit outcome-space evaluation.

The core stance is to learn an amortised inverse model that represents the inverse relation as a conditional distribution rather than as a single-valued map. A probabilistic inverse supports one-to-many structure by generating multiple candidates for the same target, enabling exploration rather than forcing an arbitrary point estimate. This conditional-density viewpoint is widely used in amortised inference and simulation-based inference, where offline training yields fast query-time sampling. \cite{Papamakarios2021,Cranmer2020SBI} In this thesis, evaluation is treated as a distinct stage that scores and ranks candidates using an explicit discrepancy in outcome space, computed via the best available outcome evaluator.

Three design principles shape the chapter and the experiments that follow:
\begin{itemize}
  \item Candidate-based inversion: inverse querying produces a set of candidates by sampling from a learned conditional model, making ambiguity visible through diversity rather than suppressing it by construction. \cite{Cranmer2020SBI}
  \item Decoupled evaluation: learning is performed from paired examples without requiring an explicit forward operator; attainment is assessed after training by scoring candidates with a task-defined outcome discrepancy and an outcome evaluator that may be a simulator, a surrogate, a measurement routine, or an offline mapping available only for evaluation. \cite{Arridge2019,Ongie2020}
  \item Comparable reporting: experiments fix a common candidate budget and a common scoring rule so that comparisons reflect modelling choices rather than interface differences or ad hoc evaluation. \cite{Arridge2019,Ongie2020}
\end{itemize}

The following sections formalise the probabilistic inverse model and its training objective, define the query-time candidate generation procedure, and specify how candidates are scored and ranked in outcome space under an explicit discrepancy measure. Subsequent methodological components (constraints, feasibility handling, and any optional refinement procedures) are introduced only as needed for reproducible implementation and evaluation across datasets and benchmarks.


%-------------------------------------------------------------------------------
\section{Theoretical framework}\label{sec:theory_model}
%-------------------------------------------------------------------------------

The methodology treats inverse design as learning a conditional model over decisions given outcomes. Rather than approximating a single inverse function, we learn a probabilistic inverse $q_\theta(\vect{x}\mid\vect{y})$ that can represent multi-modality and generate multiple candidates for the same target. This conditional-density framing supports amortised querying: training fits a reusable conditional model from paired examples, and inference reduces to sampling conditioned on the target. \cite{Papamakarios2021,Cranmer2020SBI}

Given a representative dataset $\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^{N}$, model parameters are fit by conditional likelihood:
\begin{equation}
\widehat{\theta}\in\arg\min_{\theta}\frac{1}{N}\sum_{i=1}^{N}
\Big[-\log q_\theta(\vect{x}_i\mid\vect{y}_i)\Big] + \lambda\,\Omega(\theta),
\label{eq:cond_mle_inverse}
\end{equation}
where $\Omega(\theta)$ denotes regularisation. This training objective depends only on paired data and does not assume access to a forward operator during learning, which keeps the inverse model applicable across settings with different levels of forward-model availability. \cite{Arridge2019,Ongie2020}

At query time, the inverse model produces a candidate set by sampling:
\begin{equation}
  \widehat{\vect{x}}^{(k)} \sim q_\theta(\vect{x}\mid\vect{y}^\star),\qquad k=1,\dots,K.
  \label{eq:candidate_generation}
\end{equation}
Candidates are then assessed in outcome space using a task-defined discrepancy $d(\cdot,\cdot)$ together with an outcome evaluator $\mathcal{E}:\calX\to\calY$ that represents the best available mechanism for assessing outcomes. Depending on the setting, $\mathcal{E}$ may be a simulator, a learned surrogate, a measurement routine, or an offline mapping used only for evaluation. \cite{Arridge2019,Ongie2020} For a target $\vect{y}^\star$, each candidate receives a score
\begin{equation}
  r^{(k)}(\vect{y}^\star) \;=\; d\!\big(\mathcal{E}(\widehat{\vect{x}}^{(k)}),\,\vect{y}^\star\big),
  \qquad k=1,\dots,K,
  \label{eq:candidate_scores}
\end{equation}
which induces a ranking and supports two usage modes: exploratory use returns the ranked set (or a top subset), while deployment can optionally select a single decision as the best-aligned candidate under the declared discrepancy and evaluator. Fixing $K$, $\mathcal{E}$, and $d$ at evaluation time makes attainment statements traceable to an explicit scoring rule rather than to implicit model behaviour. \cite{Arridge2019,Ongie2020}


%-------------------------------------------------------------------------------
\section{Methodology selection and defence}\label{sec:method_defence}
%-------------------------------------------------------------------------------

The research questions call for an inverse querying mechanism that (i) respects non-uniqueness by producing meaningful alternatives, (ii) answers queries quickly enough for interactive use, and (iii) supports reproducible attainment assessment in outcome space. These requirements motivate an inverse-mapping methodology built around probabilistic candidate generation: most computation occurs offline by fitting $q_\theta(\vect{x}\mid\vect{y})$ via \eqref{eq:cond_mle_inverse}, and each new target is answered by sampling candidates via \eqref{eq:candidate_generation}. This amortised design is consistent with established practice in learned inverse problems and simulation-based inference, where conditional density estimation is used to make repeated inverse queries efficient. \cite{Arridge2019,Ongie2020,Cranmer2020SBI}

A second choice is to separate learning from evaluation. The inverse model is trained from paired examples, while attainment is assessed after training by scoring candidates with an explicit outcome discrepancy under a stated evaluator, as in \eqref{eq:candidate_scores}. This decoupling is methodologically useful because forward-model access varies by dataset and domain: some settings provide a trusted simulator, others support only surrogate or measurement-based assessment, and some benchmarks permit evaluation through recorded outcomes alone. Stating the evaluator and discrepancy explicitly ties conclusions to a declared notion of attainment and supports comparable reporting across experiments. \cite{Arridge2019,Ongie2020}

Finally, optimisation-at-query-time remains complementary. Black-box search procedures, including Bayesian optimisation, can refine a candidate when evaluations are affordable, but they increase query-time cost and introduce additional tuning and search design choices. The default methodology therefore prioritises amortised candidate generation with post-hoc scoring, while treating query-time optimisation as an optional refinement stage when deployment constraints permit. \cite{Shahriari2016BayesOpt}



%-------------------------------------------------------------------------------
\section{Methodological framework}\label{sec:framework}
%-------------------------------------------------------------------------------

From this point onward the chapter shifts from motivation to execution. The goal is to make
inverse querying behave like a practical routine that can be run repeatedly and compared
fairly across model families. The workflow follows a simple pattern: learn an inverse proposal
model, generate multiple candidates for each new target, and then score those
candidates in outcome space using an explicit discrepancy rule. In this way, ambiguity is
handled by generation, while attainment is handled by checking under a declared evaluator
and metric rather than by relying on implicit properties of the inverse model. 

\begin{figure}[!htbp]
  \centering
  \begin{adjustbox}{width=\linewidth}
      \input{figures/framework}
  \end{adjustbox}
  \caption{Propose--check workflow used throughout the thesis. An inverse model generates a candidate set for a target outcome, and an explicit evaluation stage scores and ranks candidates using a stated outcome evaluator and discrepancy.}
  \label{fig:framework}
\end{figure}

The starting point is a conditional distribution over decisions given outcomes,
$q_{\theta}(\vect{x}\mid\vect{y})$. Once this model is trained, a user query $\vect{y}^{\star}$ is
answered by drawing a fixed-size set of candidates:
\begin{equation}
  \widehat{\vect{x}}^{(k)} \sim q_{\widehat{\theta}}(\vect{x}\mid\vect{y}^{\star}),
  \qquad k=1,\dots,K .
  \label{eq:candidate_generation}
\end{equation}
We keep the candidate budget $K$ fixed across experiments so the user-facing interface stays
the same regardless of the chosen model family, and so comparisons reflect modelling choices
rather than differences in output conventions.

Candidate generation alone is not sufficient, because the thesis evaluates success in outcome
space. After sampling, each candidate is therefore checked by computing its outcome through
the best available outcome evaluator $\mathcal{E}:\mathcal{X}\to\mathcal{Y}$ and scoring it with a
task-defined discrepancy $d(\cdot,\cdot)$:
\begin{equation}
  r^{(k)}(\vect{y}^{\star}) \;=\;
  d\!\big(\mathcal{E}(\widehat{\vect{x}}^{(k)}),\,\vect{y}^{\star}\big),
  \qquad k=1,\dots,K .
  \label{eq:candidate_scoring}
\end{equation}
The evaluator $\mathcal{E}$ is intentionally defined broadly. Depending on the dataset and the
deployment context, it may be an explicit simulator, a measurement routine, a learned
surrogate trained for checking, or an offline mapping used only for evaluation. The method
returns candidates ranked by $r^{(k)}(\vect{y}^{\star})$; when a single deployed configuration is
required, it is taken as the best-ranked candidate under the declared scoring rule. 



%-------------------------------------------------------------------------------
\section{Inverse-model training}\label{sec:training}
%-------------------------------------------------------------------------------

With the propose--check interface fixed in Section~\ref{sec:framework}, implementation starts by fitting the candidate generator from paired examples
$\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^{N}$. The goal is to learn a conditional distribution
$q_{\theta}(\vect{x}\mid\vect{y})$ that assigns high probability to decisions that are consistent with the observed decision--outcome relationship, while remaining expressive enough to represent one-to-many structure. Chapter~3 motivates the three conditional families used in this thesis (MDN, CVAE, INN) and derives their probabilistic forms; here we restate the corresponding training objectives in the form used for experiments and implementation.

Across all models, parameter estimation follows the same principle: maximise conditional fit to the observed pairs while controlling capacity through regularisation. In practice, we minimise the regularised negative conditional log-likelihood,
\begin{equation}
  \widehat{\theta}\in\arg\min_{\theta}\;
  \frac{1}{N}\sum_{i=1}^{N}\Big[-\log q_{\theta}(\vect{x}_i\mid\vect{y}_i)\Big]
  \;+\;\lambda\,\Omega(\theta),
  \label{eq:cond_mle_inverse}
\end{equation}
where $\Omega(\theta)$ collects stabilising terms such as weight decay and any model-specific penalties. This fitting step uses only paired data; the outcome evaluator $\mathcal{E}$ and discrepancy $d$ are introduced later and only enter through the candidate scoring rule in Section~\ref{sec:framework}.

The three instantiations below differ in how they parameterise $q_{\theta}(\vect{x}\mid\vect{y})$ and how the log-density in \eqref{eq:cond_mle_inverse} is computed or approximated.

Mixture Density Network (MDN).
The MDN represents the conditional density as a finite Gaussian mixture with outcome-dependent parameters (see Chapter~3, Eq.~(3.14)--(3.15)):
\begin{equation}
  q_{\theta}(\vect{x}\mid\vect{y}) =
  \sum_{j=1}^{M}\pi_{\theta,j}(\vect{y})\,
  \mathcal{N}\!\big(\vect{x};\,\boldsymbol{\mu}_{\theta,j}(\vect{y}),\,\boldsymbol{\Sigma}_{\theta,j}(\vect{y})\big),
  \qquad
  \sum_{j=1}^{M}\pi_{\theta,j}(\vect{y})=1,\ \ \pi_{\theta,j}(\vect{y})\ge 0 .
  \label{eq:mdn_density}
\end{equation}
Training substitutes \eqref{eq:mdn_density} into \eqref{eq:cond_mle_inverse} and optimises the resulting mixture negative log-likelihood. The implementation follows standard constraints for mixture weights and covariance parameterisation, as detailed in the Modeling Background.

Conditional Variational Autoencoder (CVAE).
The CVAE introduces latent variables to represent conditional variability and trains by maximising a variational lower bound on $\log q_{\theta}(\vect{x}\mid\vect{y})$ (see Chapter~3, Eq.~(3.16)--(3.17)). Using an encoder $q_{\phi}(\vect{z}\mid\vect{x},\vect{y})$ and a decoder $p_{\theta}(\vect{x}\mid\vect{y},\vect{z})$, the objective becomes
\begin{equation}
  \widehat{\theta},\widehat{\phi}\in\arg\min_{\theta,\phi}\;
  -\frac{1}{N}\sum_{i=1}^{N}\Big[
    \mathbb{E}_{q_{\phi}(\vect{z}\mid\vect{x}_i,\vect{y}_i)}\log p_{\theta}(\vect{x}_i\mid\vect{y}_i,\vect{z})
    - \mathrm{KL}\!\big(q_{\phi}(\vect{z}\mid\vect{x}_i,\vect{y}_i)\,\|\,p(\vect{z})\big)
  \Big]
  +\lambda\,\Omega(\theta,\phi),
  \label{eq:cvae_elbo}
\end{equation}
which is the negative ELBO plus regularisation. This keeps the training criterion tractable while still targeting a conditional generative model whose latent variation can produce diverse candidates for a fixed $\vect{y}^{\star}$.

Invertible Neural Network (INN).
The INN implements a conditional density through an invertible transformation between a simple reference latent and the decision, conditioned on the outcome target. Using a conditional invertible map $T_{\theta}(\cdot;\vect{y})$,
\begin{equation}
  \vect{x} = T_{\theta}(\vect{z};\vect{y}),\qquad \vect{z}\sim \mathcal{N}(\mathbf{0},\mathbf{I}),
  \label{eq:inn_forward_map}
\end{equation}
the conditional log-density follows from a change-of-variables construction (see the Modeling Background for the invertibility setup; the implementation uses the standard log-determinant form):
\begin{equation}
  \log q_{\theta}(\vect{x}\mid\vect{y}) =
  \log p(\vect{z}) +
  \log\left|\det \frac{\partial T_{\theta}^{-1}(\vect{x};\vect{y})}{\partial \vect{x}}\right|,
  \qquad
  \vect{z}=T_{\theta}^{-1}(\vect{x};\vect{y}).
  \label{eq:inn_change_of_vars}
\end{equation}
Substituting \eqref{eq:inn_change_of_vars} into \eqref{eq:cond_mle_inverse} yields a direct likelihood-based training objective. This preserves an explicit density model while maintaining a sampling mechanism that is compatible with the fixed candidate budget $K$ used throughout the experiments.

In summary, MDN, CVAE, and INN all realise the same training goal---a conditional generator $q_{\theta}(\vect{x}\mid\vect{y})$ fitted from paired data---but they differ in how multimodality is represented (mixture components, latent-variable decoding, or invertible latent transport). Downstream evaluation then treats their sampled candidates uniformly via the shared scoring rule in \eqref{eq:candidate_scoring}, so empirical comparisons reflect differences in the learned conditional model rather than differences in the evaluation interface.


%-------------------------------------------------------------------------------
\section{Evaluation plan}\label{sec:evaluation}
%-------------------------------------------------------------------------------

The inverse models in this thesis return a candidate set for each target rather than a single point estimate. Evaluation therefore answers a concrete question: given a target $\vect{y}^{\star}$ and candidates $\{\widehat{\vect{x}}^{(k)}\}_{k=1}^{K}$, which candidates most closely satisfy the objective under a stated checking mechanism, and how often does at least one candidate meet a specification-level requirement. This section fixes that checking mechanism so that results remain comparable across model families and datasets.

Evaluation is performed through an outcome evaluator $\mathcal{E}:\mathcal{X}\to\mathcal{Y}$, which represents the best available way to assess outcomes for proposed decisions. Depending on the setting, $\mathcal{E}$ may be a trusted forward simulator, an experimental measurement routine, or a surrogate model used purely for checking when an explicit forward model is not available. 

\subsection{Outcome-space checking and ranking}\label{sec:eval_checking}

For each candidate, we compute an outcome-space discrepancy score relative to the user target:
\begin{equation}
  r^{(k)}(\vect{y}^{\star})
  \;=\;
  d\!\big(\mathcal{E}(\widehat{\vect{x}}^{(k)}),\,\vect{y}^{\star}\big),
  \qquad k=1,\dots,K,
  \label{eq:candidate_scoring}
\end{equation}
and rank candidates by increasing $r^{(k)}(\vect{y}^{\star})$. This score is the main evaluation primitive: it is the mechanism by which the forward model or surrogate contributes to validation, and it is the basis for all reported attainment metrics.

To summarise performance in a way that respects the one-to-many nature of the inverse mapping, we report the best-achieved discrepancy within the candidate set:
\begin{equation}
  \mathcal{E}_{\mathcal{Y}}^{(K)}(\vect{y}^{\star})
  \;=\;
  \min_{k\in\{1,\dots,K\}}
  d\!\big(\mathcal{E}(\widehat{\vect{x}}^{(k)}),\,\vect{y}^{\star}\big).
  \label{eq:best_of_k_error}
\end{equation}
When qualitative exploration is the goal, the ranked list itself is retained; when a single deployed decision is required, it is chosen as the top-ranked candidate under \eqref{eq:candidate_scoring}. Using $\min_k$ as the reporting convention keeps comparisons consistent while still crediting methods that generate useful diversity.

The discrepancy $d$ is selected to match the outcome semantics and scaling. After standardising outcomes, Euclidean distance provides a transparent default. When outcome dimensions are correlated or have heterogeneous noise, a Mahalanobis discrepancy incorporates covariance structure:
\begin{equation}
  d_{\mathrm{Mah}}(\vect{u},\vect{v})
  \;=\;
  \sqrt{(\vect{u}-\vect{v})^{\top}\boldsymbol{\Sigma}^{-1}(\vect{u}-\vect{v})},
  \label{eq:mahalanobis_distance}
\end{equation}
with $\boldsymbol{\Sigma}$ estimated from a reference split of outcomes. \cite{Bishop2006PRML}

\subsection{Tolerance-based attainment}\label{sec:eval_tolerance}

Outcome-space distances are informative, but many applications ultimately require a specification-level statement: whether the method can produce at least one candidate within an acceptable tolerance. Using the same discrepancy as in \eqref{eq:candidate_scoring}, we report
\begin{equation}
  \mathrm{Succ}^{(K)}_{\epsilon}(\vect{y}^{\star})
  \;=\;
  \mathbb{I}\!\left[\,
    \min_{k\in\{1,\dots,K\}}
    d\!\big(\mathcal{E}(\widehat{\vect{x}}^{(k)}),\,\vect{y}^{\star}\big)
    \le \epsilon
  \right],
  \label{eq:succ_eps_bestofk}
\end{equation}
where $\epsilon$ is set by domain tolerances or by a declared experimental specification. Reporting both $\mathcal{E}_{\mathcal{Y}}^{(K)}$ and $\mathrm{Succ}^{(K)}_{\epsilon}$ separates continuous closeness from compliance with a target specification.

\subsection{Operational-domain validator (ODD) for constraint-aware evaluation}\label{sec:eval_odd}

Discrepancy alone does not capture all evaluation needs. In practice, users often impose additional constraints beyond matching $\vect{y}^{\star}$, such as admissible operating regions, safety limits, or validity requirements tied to the training distribution. To make these requirements explicit, we include an operational-domain validator, denoted ODD, that acts as an additional evaluation mechanism.

The ODD produces an in-domain score or acceptance decision for a candidate outcome (and, if needed, for a candidate decision):
\begin{equation}
  s_{\psi}^{(k)} \;=\; \mathrm{ODD}_{\psi}\!\big(\widehat{\vect{x}}^{(k)},\,\mathcal{E}(\widehat{\vect{x}}^{(k)})\big),
  \qquad k=1,\dots,K,
  \label{eq:odd_score}
\end{equation}
where larger $s_{\psi}^{(k)}$ indicates better conformity to the operational domain. In machine-learning terms, this corresponds to out-of-distribution or novelty detection: the validator is trained beforehand to capture the data distribution (and any user-declared admissibility structure), so that candidates outside the supported regime can be flagged or rejected. \cite{Lu2025OODSurvey}

In evaluation, we combine target attainment and operational-domain validity by filtering or re-ranking. A simple filtering rule keeps only candidates that satisfy a validator threshold $\tau$ and any explicit user constraints encoded as a predicate $c(\cdot)\le 0$ in outcome space:
\begin{equation}
  \mathcal{K}_{\mathrm{valid}}(\vect{y}^{\star})
  \;=\;
  \Big\{\,k:\ s_{\psi}^{(k)}\ge\tau\ \ \text{and}\ \ c\!\big(\mathcal{E}(\widehat{\vect{x}}^{(k)})\big)\le 0 \Big\}.
  \label{eq:valid_set}
\end{equation}
Ranking is then applied within $\mathcal{K}_{\mathrm{valid}}(\vect{y}^{\star})$ using the same discrepancy score \eqref{eq:candidate_scoring}. If no candidate passes the validator, the method reports this explicitly and the query is treated as not attained under the declared operational constraints. This makes failure modes interpretable: the method may fail because candidates do not match the target, because they violate user constraints, or because they fall outside the learned operational domain.

The threshold $\tau$ is selected in advance on held-out data to control the strictness of the validator under the training regime. This keeps ODD decisions reproducible and prevents post hoc threshold tuning that would compromise comparability across experiments. The exact ODD model class used (e.g., one-class classification, energy-based scoring, or density-based scoring) is treated as part of the evaluation configuration and is reported alongside the discrepancy definition. \cite{Liu2020EnergyOOD}


%-------------------------------------------------------------------------------
\section{Pipelines}\label{sec:pipelines}
%-------------------------------------------------------------------------------

\subsection{Training pipeline}\label{sec:train_pipeline}

Model training follows a repeatable routine that keeps preprocessing and model selection consistent
across problems. Algorithm~\ref{alg:train_pipeline} provides a compact description. In short, we
compute fixed normalisation transforms, split the data for validation-based selection, and fit the
chosen inverse model by minimising its training loss. Hyperparameters are selected under a fixed
search budget using a validation criterion. The trained model and preprocessing transforms are then
frozen and used unchanged in the candidate-generation and assessment steps described next.

\begin{algorithm}[!htbp]
  \caption{Training and model selection for inverse candidate generation}\label{alg:train_pipeline}
  \begin{algorithmic}[1]
    \Require Paired data $\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^{N}$
    \Require Hyperparameter search space $\mathcal{H}$ and search budget $B$
    \Ensure Best fitted inverse model $\widehat{q}$ and preprocessing transforms $\mathcal{T}$

    \State $\mathcal{T} \leftarrow \text{compute\_transforms}(\mathcal{D})$ \Comment{Scaling for $\vect{x}$ and $\vect{y}$}
    \State $\mathcal{D}_{T}, \mathcal{D}_{V} \leftarrow \text{split}(\mathcal{T}(\mathcal{D}))$ \Comment{Train and validation splits}
    \State $S_{\mathrm{best}} \leftarrow +\infty$
    
    \For{$b=1$ to $B$}
      \State Sample configuration $h_b \in \mathcal{H}$ 
      \State Initialize parameters $\theta_b$ according to $h_b$
      \While{stopping criterion not met}
        \State $\theta_b \leftarrow \text{minimize } \mathcal{L}(\theta_b; \mathcal{D}_{T})$
        \State Monitor validation score $S_b$ on $\mathcal{D}_{V}$
      \EndWhile
      \If{$S_b < S_{\mathrm{best}}$}
        \State $S_{\mathrm{best}} \leftarrow S_b$
        \State $(h_{\mathrm{best}}, \theta_{\mathrm{best}}) \leftarrow (h_b, \theta_b)$
      \EndIf
    \EndFor
    
    \State $\widehat{q} \leftarrow \text{refit}(h_{\mathrm{best}}, \mathcal{D}_{T} \cup \mathcal{D}_{V})$ \Comment{Optional: final fit on all data}
    \State \Return $\widehat{q}, \mathcal{T}$
  \end{algorithmic}
\end{algorithm}


\subsection{Querying pipeline}\label{sec:query_pipeline}

Given a target outcome $\vect{y}^{\star}$, the inverse model generates a fixed-size candidate set and
the assessment suite assigns scores used to rank (and, if configured, filter) candidates.
Algorithm~\ref{alg:query_pipeline} summarises this target-driven loop. The same routine is used for
experimental evaluation on held-out targets and for interactive exploration, ensuring that reported
performance reflects a consistent candidate-generation and assessment interface.

\begin{algorithm}[!htbp]
  \caption{Target-driven querying and candidate assessment}\label{alg:query_pipeline}
  \begin{algorithmic}[1]
    \Require Trained inverse model $\widehat{q}$, target outcome $\vect{y}^{\star}$, candidate budget $K$
    \Require Assessment Suite $\mathcal{A}$ (includes Evaluators, ODD, and constraints)
    \Ensure Ranked candidate list $\mathcal{R}$

    \Repeat
      \State Obtain target outcome $\vect{y}^{\star}$
      \State Generate candidates $\{\widehat{\vect{x}}^{(k)}\}_{k=1}^{K} \sim \widehat{q}(\vect{x}\mid\vect{y}^{\star})$
      
      \For{$k=1$ to $K$}
        \State $\widehat{\vect{y}}^{(k)} \leftarrow \text{Evaluate}(\widehat{\vect{x}}^{(k)})$ \Comment{Using surrogate or ground truth}
        \State $S^{(k)} \leftarrow \text{Assess}(\widehat{\vect{x}}^{(k)}, \widehat{\vect{y}}^{(k)}, \vect{y}^{\star})$ \Comment{Discrepancy, ODD, and validity}
      \EndFor
      
      \State $\mathcal{R} \leftarrow \text{RankCandidates}(\{\widehat{\vect{x}}^{(k)}\}, \{S^{(k)}\})$ 
      \State \Return top subset of $\mathcal{R}$
    \Until{the exploration session terminates}
  \end{algorithmic}
\end{algorithm}
