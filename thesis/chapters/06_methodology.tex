%-------------------------------------------------------------------------------
% Methodology
%-------------------------------------------------------------------------------

\chapter{Methodology}\label{chap:method}


This chapter develops and defends the research design for learning an inverse map that takes a target outcome $\vect{y}^{\star}$ and proposes decision(s) $\widehat{\vect{x}}$ expected to realise it under the forward process $\vect{f}$. The approach is modular and agnostic to the eventual dataset: representative pairs $(\vect{x},\vect{y})$ may come from archives, curated sweeps, or standard benchmarks.


We pursue an inverse–mapping approach: learn a function that takes a target outcome $\vect{y}^{\star}$ and returns a single decision $\widehat{\vect{x}}$ expected to realise it under the forward process $\vect{f}$. The emphasis is on a transparent, reproducible recipe that turns representative examples $(\vect{x},\vect{y})$ into a model usable for real-time inverse queries, without prescribing a specific data shape in advance.

Our methodological stance is guided by three principles. First, we treat inverse design as learning an outcome$\rightarrow$decision map that is \emph{explicitly aligned} with the forward behaviour of the system: training encourages forward consistency so that $\vect{f}(\widehat{\vect{x}})$ tracks the target\footnote{Forward-consistency or tandem training helps mitigate non-uniqueness in inverse problems by validating inverse proposals through a forward model.}. Second, we keep outputs uniform across models by reducing any distributional predictions to one representative decision (e.g., mean or median) for evaluation and use. Third, we separate model-specific choices (architectures, priors, loss terms) from evaluation choices (distances and tolerances), so each can be justified on its own merits.

Concretely, the chapter develops: (i) the learning objective for the inverse map, including outcome normalisation and a forward-consistency term that couples inverse predictions to $\vect{f}$; (ii) model classes considered (deterministic regressors and probabilistic generators with point selection), together with selection rules that yield a single $\widehat{\vect{x}}$ per query; (iii) mechanisms for handling constraints in $\mathcal{X}$ (e.g., feasibility penalties or projections); (iv) the query-time procedure that turns $\vect{y}^{\star}$ into $\widehat{\vect{x}}$ and optionally performs a lightweight forward check for verification or refinement; and (v) the evaluation protocol that reports outcome-space discrepancy and a tolerance-based success indicator, matching the inverse goal of attaining $\vect{y}^{\star}$ while remaining simple to interpret and reproduce.

The result is a modular, data-shape-agnostic workflow: learn an inverse map from representative pairs, align it with the forward process during training, produce one decision per target at query time, and evaluate performance with task-appropriate outcome distances and tolerances.

\section{Problem setup and formal statement}\label{sec:setup}

Let $\mathcal{X}\subset\mathbb{R}^{n}$ denote the set of feasible decisions with vectors $\mathbf{x}\in\mathcal{X}$, and let $\mathcal{Y}\subset\mathbb{R}^{m}$ denote the space of observable outcomes. A forward process $\mathbf{f}:\mathcal{X}\to\mathcal{Y}$ maps any decision to its outcome, $\mathbf{y}=\mathbf{f}(\mathbf{x})$. We assume access to a dataset
\[
  \mathcal{D}=\{(\mathbf{x}_i,\mathbf{y}_i)\}_{i=1}^{N},\qquad
  \mathbf{x}_i\in\mathcal{X},\ \ \mathbf{y}_i=\mathbf{f}(\mathbf{x}_i),
\]
assembled from historical runs, simulation sweeps, or exploratory optimisation.

Given a user-specified target outcome $\mathbf{y}^{\ast}\in\mathcal{Y}$, the goal is to infer a decision $\widehat{\mathbf{x}}\in\mathcal{X}$ that realises the target under the forward process:
\[
  \mathbf{f}(\widehat{\mathbf{x}})\ \approx\ \mathbf{y}^{\ast}.
\]
We focus on single-decision outputs for each query, meaning that for every $\mathbf{y}^{\ast}$ the method returns one representative $\widehat{\mathbf{x}}$. When exact equality is unattainable, the aim is to produce a best-effort decision relative to the task at hand. This formulation accommodates both data-driven and physics-based forward maps and allows for standard extensions such as constraints, regularisation, or prior knowledge within $\mathcal{X}$, without committing to a particular modelling choice here.


\section{Research questions and design principles}\label{sec:rq_design}
We address three questions: (i) how to learn an outcome$\!\to\!$decision map aligned with the forward behaviour of the system; (ii) how to return either one decision or a small set of proposals on demand; and (iii) how to evaluate attainment of $\vect{y}^{\star}$ in outcome space in a simple, reproducible way. The design follows three principles: \emph{forward consistency} (validate inverse predictions through $\vect{f}$), \emph{output uniformity} (define a clear selection policy for single or multiple proposals), and \emph{clean separation} between modelling choices (architectures, priors, loss terms) and evaluation choices (distances and tolerances).


%-------------------------------------------------------------------------------
\section{Theoretical framework and model}\label{sec:theory_model}
%-------------------------------------------------------------------------------

We formalise inverse design as learning $g_\theta:\mathcal{Y}\!\to\!\mathcal{X}$ with a
hybrid objective that balances decision-space supervision and forward outcome
fidelity:
\begin{equation}
\widehat{\theta}\in\arg\min_{\theta}\frac{1}{N}\sum_{i=1}^{N}\Big[
  \ell_{\mathcal{X}}(g_\theta(\vect{y}_i),\vect{x}_i)
  + \alpha\, d\!\big(\vect{f}(g_\theta(\vect{y}_i)),\vect{y}_i\big)^2
\Big] + \lambda\,\Omega(\theta),
\label{eq:hybrid_inverse}
\end{equation}
with $d$ an outcome-space metric and $\alpha\!\ge\!0$ controlling forward
consistency (the ``tandem'' idea). For distributional models, we fit
$p_\theta(\vect{x}\mid\vect{y})$ by conditional likelihood and apply a selection
policy to return a single $\widehat{\vect{x}}$ at query time; conformal calibration
(\S\ref{sec:calibration}) uses split residuals in outcome space to provide
distribution-free coverage.

%-------------------------------------------------------------------------------
\section{Methodology selection and defence}\label{sec:method_defence}
%-------------------------------------------------------------------------------

We adopt an \emph{inverse–mapping} methodology (rather than running a
black-box optimiser at query-time) to enable amortised, real-time proposals from
diverse archives while aligning predictions with the forward map. Forward
consistency mitigates non-uniqueness; outcome-side evaluation matches the design
goal; and conformal calibration adds simple, finite-sample guarantees without
restrictive distributional assumptions. Optimisation-at-query-time (e.g., BO)
is complementary but incurs additional search and simulator calls; we therefore
treat it as an optional refinement stage rather than the default.


\section{Methodological framework}\label{sec:framework}

We consider two complementary inverse families for $\mathcal{Y}\!\to\!\mathcal{X}$.
\begin{enumerate}
  \item Deterministic inverse models $g_{\theta}:\mathcal{Y}\!\to\!\mathcal{X}$
  learned by empirical risk minimisation (ERM) with capacity control and
  constraint handling (projection or penalties).

  \item Probabilistic inverse models $p_{\theta}(\vect{x}\mid\vect{y})$ trained by
  conditional maximum likelihood in expressive families (e.g., CVAEs or
  normalising flows), equipped with a \emph{selection policy} that returns a
  single decision (MAP/mean/median) or a small set of $k$ proposals at query
  time. For diversity-aware sets, we optionally use a determinantal point
  process (DPP) subset selector over candidates.
\end{enumerate}
To reduce one-to-many ambiguity, both families may include a
\emph{forward-consistency} term that encourages
$\vect{f}(g_{\theta}(\vect{y}))\!\approx\!\vect{y}$ during training.
\section{Inverse-model training}\label{sec:training}

\subsection{Deterministic inverse ERM}\label{sec:det_inverse}
We estimate $g_\theta$ by minimising a supervised loss in decision space with regularisation:
\begin{equation}
  \widehat{\theta}\;\in\;\arg\min_{\theta}\;
  \frac{1}{N}\sum_{i=1}^{N}\ell_{\mathcal{X}}\!\Big(g_{\theta}(\mathbf{y}_i),\,\mathbf{x}_i\Big)
  \;+\;\lambda\,\Omega(\theta).
  \label{eq:erm_inverse}
\end{equation}
Here $\ell_{\mathcal{X}}$ measures deviation in $\mathcal{X}$ (e.g., scaled $\ell_2^2$), while $\Omega(\theta)$ (e.g., weight decay or sparsity) improves generalisation and numerical stability in ill-posed settings.

\paragraph{Constraint-aware training (projection/penalties).}
If decisions must satisfy $g_j(\vect{x})\!\le\!0$ and $h_k(\vect{x})\!=\!0$, we use projection when $\mathcal{X}$ is convex and closed, or exterior penalties otherwise:
\begin{equation}
  \widehat{\theta}\;\in\;\arg\min_{\theta}\;\frac{1}{N}\sum_{i=1}^{N}\!\Big[
    \ell_{\mathcal{X}}\!\big(\,\Pi_{\mathcal{X}}\{g_{\theta}(\mathbf{y}_i)\},\,\mathbf{x}_i\big)
    \;+\;\beta\!\sum_j [\,g_j(g_{\theta}(\mathbf{y}_i))\,]_+^{2}
    \;+\;\beta'\!\sum_k h_k(g_{\theta}(\mathbf{y}_i))^{2}\Big]
  \;+\;\lambda\,\Omega(\theta),
  \label{eq:erm_constraints}
\end{equation}
where $[\cdot]_+=\max\{\cdot,0\}$; squared-hinge and quadratic terms are standard, keeping optimisation unconstrained.

\paragraph{Forward-consistency (optional).}
To bias toward decisions whose forward responses match observed outcomes, we add an outcome-space penalty:
\begin{equation}
  \widehat{\theta}\;\in\;\arg\min_{\theta}\;
  \frac{1}{N}\sum_{i=1}^{N}\!\Big[
    \ell_{\mathcal{X}}\!\big(g_{\theta}(\mathbf{y}_i),\,\mathbf{x}_i\big)
    \;+\;\alpha\,d\!\big(\mathbf{f}(g_{\theta}(\mathbf{y}_i)),\,\mathbf{y}_i\big)^{2}
  \Big]\;+\;\lambda\,\Omega(\theta),
  \qquad \alpha\!\ge\!0,
  \label{eq:forward_consistency}
\end{equation}
with $d$ an outcome-space distance (defined in \S\ref{sec:evaluation}). This “tandem” idea reduces non-uniqueness by validating inverse predictions through $\vect{f}$.

\subsection{Probabilistic inverse training}\label{sec:prob_inverse}

We model the inverse relation as a conditional distribution $p_{\theta}(\mathbf{x}\mid\mathbf{y})$ and fit it by conditional maximum likelihood:
\begin{equation}
  \widehat{\theta}\in\arg\max_{\theta}\;\sum_{i=1}^{N}\log p_{\theta}(\mathbf{x}_i\mid\mathbf{y}_i)\;-\;\gamma\,\mathcal{R}(\theta),
  \label{eq:cll_inverse}
\end{equation}
where $\mathcal{R}(\theta)$ regularises the density model (e.g., VAE/flow terms or weight decay). Expressive conditional families such as CVAEs and normalizing flows are standard choices for multi–modal $p(\mathbf{x}\mid\mathbf{y})$. 

At inference, we expose a \emph{selection policy} $\pi$ that maps the learned conditional to either a single decision or a set of $k$ proposals, depending on user intent:
\[
  \widehat{\mathcal{X}}_{\pi}(\mathbf{y}^{\ast};k)\;=\;S_{\pi}\!\left(p_{\widehat{\theta}}(\mathbf{x}\mid\mathbf{y}^{\ast}),\,k\right).
\]
Typical instantiations are:
(i) \emph{MAP:} $S_{\text{MAP}}(\cdot,1)=\arg\max_{\mathbf{x}}p_{\widehat{\theta}}(\mathbf{x}\mid\mathbf{y}^{\ast})$;
(ii) \emph{risk–optimal summaries:} $S_{\text{mean}}(\cdot,1)=\mathbb{E}[\mathbf{x}\mid\mathbf{y}^{\ast}]$ or a coordinate-wise median (squared/absolute–loss Bayes rules); \cite{Bishop2006PRML}
(iii) \emph{i.i.d.\ sampling:} $S_{\text{sample}}$ draws $k$ independent $\mathbf{x}^{(r)}\!\sim p_{\widehat{\theta}}(\cdot\mid\mathbf{y}^{\ast})$;
(iv) \emph{diversity–aware sets:} draw $L\!>\!k$ candidates then select a diverse subset, e.g., via a determinantal point process (DPP) kernel on decision space, yielding $\arg\max_{|\mathcal{S}|=k}\det K(\mathcal{S})$. \cite{KuleszaTaskar2012DPP}
Policies (iii)–(iv) support multi-solution exploration when the inverse is intrinsically one-to-many; (i)–(ii) provide single, reproducible decisions when required.


\section{Evaluation plan}\label{sec:evaluation}
We evaluate the decision quality with outcome-space discrepancy and a specification-level success indicator. Unless otherwise noted, when multiple proposals are returned ($k\!>\!1$) for exploration, we report metrics on the \emph{top-ranked} candidate under the chosen policy to keep comparisons consistent.

\subsection{forward estimator evaluation}
Write about the use of the forward estimator

\subsection{Outcome-space error}
\begin{equation}
  \mathcal{E}_{\mathcal{Y}}(\widehat{\mathbf{x}},\mathbf{y}^{\ast})
  \;=\;
  d\!\big(\,\mathbf{f}(\widehat{\mathbf{x}}),\,\mathbf{y}^{\ast}\big),
  \qquad
  d\in\{\ \|\cdot\|_2\,,\ \text{Mahalanobis}\ \}.
  \label{eq:target_error}
\end{equation}
Euclidean distance is a natural default after standardising outcome scales; Mahalanobis distance incorporates outcome covariance and is preferable when dimensions have different units or correlations.

\subsection{Tolerance-based success}
For a binary, specification-level view we report
\begin{equation}
  \mathrm{Succ}_\epsilon(\widehat{\mathbf{x}},\mathbf{y}^{\ast})
  \;=\;
  \mathbb{I}\!\left[\, d\!\big(\mathbf{f}(\widehat{\mathbf{x}}),\, \mathbf{y}^{\ast}\big) \le \epsilon \right],
  \label{eq:succ_eps_single}
\end{equation}
with $\epsilon$ set by domain tolerances. Together, $\mathcal{E}_{\mathcal{Y}}$ (closeness) and $\mathrm{Succ}_\epsilon$ (meets spec) offer a concise, reproducible summary.

\section{Calibration procedures}\label{sec:calibration}
To attach coverage-style guarantees to the forward-validated outcomes, we use split conformal calibration on a held-out calibration set.

\paragraph{SplitConformalL2.}
Define residuals $r_i=\|\mathbf{f}(g_{\widehat{\theta}}(\mathbf{y}_i))-\mathbf{y}_i\|_2$ on calibration data and form
\begin{equation}
  \mathcal{C}_{\alpha}(\mathbf{y}^{\ast})=\{\mathbf{y}:\ \|\mathbf{y}-\mathbf{y}^{\ast}\|_2\le q_{1-\alpha}\},
  \label{eq:split_conformal_l2}
\end{equation}
where $q_{1-\alpha}$ is the $(1-\alpha)$ empirical quantile of $\{r_i\}$.

\paragraph{SplitConformal–Mahalanobis.}
With covariance $\mathbf{\Sigma}$ estimated from calibration outcomes and $\Delta(\mathbf{y},\mathbf{y}^{\ast})=\sqrt{(\mathbf{y}-\mathbf{y}^{\ast})^{\top}\mathbf{\Sigma}^{-1}(\mathbf{y}-\mathbf{y}^{\ast})}$,
\begin{equation}
  \mathcal{C}^{\mathrm{Mah}}_{\alpha}(\mathbf{y}^{\ast})=\{\mathbf{y}:\ \Delta(\mathbf{y},\mathbf{y}^{\ast})\le q^{\mathrm{Mah}}_{1-\alpha}\}.
  \label{eq:mahalanobis_conformal}
\end{equation}
We test whether $\mathbf{f}(\widehat{\mathbf{x}})\in\mathcal{C}_{\alpha}(\mathbf{y}^{\ast})$ or $\mathcal{C}^{\mathrm{Mah}}_{\alpha}(\mathbf{y}^{\ast})$; by construction, these sets deliver finite-sample marginal coverage under exchangeability.

\section{Data sources and ethics}\label{sec:data_ethics}
We use benchmarked or archival pairs $(\vect{x},\vect{y})$ originating from simulators or standard suites (e.g., COCO/BBOB for black-box optimisation) as well as domain archives when available. For each source, we document provenance, licensing, storage, and disposal. No personally identifiable or sensitive data are involved.

\section{Methods conclusion}\label{sec:methods_conclusion}
In sum, the methodology learns an outcome$\!\to\!$decision mapping aligned with forward behaviour, supports either a single decision or a small set of proposals at query time, and evaluates attainment of $\vect{y}^{\star}$ with transparent outcome-space criteria. Calibration procedures provide specification-level guarantees for forward-validated outcomes.









\section{Offline Phase: Data Generation and Model Training}

\subsection{Generation of Pareto Samples}

The first step is to obtain a representative set of Pareto‑optimal or
near‑optimal solutions.  Any suitable multi‑objective optimisation
algorithm can be employed; in our experiments we use the Non‑dominated
Sorting Genetic Algorithm II (NSGA‑II) and the Multi‑Objective
Evolutionary Algorithm based on Decomposition (MOEA/D) as baselines.  The
resulting archive \(\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^N\)
contains decision vectors \(\vect{x}_i\in\calX\) and their objective
values \(\vect{y}_i=\vect{f}(\vect{x}_i)\).  To ensure diversity and
coverage of the Pareto front the optimisation is run with standard
population sizes and termination criteria, and the union of final
non‑dominated solutions over multiple runs is stored.

\subsection{Data Preprocessing}

Prior to model training we normalise the objective vectors to lie in
\([0,1]^m\) using min–max scaling.  This step ensures that objectives
with different units or ranges contribute equally to the inverse model.
Similarly, decision variables are scaled to \([0,1]^n\) to avoid biasing
the regressors.  Outliers are removed using interquartile range
thresholding.  The processed dataset is then randomly split into
training, validation and test sets.

\subsection{Inverse Model Training}

We consider several modelling strategies for the inverse mapping
\(\hat{g}\colon\calY\to\calX\):


\paragraph{Feed‑Forward Neural Networks (FFNN).}  A multi‑layer perceptron
with fully connected layers and ReLU activations can approximate complex
nonlinear mappings.  We select the network architecture using
hyperparameter search and train using Adam optimisation with early
stopping.  Data augmentation and regularisation (dropout and weight
decay) improve generalisation.

\paragraph{Transfer Learning and Multi‑Source Data.}  When the available
dataset is small, transfer learning can leverage data from related tasks.
Tan et~al. propose learning across common objective spaces to mitigate
data scarcity and demonstrate that combining multiple Pareto fronts can
  improve inverse model generalisation~\cite{Tan2023}.  We adopt
this idea by pooling Pareto samples from benchmark problems with similar
objective definitions during training.

\subsection{Model Selection and Validation}

For each modelling strategy we perform hyperparameter tuning via
k‑fold cross‑validation on the training set.  The loss function combines
the mean squared error between predicted and true decision variables and a
penalty on predicted values outside the feasible range.  Models are
compared using performance metrics such as root mean squared error
(RMSE), coefficient of determination \(R^2\) and the generational
distance between the reconstructed Pareto front and the true front.  We
select the model with the best validation performance and retain it for
the online phase.

\subsection{Forward Surrogate Model}

In addition to the inverse model we train a surrogate forward model
\(\hat{f}\colon\calX\to\calY\) on the same data.  This model is used
during the online phase to rapidly evaluate candidate decisions without
incurring the cost of running the underlying simulation or experiment.  A
Gaussian process is employed due to its good performance on small data;
alternatively, a neural network or polynomial regression can be used.

\section{Online Phase: Interactive Exploration}

During the online phase the trained inverse and forward models are used
to support interactive design exploration.  The process comprises the
following steps:

\paragraph{Target specification.}  The user specifies a desired objective
vector \(\vect{y}^\star\) either numerically or by selecting a point in a
visualisation of the objective space.  Input widgets allow users to
adjust individual objectives and observe how the target moves relative
to the sampled Pareto front.

\paragraph{Plausibility check.}  The system computes the distance
\(d(\vect{y}^\star, \mathcal{D})\) to the nearest sample in the objective
space and derives a plausibility score.  If the score is below a
user‑defined threshold, a warning is displayed indicating that the
desired trade‑off may not be achievable.

\paragraph{Inverse prediction.}  The inverse model \(\hat{g}\) is queried
with \(\vect{y}^\star\) to produce a set of candidate decisions
\(\{\hat{\vect{x}}^{(k)}\}_{k=1}^K\).  For probabilistic models such as
GPR we draw multiple samples from the posterior distribution; for
deterministic models such as RBFs and FFNNs we sample by adding small
Gaussian noise to the input.

\paragraph{Forward evaluation.}  Each candidate is evaluated using the
forward surrogate \(\hat{f}\) to compute the predicted objective vector
\(\hat{\vect{y}}^{(k)}=\hat{f}(\hat{\vect{x}}^{(k)})\).  The forward
error \(e_f^{(k)}=\|\hat{\vect{y}}^{(k)}-\vect{y}^\star\|\) and distance
to the sampled Pareto front are computed.

\paragraph{Ranking and selection.}  Candidates are ranked according to a
multi‑criteria score combining forward error, diversity and plausibility.
The top‑\(M\) candidates are presented to the user along with their
predicted objective values and a visual indication of their proximity to
the Pareto front.

\paragraph{User feedback.}  The user can select a candidate and, if
desired, refine the target or adjust weights in the ranking.  Selected
candidates can be saved for further simulation or fabrication.  User
interactions can be logged to adapt the inverse model over time.


\section{Training Pipeline}

Given a dataset \(\mathcal{D}\) we perform the following steps for each
problem:

\begin{enumerate}
  \item \textbf{Normalisation.}  Compute the minimum and maximum of each
    objective and decision dimension and scale the data to \([0,1]\).
  \item \textbf{Train–validation split.}  Use an 80/20 split for
    training and validation.  When data is scarce we employ a leave‑one‑out
    scheme.
  \item \textbf{Hyperparameter search.}  For each inverse model type
    (GPR, RBF, RF, FFNN) perform 50 random trials over predefined
    hyperparameter ranges.  For GPR we search over squared exponential
    kernels with length scales in \([0.1,10]\); for RBF we vary the
    number of centres between 10 and 200; for RF we vary the number of
    trees (50–300) and maximum depth (3–20); for FFNN we vary the number
    of hidden layers (1–3) and neurons (50–200).
  \item \textbf{Model training.}  Fit each candidate model on the
    training data and evaluate performance on the validation set using
    RMSE and generational distance.  Select the best hyperparameter
    configuration.
  \item \textbf{Forward surrogate.}  Train a Gaussian process regression
    model on the same training data to act as the forward surrogate.  A
    Matérn kernel is used with noise estimated via maximum likelihood.
  \item \textbf{Testing.}  Evaluate the selected inverse and forward
    models on the held‑out test data.  Report RMSE, \(R^2\) and the
    proportion of predictions within a tolerance of the true decision
    values.
\end{enumerate}



\section{Algorithmic Summary}

Algorithm~\ref{alg:offline} summarises the offline training procedure and
Algorithm~\ref{alg:online} details the online exploration loop.  The
pseudocode abstracts away implementation details to highlight the main
steps.

\begin{algorithm}[h]
  \caption{Offline Training of Inverse and Forward Models}
  \label{alg:offline}
  \begin{algorithmic}[1]
    \Require multi‑objective function \(\vect{f}\), decision space \(\calX\)
    \Ensure trained inverse model \(\hat{g}\) and forward surrogate \(\hat{f}\)
    \State \textbf{Generate Pareto samples} \(\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}\)
       by running a MOO algorithm (e.g. NSGA‑II)
    \State \textbf{Normalise} decision and objective vectors
    \State \textbf{Split} \(\mathcal{D}\) into training/validation/test sets
    \State \textbf{Select model type} (GP, RBF, RF, FFNN)
    \State \textbf{Tune hyperparameters} via cross‑validation
    \State \textbf{Train inverse model} \(\hat{g}\) on training set
    \State \textbf{Train forward model} \(\hat{f}\) on training set
    \State \textbf{Validate} on test set; select best model
    \State \Return \(\hat{g},\hat{f}\)
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
  \caption{Online Inverse Decision Exploration}
  \label{alg:online}
  \begin{algorithmic}[1]
    \Require trained models \(\hat{g},\hat{f}\), dataset \(\mathcal{D}\)
    \Repeat
      \State \textbf{Get target} objectives \(\vect{y}^\star\) from user
      \State \textbf{Compute plausibility score} using distance to \(\mathcal{D}\)
      \If{score $<\tau$}
        \State display warning and optionally project \(\vect{y}^\star\) onto front
      \EndIf
      \State \textbf{Generate candidates} \(\{\hat{\vect{x}}^{(k)}\}\) via \(\hat{g}\)
      \For{each candidate \(k\)}
        \State evaluate \(\hat{\vect{y}}^{(k)}=\hat{f}(\hat{\vect{x}}^{(k)})\)
        \State compute forward error and distance metrics
      \EndFor
      \State \textbf{Rank} candidates by error, diversity and plausibility
      \State \textbf{Display} top candidates and solicit user feedback
    \Until{user terminates}
  \end{algorithmic}
\end{algorithm}
