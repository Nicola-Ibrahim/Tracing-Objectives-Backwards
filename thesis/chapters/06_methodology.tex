%-------------------------------------------------------------------------------
% Methodology
%-------------------------------------------------------------------------------

\chapter{Methodology}\label{chap:method}

Real-time inverse querying only becomes useful when it behaves like a reliable design instrument: a user specifies a target outcome $\vect{y}^{\star}$, and the method returns decision(s) $\widehat{\vect{x}}\in\calX$ expected to satisfy the design intent under the best available representation of the system behaviour. The Introduction framed this as inverse decision mapping under ambiguity and potential infeasibility; the research questions then ask for (i) a practical outcome $\!\to\!$ decision mechanism, (ii) a clear and reproducible notion of attainment in outcome space, and (iii) evidence about which modelling choices generalise to unseen targets. The methodology below turns these requirements into an executable, comparable protocol.

The core stance is to use machine-learning models as fast proposal mechanisms, while grounding checking and assessment in a forward description of the system when it is available, or in a learned surrogate that approximates it when it is not. This mirrors a common pattern in learned inverse problems: the inverse network accelerates querying, and a forward-consistency check anchors correctness to the task definition rather than to the internal calibration of the inverse model itself.  The protocol is modular with respect to data sources: it assumes only paired examples $(\vect{x},\vect{y})$ that are representative of the deployment regime, without prescribing how those pairs were generated.

Three design principles shape the chapter and the experiments that follow:
\begin{itemize}
  \item Forward alignment: training objectives encourage proposals whose predicted outcomes track the conditioning target, using forward-consistency ideas that remain applicable whether the forward behaviour is given explicitly or approximated by a surrogate trained from data. 
  \item Output uniformity: all models, including distributional predictors, are reduced to a single reported decision via a fixed selection rule so that comparisons reflect modelling differences rather than interface differences.
  \item Separation of concerns: modelling choices (architectures, priors, loss terms, sampling) are specified independently from evaluation choices (discrepancies, tolerances, success criteria), supporting transparent reporting and reproducible comparison.
\end{itemize}

The following sections formalise the learning objective for the inverse map (including outcome normalisation and an alignment term that couples inverse proposals to predicted outcomes), specify the model families considered (deterministic regressors and probabilistic generators together with a point-selection policy), describe how feasibility in $\calX$ is handled, and define the query-time procedure that maps $\vect{y}^{\star}$ to $\widehat{\vect{x}}$ with an optional consistency check using either the available forward model or its surrogate approximation. The chapter closes by fixing an evaluation protocol in outcome space so every reported result answers the same operational question: how often does a returned decision attain the user’s target under a clearly stated checking rule?


%-------------------------------------------------------------------------------
\section{Theoretical framework}\label{sec:theory_model}
%-------------------------------------------------------------------------------

The methodology treats inverse design as learning a conditional model over decisions given outcomes. Rather than approximating a single inverse function, we learn a probabilistic inverse
$q_\theta(\vect{x}\mid\vect{y})$ that can represent multi-modality and generate multiple candidates for the same target. This conditional-density viewpoint is standard in amortised inference: training uses paired examples to fit a reusable inverse mechanism, and query-time inference becomes fast sampling from the learned conditional. \cite{Papamakarios2021}

Given a representative dataset $\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^{N}$, the inverse model parameters are fit by conditional likelihood:
\begin{equation}
\widehat{\theta}\in\arg\min_{\theta}\frac{1}{N}\sum_{i=1}^{N}
\Big[-\log q_\theta(\vect{x}_i\mid\vect{y}_i)\Big] + \lambda\,\Omega(\theta),
\label{eq:cond_mle_inverse}
\end{equation}
where $\Omega(\theta)$ denotes model regularisation. This objective makes the training stage independent of any explicit forward operator: it learns the inverse relationship directly from paired examples, while leaving all checking and attainment claims to a separate evaluation layer.

At query time, the inverse model produces a small candidate set by sampling:
\begin{equation}
  \widehat{\vect{x}}^{(k)} \sim q_\theta(\vect{x}\mid\vect{y}^\star),\qquad k=1,\dots,K.
  \label{eq:candidate_generation}
\end{equation}
To keep outputs uniform across experiments and model families, we fix a common candidate budget $K$ and treat inverse querying as candidate generation followed by outcome-space scoring. Each sampled candidate is assigned a discrepancy score using a task-defined metric $d(\cdot,\cdot)$ and an outcome evaluator $\mathcal{E}:\calX\to\calY$ that represents the best available mechanism for assessing outcomes. Depending on the setting, $\mathcal{E}$ may be a simulator, a learned surrogate, a measurement routine, or an offline mapping available only for evaluation. For a target $\vect{y}^\star$, we compute
\begin{equation}
  r^{(k)}(\vect{y}^\star) \;=\; d\!\big(\mathcal{E}(\widehat{\vect{x}}^{(k)}),\,\vect{y}^\star\big),
  \qquad k=1,\dots,K,
  \label{eq:candidate_scores}
\end{equation}
and use these scores to rank candidates or to identify a subset that best aligns with the target. When an application requires a single deployed decision, this becomes a special case that selects the top-ranked candidate; otherwise, the ranked set itself is the primary output for exploration.



%-------------------------------------------------------------------------------
\section{Methodology selection and defence}\label{sec:method_defence}
%-------------------------------------------------------------------------------

The research questions require an inverse querying mechanism that supports ambiguity, yields a consistent output interface, and admits simple, reproducible assessment. These requirements motivate an inverse-mapping methodology built around probabilistic candidate generation: most computation occurs offline by fitting $q_\theta(\vect{x}\mid\vect{y})$, and each new query is answered by sampling a small candidate set followed by a fixed selection rule. This amortised design follows established practice in learned inverse problems and simulation-based inference, where conditional density estimation is used to make repeated inverse queries efficient. \cite{Arridge2019,Ongie2020,Cranmer2020SBI}

A second design choice is to decouple learning from checking. The inverse model is trained solely from paired examples via \eqref{eq:cond_mle_inverse}, while the best available forward description is used after training to check and rank candidates in outcome space. This reflects a pragmatic deployment reality: some settings provide an explicit simulator or forward operator, while others require a surrogate evaluator trained for checking. In both cases, the checking rule is stated explicitly through $d(\widetilde{\vect{f}}(\widehat{\vect{x}}),\vect{y}^\star)$, so conclusions are tied to a declared notion of attainment rather than to informal inspection.

Optimisation-at-query-time remains complementary. Methods such as Bayesian optimisation can refine candidates when evaluations are affordable, but they introduce query-time search cost and additional tuning choices. The default methodology therefore prioritises amortised inverse querying, with optional refinement as a separate stage when deployment constraints permit. \cite{Shahriari2016BayesOpt}


%-------------------------------------------------------------------------------
\section{Methodological framework}\label{sec:framework}
%-------------------------------------------------------------------------------

We consider two complementary inverse families for $\mathcal{Y}\!\to\!\mathcal{X}$.
\begin{enumerate}
  \item Deterministic inverse models $g_{\theta}:\mathcal{Y}\!\to\!\mathcal{X}$
  learned by empirical risk minimisation (ERM) with capacity control and
  constraint handling (projection or penalties).

  \item Probabilistic inverse models $p_{\theta}(\vect{x}\mid\vect{y})$ trained by
  conditional maximum likelihood in expressive families (e.g., CVAEs or
  normalising flows), equipped with a \emph{selection policy} that returns a
  single decision (MAP/mean/median) or a small set of $k$ proposals at query
  time. For diversity-aware sets, we optionally use a determinantal point
  process (DPP) subset selector over candidates.
\end{enumerate}



\begin{figure}[h]
  \centering
  \begin{adjustbox}{width=\linewidth}
      \input{figures/framework}
  \end{adjustbox}
  \caption{Overview of the modular propose--check workflow used throughout the thesis: learning an inverse proposal model from paired examples, selecting a single decision per target with a fixed policy, and assessing attainment via a task-defined consistency check in outcome space.}
  \label{fig:framework}
\end{figure}




To reduce one-to-many ambiguity, both families may include a
\emph{forward-consistency} term that encourages
$\vect{f}(g_{\theta}(\vect{y}))\!\approx\!\vect{y}$ during training.





\section{Inverse-model training}\label{sec:training}

\subsection{Deterministic inverse ERM}\label{sec:det_inverse}
We estimate $g_\theta$ by minimising a supervised loss in decision space with regularisation:
\begin{equation}
  \widehat{\theta}\;\in\;\arg\min_{\theta}\;
  \frac{1}{N}\sum_{i=1}^{N}\ell_{\mathcal{X}}\!\Big(g_{\theta}(\mathbf{y}_i),\,\mathbf{x}_i\Big)
  \;+\;\lambda\,\Omega(\theta).
  \label{eq:erm_inverse}
\end{equation}
Here $\ell_{\mathcal{X}}$ measures deviation in $\mathcal{X}$ (e.g., scaled $\ell_2^2$), while $\Omega(\theta)$ (e.g., weight decay or sparsity) improves generalisation and numerical stability in ill-posed settings.

\paragraph{Constraint-aware training (projection/penalties).}
If decisions must satisfy $g_j(\vect{x})\!\le\!0$ and $h_k(\vect{x})\!=\!0$, we use projection when $\mathcal{X}$ is convex and closed, or exterior penalties otherwise:
\begin{equation}
  \widehat{\theta}\;\in\;\arg\min_{\theta}\;\frac{1}{N}\sum_{i=1}^{N}\!\Big[
    \ell_{\mathcal{X}}\!\big(\,\Pi_{\mathcal{X}}\{g_{\theta}(\mathbf{y}_i)\},\,\mathbf{x}_i\big)
    \;+\;\beta\!\sum_j [\,g_j(g_{\theta}(\mathbf{y}_i))\,]_+^{2}
    \;+\;\beta'\!\sum_k h_k(g_{\theta}(\mathbf{y}_i))^{2}\Big]
  \;+\;\lambda\,\Omega(\theta),
  \label{eq:erm_constraints}
\end{equation}
where $[\cdot]_+=\max\{\cdot,0\}$; squared-hinge and quadratic terms are standard, keeping optimisation unconstrained.

\paragraph{Forward-consistency (optional).}
To bias toward decisions whose forward responses match observed outcomes, we add an outcome-space penalty:
\begin{equation}
  \widehat{\theta}\;\in\;\arg\min_{\theta}\;
  \frac{1}{N}\sum_{i=1}^{N}\!\Big[
    \ell_{\mathcal{X}}\!\big(g_{\theta}(\mathbf{y}_i),\,\mathbf{x}_i\big)
    \;+\;\alpha\,d\!\big(\mathbf{f}(g_{\theta}(\mathbf{y}_i)),\,\mathbf{y}_i\big)^{2}
  \Big]\;+\;\lambda\,\Omega(\theta),
  \qquad \alpha\!\ge\!0,
  \label{eq:forward_consistency}
\end{equation}
with $d$ an outcome-space distance (defined in \S\ref{sec:evaluation}). This “tandem” idea reduces non-uniqueness by validating inverse predictions through $\vect{f}$.

\subsection{Probabilistic inverse training}\label{sec:prob_inverse}

We model the inverse relation as a conditional distribution $p_{\theta}(\mathbf{x}\mid\mathbf{y})$ and fit it by conditional maximum likelihood:
\begin{equation}
  \widehat{\theta}\in\arg\max_{\theta}\;\sum_{i=1}^{N}\log p_{\theta}(\mathbf{x}_i\mid\mathbf{y}_i)\;-\;\gamma\,\mathcal{R}(\theta),
  \label{eq:cll_inverse}
\end{equation}
where $\mathcal{R}(\theta)$ regularises the density model (e.g., VAE/flow terms or weight decay). Expressive conditional families such as CVAEs and normalizing flows are standard choices for multi–modal $p(\mathbf{x}\mid\mathbf{y})$. 

At inference, we expose a \emph{selection policy} $\pi$ that maps the learned conditional to either a single decision or a set of $k$ proposals, depending on user intent:
\[
  \widehat{\mathcal{X}}_{\pi}(\mathbf{y}^{\ast};k)\;=\;S_{\pi}\!\left(p_{\widehat{\theta}}(\mathbf{x}\mid\mathbf{y}^{\ast}),\,k\right).
\]
Typical instantiations are:
(i) \emph{MAP:} $S_{\text{MAP}}(\cdot,1)=\arg\max_{\mathbf{x}}p_{\widehat{\theta}}(\mathbf{x}\mid\mathbf{y}^{\ast})$;
(ii) \emph{risk–optimal summaries:} $S_{\text{mean}}(\cdot,1)=\mathbb{E}[\mathbf{x}\mid\mathbf{y}^{\ast}]$ or a coordinate-wise median (squared/absolute–loss Bayes rules); \cite{Bishop2006PRML}
(iii) \emph{i.i.d.\ sampling:} $S_{\text{sample}}$ draws $k$ independent $\mathbf{x}^{(r)}\!\sim p_{\widehat{\theta}}(\cdot\mid\mathbf{y}^{\ast})$;
(iv) \emph{diversity–aware sets:} draw $L\!>\!k$ candidates then select a diverse subset, e.g., via a determinantal point process (DPP) kernel on decision space, yielding $\arg\max_{|\mathcal{S}|=k}\det K(\mathcal{S})$. \cite{KuleszaTaskar2012DPP}
Policies (iii)–(iv) support multi-solution exploration when the inverse is intrinsically one-to-many; (i)–(ii) provide single, reproducible decisions when required.


\section{Evaluation plan}\label{sec:evaluation}
We evaluate the decision quality with outcome-space discrepancy and a specification-level success indicator. Unless otherwise noted, when multiple proposals are returned ($k\!>\!1$) for exploration, we report metrics on the \emph{top-ranked} candidate under the chosen policy to keep comparisons consistent.

\subsection{forward estimator evaluation}
Write about the use of the forward estimator

\subsection{Outcome-space error}
\begin{equation}
  \mathcal{E}_{\mathcal{Y}}(\widehat{\mathbf{x}},\mathbf{y}^{\ast})
  \;=\;
  d\!\big(\,\mathbf{f}(\widehat{\mathbf{x}}),\,\mathbf{y}^{\ast}\big),
  \qquad
  d\in\{\ \|\cdot\|_2\,,\ \text{Mahalanobis}\ \}.
  \label{eq:target_error}
\end{equation}
Euclidean distance is a natural default after standardising outcome scales; Mahalanobis distance incorporates outcome covariance and is preferable when dimensions have different units or correlations.

\subsection{Tolerance-based success}
For a binary, specification-level view we report
\begin{equation}
  \mathrm{Succ}_\epsilon(\widehat{\mathbf{x}},\mathbf{y}^{\ast})
  \;=\;
  \mathbb{I}\!\left[\, d\!\big(\mathbf{f}(\widehat{\mathbf{x}}),\, \mathbf{y}^{\ast}\big) \le \epsilon \right],
  \label{eq:succ_eps_single}
\end{equation}
with $\epsilon$ set by domain tolerances. Together, $\mathcal{E}_{\mathcal{Y}}$ (closeness) and $\mathrm{Succ}_\epsilon$ (meets spec) offer a concise, reproducible summary.

\section{Calibration procedures}\label{sec:calibration}
To attach coverage-style guarantees to the forward-validated outcomes, we use split conformal calibration on a held-out calibration set.

\paragraph{SplitConformalL2.}
Define residuals $r_i=\|\mathbf{f}(g_{\widehat{\theta}}(\mathbf{y}_i))-\mathbf{y}_i\|_2$ on calibration data and form
\begin{equation}
  \mathcal{C}_{\alpha}(\mathbf{y}^{\ast})=\{\mathbf{y}:\ \|\mathbf{y}-\mathbf{y}^{\ast}\|_2\le q_{1-\alpha}\},
  \label{eq:split_conformal_l2}
\end{equation}
where $q_{1-\alpha}$ is the $(1-\alpha)$ empirical quantile of $\{r_i\}$.

\paragraph{SplitConformal–Mahalanobis.}
With covariance $\mathbf{\Sigma}$ estimated from calibration outcomes and $\Delta(\mathbf{y},\mathbf{y}^{\ast})=\sqrt{(\mathbf{y}-\mathbf{y}^{\ast})^{\top}\mathbf{\Sigma}^{-1}(\mathbf{y}-\mathbf{y}^{\ast})}$,
\begin{equation}
  \mathcal{C}^{\mathrm{Mah}}_{\alpha}(\mathbf{y}^{\ast})=\{\mathbf{y}:\ \Delta(\mathbf{y},\mathbf{y}^{\ast})\le q^{\mathrm{Mah}}_{1-\alpha}\}.
  \label{eq:mahalanobis_conformal}
\end{equation}
We test whether $\mathbf{f}(\widehat{\mathbf{x}})\in\mathcal{C}_{\alpha}(\mathbf{y}^{\ast})$ or $\mathcal{C}^{\mathrm{Mah}}_{\alpha}(\mathbf{y}^{\ast})$; by construction, these sets deliver finite-sample marginal coverage under exchangeability.

\section{Data sources and ethics}\label{sec:data_ethics}
We use benchmarked or archival pairs $(\vect{x},\vect{y})$ originating from simulators or standard suites (e.g., COCO/BBOB for black-box optimisation) as well as domain archives when available. For each source, we document provenance, licensing, storage, and disposal. No personally identifiable or sensitive data are involved.

\section{Methods conclusion}\label{sec:methods_conclusion}
In sum, the methodology learns an outcome$\!\to\!$decision mapping aligned with forward behaviour, supports either a single decision or a small set of proposals at query time, and evaluates attainment of $\vect{y}^{\star}$ with transparent outcome-space criteria. Calibration procedures provide specification-level guarantees for forward-validated outcomes.









\section{Offline Phase: Data Generation and Model Training}

\subsection{Generation of Pareto Samples}

The first step is to obtain a representative set of Pareto‑optimal or
near‑optimal solutions.  Any suitable multi‑objective optimisation
algorithm can be employed; in our experiments we use the Non‑dominated
Sorting Genetic Algorithm II (NSGA‑II) and the Multi‑Objective
Evolutionary Algorithm based on Decomposition (MOEA/D) as baselines.  The
resulting archive \(\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^N\)
contains decision vectors \(\vect{x}_i\in\calX\) and their objective
values \(\vect{y}_i=\vect{f}(\vect{x}_i)\).  To ensure diversity and
coverage of the Pareto front the optimisation is run with standard
population sizes and termination criteria, and the union of final
non‑dominated solutions over multiple runs is stored.

\subsection{Data Preprocessing}

Prior to model training we normalise the objective vectors to lie in
\([0,1]^m\) using min–max scaling.  This step ensures that objectives
with different units or ranges contribute equally to the inverse model.
Similarly, decision variables are scaled to \([0,1]^n\) to avoid biasing
the regressors.  Outliers are removed using interquartile range
thresholding.  The processed dataset is then randomly split into
training, validation and test sets.

\subsection{Inverse Model Training}

We consider several modelling strategies for the inverse mapping
\(\hat{g}\colon\calY\to\calX\):


\paragraph{Feed‑Forward Neural Networks (FFNN).}  A multi‑layer perceptron
with fully connected layers and ReLU activations can approximate complex
nonlinear mappings.  We select the network architecture using
hyperparameter search and train using Adam optimisation with early
stopping.  Data augmentation and regularisation (dropout and weight
decay) improve generalisation.

\paragraph{Transfer Learning and Multi‑Source Data.}  When the available
dataset is small, transfer learning can leverage data from related tasks.
Tan et~al. propose learning across common objective spaces to mitigate
data scarcity and demonstrate that combining multiple Pareto fronts can
  improve inverse model generalisation~\cite{Tan2023}.  We adopt
this idea by pooling Pareto samples from benchmark problems with similar
objective definitions during training.

\subsection{Model Selection and Validation}

For each modelling strategy we perform hyperparameter tuning via
k‑fold cross‑validation on the training set.  The loss function combines
the mean squared error between predicted and true decision variables and a
penalty on predicted values outside the feasible range.  Models are
compared using performance metrics such as root mean squared error
(RMSE), coefficient of determination \(R^2\) and the generational
distance between the reconstructed Pareto front and the true front.  We
select the model with the best validation performance and retain it for
the online phase.

\subsection{Forward Surrogate Model}

In addition to the inverse model we train a surrogate forward model
\(\hat{f}\colon\calX\to\calY\) on the same data.  This model is used
during the online phase to rapidly evaluate candidate decisions without
incurring the cost of running the underlying simulation or experiment.  A
Gaussian process is employed due to its good performance on small data;
alternatively, a neural network or polynomial regression can be used.

\section{Online Phase: Interactive Exploration}

During the online phase the trained inverse and forward models are used
to support interactive design exploration.  The process comprises the
following steps:

\paragraph{Target specification.}  The user specifies a desired objective
vector \(\vect{y}^\star\) either numerically or by selecting a point in a
visualisation of the objective space.  Input widgets allow users to
adjust individual objectives and observe how the target moves relative
to the sampled Pareto front.

\paragraph{Plausibility check.}  The system computes the distance
\(d(\vect{y}^\star, \mathcal{D})\) to the nearest sample in the objective
space and derives a plausibility score.  If the score is below a
user‑defined threshold, a warning is displayed indicating that the
desired trade‑off may not be achievable.

\paragraph{Inverse prediction.}  The inverse model \(\hat{g}\) is queried
with \(\vect{y}^\star\) to produce a set of candidate decisions
\(\{\hat{\vect{x}}^{(k)}\}_{k=1}^K\).  For probabilistic models such as
GPR we draw multiple samples from the posterior distribution; for
deterministic models such as RBFs and FFNNs we sample by adding small
Gaussian noise to the input.

\paragraph{Forward evaluation.}  Each candidate is evaluated using the
forward surrogate \(\hat{f}\) to compute the predicted objective vector
\(\hat{\vect{y}}^{(k)}=\hat{f}(\hat{\vect{x}}^{(k)})\).  The forward
error \(e_f^{(k)}=\|\hat{\vect{y}}^{(k)}-\vect{y}^\star\|\) and distance
to the sampled Pareto front are computed.

\paragraph{Ranking and selection.}  Candidates are ranked according to a
multi‑criteria score combining forward error, diversity and plausibility.
The top‑\(M\) candidates are presented to the user along with their
predicted objective values and a visual indication of their proximity to
the Pareto front.

\paragraph{User feedback.}  The user can select a candidate and, if
desired, refine the target or adjust weights in the ranking.  Selected
candidates can be saved for further simulation or fabrication.  User
interactions can be logged to adapt the inverse model over time.


\section{Training Pipeline}

Given a dataset \(\mathcal{D}\) we perform the following steps for each
problem:

\begin{enumerate}
  \item \textbf{Normalisation.}  Compute the minimum and maximum of each
    objective and decision dimension and scale the data to \([0,1]\).
  \item \textbf{Train–validation split.}  Use an 80/20 split for
    training and validation.  When data is scarce we employ a leave‑one‑out
    scheme.
  \item \textbf{Hyperparameter search.}  For each inverse model type
    (GPR, RBF, RF, FFNN) perform 50 random trials over predefined
    hyperparameter ranges.  For GPR we search over squared exponential
    kernels with length scales in \([0.1,10]\); for RBF we vary the
    number of centres between 10 and 200; for RF we vary the number of
    trees (50–300) and maximum depth (3–20); for FFNN we vary the number
    of hidden layers (1–3) and neurons (50–200).
  \item \textbf{Model training.}  Fit each candidate model on the
    training data and evaluate performance on the validation set using
    RMSE and generational distance.  Select the best hyperparameter
    configuration.
  \item \textbf{Forward surrogate.}  Train a Gaussian process regression
    model on the same training data to act as the forward surrogate.  A
    Matérn kernel is used with noise estimated via maximum likelihood.
  \item \textbf{Testing.}  Evaluate the selected inverse and forward
    models on the held‑out test data.  Report RMSE, \(R^2\) and the
    proportion of predictions within a tolerance of the true decision
    values.
\end{enumerate}



\section{Algorithmic Summary}

Algorithm~\ref{alg:offline} summarises the offline training procedure and
Algorithm~\ref{alg:online} details the online exploration loop.  The
pseudocode abstracts away implementation details to highlight the main
steps.

\begin{algorithm}[h]
  \caption{Offline Training of Inverse and Forward Models}
  \label{alg:offline}
  \begin{algorithmic}[1]
    \Require multi‑objective function \(\vect{f}\), decision space \(\calX\)
    \Ensure trained inverse model \(\hat{g}\) and forward surrogate \(\hat{f}\)
    \State \textbf{Generate Pareto samples} \(\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}\)
       by running a MOO algorithm (e.g. NSGA‑II)
    \State \textbf{Normalise} decision and objective vectors
    \State \textbf{Split} \(\mathcal{D}\) into training/validation/test sets
    \State \textbf{Select model type} (GP, RBF, RF, FFNN)
    \State \textbf{Tune hyperparameters} via cross‑validation
    \State \textbf{Train inverse model} \(\hat{g}\) on training set
    \State \textbf{Train forward model} \(\hat{f}\) on training set
    \State \textbf{Validate} on test set; select best model
    \State \Return \(\hat{g},\hat{f}\)
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
  \caption{Online Inverse Decision Exploration}
  \label{alg:online}
  \begin{algorithmic}[1]
    \Require trained models \(\hat{g},\hat{f}\), dataset \(\mathcal{D}\)
    \Repeat
      \State \textbf{Get target} objectives \(\vect{y}^\star\) from user
      \State \textbf{Compute plausibility score} using distance to \(\mathcal{D}\)
      \If{score $<\tau$}
        \State display warning and optionally project \(\vect{y}^\star\) onto front
      \EndIf
      \State \textbf{Generate candidates} \(\{\hat{\vect{x}}^{(k)}\}\) via \(\hat{g}\)
      \For{each candidate \(k\)}
        \State evaluate \(\hat{\vect{y}}^{(k)}=\hat{f}(\hat{\vect{x}}^{(k)})\)
        \State compute forward error and distance metrics
      \EndFor
      \State \textbf{Rank} candidates by error, diversity and plausibility
      \State \textbf{Display} top candidates and solicit user feedback
    \Until{user terminates}
  \end{algorithmic}
\end{algorithm}
