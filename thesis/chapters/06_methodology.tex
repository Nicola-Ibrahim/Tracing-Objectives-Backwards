%-------------------------------------------------------------------------------
% Methodology
%-------------------------------------------------------------------------------

\chapter{Methodology}\label{chap:method}

Inverse decision mapping becomes practically useful when it supports interactive design exploration: a user specifies a target outcome $\vect{y}^{\star}$ and the method proposes candidate decisions $\widehat{\vect{x}}\in\calX$ that plausibly realise that target. The Introduction framed this task as inherently ambiguous and sometimes infeasible, and posed research questions about (i) constructing an outcome $\!\to\!$ decision mechanism, (ii) defining attainment in a reproducible way, and (iii) understanding which modelling choices generalise to unseen targets. The methodology presented here turns those requirements into a concrete protocol centered on probabilistic candidate generation followed by explicit outcome-space evaluation.

The core stance is to learn an amortised inverse model that represents the inverse relation as a conditional distribution rather than as a single-valued map. A probabilistic inverse supports one-to-many structure by generating multiple candidates for the same target, enabling exploration rather than forcing an arbitrary point estimate. This conditional-density viewpoint is widely used in amortised inference and simulation-based inference, where offline training yields fast query-time sampling. \cite{Papamakarios2021,Cranmer2020SBI} In this thesis, evaluation is treated as a distinct stage that scores and ranks candidates using an explicit discrepancy in outcome space, computed via the best available outcome evaluator.

Three design principles shape the chapter and the experiments that follow:
\begin{itemize}
  \item Candidate-based inversion: inverse querying produces a set of candidates by sampling from a learned conditional model, making ambiguity visible through diversity rather than suppressing it by construction. \cite{Cranmer2020SBI}
  \item Decoupled evaluation: learning is performed from paired examples without requiring an explicit forward operator; attainment is assessed after training by scoring candidates with a task-defined outcome discrepancy and an outcome evaluator that may be a simulator, a surrogate, a measurement routine, or an offline mapping available only for evaluation. \cite{Arridge2019,Ongie2020}
  \item Comparable reporting: experiments fix a common candidate budget and a common scoring rule so that comparisons reflect modelling choices rather than interface differences or ad hoc evaluation. \cite{Arridge2019,Ongie2020}
\end{itemize}

The following sections formalise the probabilistic inverse model and its training objective, define the query-time candidate generation procedure, and specify how candidates are scored and ranked in outcome space under an explicit discrepancy measure. Subsequent methodological components (constraints, feasibility handling, and any optional refinement procedures) are introduced only as needed for reproducible implementation and evaluation across datasets and benchmarks.


%-------------------------------------------------------------------------------
\section{Theoretical framework}\label{sec:theory_model}
%-------------------------------------------------------------------------------

The methodology treats inverse design as learning a conditional model over decisions given outcomes. Rather than approximating a single inverse function, we learn a probabilistic inverse $q_\theta(\vect{x}\mid\vect{y})$ that can represent multi-modality and generate multiple candidates for the same target. This conditional-density framing supports amortised querying: training fits a reusable conditional model from paired examples, and inference reduces to sampling conditioned on the target. \cite{Papamakarios2021,Cranmer2020SBI}

Given a representative dataset $\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^{N}$, model parameters are fit by conditional likelihood:
\begin{equation}
\widehat{\theta}\in\arg\min_{\theta}\frac{1}{N}\sum_{i=1}^{N}
\Big[-\log q_\theta(\vect{x}_i\mid\vect{y}_i)\Big] + \lambda\,\Omega(\theta),
\label{eq:cond_mle_inverse}
\end{equation}
where $\Omega(\theta)$ denotes regularisation. This training objective depends only on paired data and does not assume access to a forward operator during learning, which keeps the inverse model applicable across settings with different levels of forward-model availability. \cite{Arridge2019,Ongie2020}

At query time, the inverse model produces a candidate set by sampling:
\begin{equation}
  \widehat{\vect{x}}^{(k)} \sim q_\theta(\vect{x}\mid\vect{y}^\star),\qquad k=1,\dots,K.
  \label{eq:candidate_generation}
\end{equation}
Candidates are then assessed in outcome space using a task-defined discrepancy $d(\cdot,\cdot)$ together with an outcome evaluator $\mathcal{E}:\calX\to\calY$ that represents the best available mechanism for assessing outcomes. Depending on the setting, $\mathcal{E}$ may be a simulator, a learned surrogate, a measurement routine, or an offline mapping used only for evaluation. \cite{Arridge2019,Ongie2020} For a target $\vect{y}^\star$, each candidate receives a score
\begin{equation}
  r^{(k)}(\vect{y}^\star) \;=\; d\!\big(\mathcal{E}(\widehat{\vect{x}}^{(k)}),\,\vect{y}^\star\big),
  \qquad k=1,\dots,K,
  \label{eq:candidate_scores}
\end{equation}
which induces a ranking and supports two usage modes: exploratory use returns the ranked set (or a top subset), while deployment can optionally select a single decision as the best-aligned candidate under the declared discrepancy and evaluator. Fixing $K$, $\mathcal{E}$, and $d$ at evaluation time makes attainment statements traceable to an explicit scoring rule rather than to implicit model behaviour. \cite{Arridge2019,Ongie2020}


%-------------------------------------------------------------------------------
\section{Methodology selection and defence}\label{sec:method_defence}
%-------------------------------------------------------------------------------

The research questions call for an inverse querying mechanism that (i) respects non-uniqueness by producing meaningful alternatives, (ii) answers queries quickly enough for interactive use, and (iii) supports reproducible attainment assessment in outcome space. These requirements motivate an inverse-mapping methodology built around probabilistic candidate generation: most computation occurs offline by fitting $q_\theta(\vect{x}\mid\vect{y})$ via \eqref{eq:cond_mle_inverse}, and each new target is answered by sampling candidates via \eqref{eq:candidate_generation}. This amortised design is consistent with established practice in learned inverse problems and simulation-based inference, where conditional density estimation is used to make repeated inverse queries efficient. \cite{Arridge2019,Ongie2020,Cranmer2020SBI}

A second choice is to separate learning from evaluation. The inverse model is trained from paired examples, while attainment is assessed after training by scoring candidates with an explicit outcome discrepancy under a stated evaluator, as in \eqref{eq:candidate_scores}. This decoupling is methodologically useful because forward-model access varies by dataset and domain: some settings provide a trusted simulator, others support only surrogate or measurement-based assessment, and some benchmarks permit evaluation through recorded outcomes alone. Stating the evaluator and discrepancy explicitly ties conclusions to a declared notion of attainment and supports comparable reporting across experiments. \cite{Arridge2019,Ongie2020}

Finally, optimisation-at-query-time remains complementary. Black-box search procedures, including Bayesian optimisation, can refine a candidate when evaluations are affordable, but they increase query-time cost and introduce additional tuning and search design choices. The default methodology therefore prioritises amortised candidate generation with post-hoc scoring, while treating query-time optimisation as an optional refinement stage when deployment constraints permit. \cite{Shahriari2016BayesOpt}



%-------------------------------------------------------------------------------
\section{Methodological framework}\label{sec:framework}
%-------------------------------------------------------------------------------

From this point onward the chapter shifts from motivation to execution. The goal is to make
inverse querying behave like a practical routine that can be run repeatedly and compared
fairly across model families. The workflow follows a simple pattern: learn an inverse proposal
model, generate multiple candidates for each new target, and then score those
candidates in outcome space using an explicit discrepancy rule. In this way, ambiguity is
handled by generation, while attainment is handled by checking under a declared evaluator
and metric rather than by relying on implicit properties of the inverse model. 

\begin{figure}[t]
  \centering
  \begin{adjustbox}{width=\linewidth}
      \input{figures/framework}
  \end{adjustbox}
  \caption{Propose--check workflow used throughout the thesis. An inverse model generates a candidate set for a target outcome, and an explicit evaluation stage scores and ranks candidates using a stated outcome evaluator and discrepancy.}
  \label{fig:framework}
\end{figure}

The starting point is a conditional distribution over decisions given outcomes,
$q_{\theta}(\vect{x}\mid\vect{y})$. Once this model is trained, a user query $\vect{y}^{\star}$ is
answered by drawing a fixed-size set of candidates:
\begin{equation}
  \widehat{\vect{x}}^{(k)} \sim q_{\widehat{\theta}}(\vect{x}\mid\vect{y}^{\star}),
  \qquad k=1,\dots,K .
  \label{eq:candidate_generation}
\end{equation}
We keep the candidate budget $K$ fixed across experiments so the user-facing interface stays
the same regardless of the chosen model family, and so comparisons reflect modelling choices
rather than differences in output conventions.

Candidate generation alone is not sufficient, because the thesis evaluates success in outcome
space. After sampling, each candidate is therefore checked by computing its outcome through
the best available outcome evaluator $\mathcal{E}:\mathcal{X}\to\mathcal{Y}$ and scoring it with a
task-defined discrepancy $d(\cdot,\cdot)$:
\begin{equation}
  r^{(k)}(\vect{y}^{\star}) \;=\;
  d\!\big(\mathcal{E}(\widehat{\vect{x}}^{(k)}),\,\vect{y}^{\star}\big),
  \qquad k=1,\dots,K .
  \label{eq:candidate_scoring}
\end{equation}
The evaluator $\mathcal{E}$ is intentionally defined broadly. Depending on the dataset and the
deployment context, it may be an explicit simulator, a measurement routine, a learned
surrogate trained for checking, or an offline mapping used only for evaluation. The method
returns candidates ranked by $r^{(k)}(\vect{y}^{\star})$; when a single deployed configuration is
required, it is taken as the best-ranked candidate under the declared scoring rule. 



%-------------------------------------------------------------------------------
\section{Inverse-model training}\label{sec:training}
%-------------------------------------------------------------------------------

With the propose--check interface fixed in Section~\ref{sec:framework}, implementation starts by fitting the candidate generator from paired examples
$\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^{N}$. The goal is to learn a conditional distribution
$q_{\theta}(\vect{x}\mid\vect{y})$ that assigns high probability to decisions that are consistent with the observed decision--outcome relationship, while remaining expressive enough to represent one-to-many structure. Chapter~3 motivates the three conditional families used in this thesis (MDN, CVAE, INN) and derives their probabilistic forms; here we restate the corresponding training objectives in the form used for experiments and implementation.

Across all models, parameter estimation follows the same principle: maximise conditional fit to the observed pairs while controlling capacity through regularisation. In practice, we minimise the regularised negative conditional log-likelihood,
\begin{equation}
  \widehat{\theta}\in\arg\min_{\theta}\;
  \frac{1}{N}\sum_{i=1}^{N}\Big[-\log q_{\theta}(\vect{x}_i\mid\vect{y}_i)\Big]
  \;+\;\lambda\,\Omega(\theta),
  \label{eq:cond_mle_inverse}
\end{equation}
where $\Omega(\theta)$ collects stabilising terms such as weight decay and any model-specific penalties. This fitting step uses only paired data; the outcome evaluator $\mathcal{E}$ and discrepancy $d$ are introduced later and only enter through the candidate scoring rule in Section~\ref{sec:framework}.

The three instantiations below differ in how they parameterise $q_{\theta}(\vect{x}\mid\vect{y})$ and how the log-density in \eqref{eq:cond_mle_inverse} is computed or approximated.

Mixture Density Network (MDN).
The MDN represents the conditional density as a finite Gaussian mixture with outcome-dependent parameters (see Chapter~3, Eq.~(3.14)--(3.15)):
\begin{equation}
  q_{\theta}(\vect{x}\mid\vect{y}) =
  \sum_{j=1}^{M}\pi_{\theta,j}(\vect{y})\,
  \mathcal{N}\!\big(\vect{x};\,\boldsymbol{\mu}_{\theta,j}(\vect{y}),\,\boldsymbol{\Sigma}_{\theta,j}(\vect{y})\big),
  \qquad
  \sum_{j=1}^{M}\pi_{\theta,j}(\vect{y})=1,\ \ \pi_{\theta,j}(\vect{y})\ge 0 .
  \label{eq:mdn_density}
\end{equation}
Training substitutes \eqref{eq:mdn_density} into \eqref{eq:cond_mle_inverse} and optimises the resulting mixture negative log-likelihood. The implementation follows standard constraints for mixture weights and covariance parameterisation, as detailed in the Modeling Background.

Conditional Variational Autoencoder (CVAE).
The CVAE introduces latent variables to represent conditional variability and trains by maximising a variational lower bound on $\log q_{\theta}(\vect{x}\mid\vect{y})$ (see Chapter~3, Eq.~(3.16)--(3.17)). Using an encoder $q_{\phi}(\vect{z}\mid\vect{x},\vect{y})$ and a decoder $p_{\theta}(\vect{x}\mid\vect{y},\vect{z})$, the objective becomes
\begin{equation}
  \widehat{\theta},\widehat{\phi}\in\arg\min_{\theta,\phi}\;
  -\frac{1}{N}\sum_{i=1}^{N}\Big[
    \mathbb{E}_{q_{\phi}(\vect{z}\mid\vect{x}_i,\vect{y}_i)}\log p_{\theta}(\vect{x}_i\mid\vect{y}_i,\vect{z})
    - \mathrm{KL}\!\big(q_{\phi}(\vect{z}\mid\vect{x}_i,\vect{y}_i)\,\|\,p(\vect{z})\big)
  \Big]
  +\lambda\,\Omega(\theta,\phi),
  \label{eq:cvae_elbo}
\end{equation}
which is the negative ELBO plus regularisation. This keeps the training criterion tractable while still targeting a conditional generative model whose latent variation can produce diverse candidates for a fixed $\vect{y}^{\star}$.

Invertible Neural Network (INN).
The INN implements a conditional density through an invertible transformation between a simple reference latent and the decision, conditioned on the outcome target. Using a conditional invertible map $T_{\theta}(\cdot;\vect{y})$,
\begin{equation}
  \vect{x} = T_{\theta}(\vect{z};\vect{y}),\qquad \vect{z}\sim \mathcal{N}(\mathbf{0},\mathbf{I}),
  \label{eq:inn_forward_map}
\end{equation}
the conditional log-density follows from a change-of-variables construction (see the Modeling Background for the invertibility setup; the implementation uses the standard log-determinant form):
\begin{equation}
  \log q_{\theta}(\vect{x}\mid\vect{y}) =
  \log p(\vect{z}) +
  \log\left|\det \frac{\partial T_{\theta}^{-1}(\vect{x};\vect{y})}{\partial \vect{x}}\right|,
  \qquad
  \vect{z}=T_{\theta}^{-1}(\vect{x};\vect{y}).
  \label{eq:inn_change_of_vars}
\end{equation}
Substituting \eqref{eq:inn_change_of_vars} into \eqref{eq:cond_mle_inverse} yields a direct likelihood-based training objective. This preserves an explicit density model while maintaining a sampling mechanism that is compatible with the fixed candidate budget $K$ used throughout the experiments.

In summary, MDN, CVAE, and INN all realise the same training goal---a conditional generator $q_{\theta}(\vect{x}\mid\vect{y})$ fitted from paired data---but they differ in how multimodality is represented (mixture components, latent-variable decoding, or invertible latent transport). Downstream evaluation then treats their sampled candidates uniformly via the shared scoring rule in \eqref{eq:candidate_scoring}, so empirical comparisons reflect differences in the learned conditional model rather than differences in the evaluation interface.


%-------------------------------------------------------------------------------
\section{Evaluation plan}\label{sec:evaluation}
%-------------------------------------------------------------------------------

The inverse models in this thesis return a candidate set for each target rather than a single point estimate. Evaluation therefore answers a concrete question: given a target $\vect{y}^{\star}$ and candidates $\{\widehat{\vect{x}}^{(k)}\}_{k=1}^{K}$, which candidates most closely satisfy the objective under a stated checking mechanism, and how often does at least one candidate meet a specification-level requirement. This section fixes that checking mechanism so that results remain comparable across model families and datasets.

Evaluation is performed through an outcome evaluator $\mathcal{E}:\mathcal{X}\to\mathcal{Y}$, which represents the best available way to assess outcomes for proposed decisions. Depending on the setting, $\mathcal{E}$ may be a trusted forward simulator, an experimental measurement routine, or a surrogate model used purely for checking when an explicit forward model is not available. 

\subsection{Outcome-space checking and ranking}\label{sec:eval_checking}

For each candidate, we compute an outcome-space discrepancy score relative to the user target:
\begin{equation}
  r^{(k)}(\vect{y}^{\star})
  \;=\;
  d\!\big(\mathcal{E}(\widehat{\vect{x}}^{(k)}),\,\vect{y}^{\star}\big),
  \qquad k=1,\dots,K,
  \label{eq:candidate_scoring}
\end{equation}
and rank candidates by increasing $r^{(k)}(\vect{y}^{\star})$. This score is the main evaluation primitive: it is the mechanism by which the forward model or surrogate contributes to validation, and it is the basis for all reported attainment metrics.

To summarise performance in a way that respects the one-to-many nature of the inverse mapping, we report the best-achieved discrepancy within the candidate set:
\begin{equation}
  \mathcal{E}_{\mathcal{Y}}^{(K)}(\vect{y}^{\star})
  \;=\;
  \min_{k\in\{1,\dots,K\}}
  d\!\big(\mathcal{E}(\widehat{\vect{x}}^{(k)}),\,\vect{y}^{\star}\big).
  \label{eq:best_of_k_error}
\end{equation}
When qualitative exploration is the goal, the ranked list itself is retained; when a single deployed decision is required, it is chosen as the top-ranked candidate under \eqref{eq:candidate_scoring}. Using $\min_k$ as the reporting convention keeps comparisons consistent while still crediting methods that generate useful diversity.

The discrepancy $d$ is selected to match the outcome semantics and scaling. After standardising outcomes, Euclidean distance provides a transparent default. When outcome dimensions are correlated or have heterogeneous noise, a Mahalanobis discrepancy incorporates covariance structure:
\begin{equation}
  d_{\mathrm{Mah}}(\vect{u},\vect{v})
  \;=\;
  \sqrt{(\vect{u}-\vect{v})^{\top}\boldsymbol{\Sigma}^{-1}(\vect{u}-\vect{v})},
  \label{eq:mahalanobis_distance}
\end{equation}
with $\boldsymbol{\Sigma}$ estimated from a reference split of outcomes. \cite{Bishop2006PRML}

\subsection{Tolerance-based attainment}\label{sec:eval_tolerance}

Outcome-space distances are informative, but many applications ultimately require a specification-level statement: whether the method can produce at least one candidate within an acceptable tolerance. Using the same discrepancy as in \eqref{eq:candidate_scoring}, we report
\begin{equation}
  \mathrm{Succ}^{(K)}_{\epsilon}(\vect{y}^{\star})
  \;=\;
  \mathbb{I}\!\left[\,
    \min_{k\in\{1,\dots,K\}}
    d\!\big(\mathcal{E}(\widehat{\vect{x}}^{(k)}),\,\vect{y}^{\star}\big)
    \le \epsilon
  \right],
  \label{eq:succ_eps_bestofk}
\end{equation}
where $\epsilon$ is set by domain tolerances or by a declared experimental specification. Reporting both $\mathcal{E}_{\mathcal{Y}}^{(K)}$ and $\mathrm{Succ}^{(K)}_{\epsilon}$ separates continuous closeness from compliance with a target specification.

\subsection{Operational-domain validator (ODD) for constraint-aware evaluation}\label{sec:eval_odd}

Discrepancy alone does not capture all evaluation needs. In practice, users often impose additional constraints beyond matching $\vect{y}^{\star}$, such as admissible operating regions, safety limits, or validity requirements tied to the training distribution. To make these requirements explicit, we include an operational-domain validator, denoted ODD, that acts as an additional evaluation mechanism.

The ODD produces an in-domain score or acceptance decision for a candidate outcome (and, if needed, for a candidate decision):
\begin{equation}
  s_{\psi}^{(k)} \;=\; \mathrm{ODD}_{\psi}\!\big(\widehat{\vect{x}}^{(k)},\,\mathcal{E}(\widehat{\vect{x}}^{(k)})\big),
  \qquad k=1,\dots,K,
  \label{eq:odd_score}
\end{equation}
where larger $s_{\psi}^{(k)}$ indicates better conformity to the operational domain. In machine-learning terms, this corresponds to out-of-distribution or novelty detection: the validator is trained beforehand to capture the data distribution (and any user-declared admissibility structure), so that candidates outside the supported regime can be flagged or rejected. \cite{Lu2025OODSurvey}

In evaluation, we combine target attainment and operational-domain validity by filtering or re-ranking. A simple filtering rule keeps only candidates that satisfy a validator threshold $\tau$ and any explicit user constraints encoded as a predicate $c(\cdot)\le 0$ in outcome space:
\begin{equation}
  \mathcal{K}_{\mathrm{valid}}(\vect{y}^{\star})
  \;=\;
  \Big\{\,k:\ s_{\psi}^{(k)}\ge\tau\ \ \text{and}\ \ c\!\big(\mathcal{E}(\widehat{\vect{x}}^{(k)})\big)\le 0 \Big\}.
  \label{eq:valid_set}
\end{equation}
Ranking is then applied within $\mathcal{K}_{\mathrm{valid}}(\vect{y}^{\star})$ using the same discrepancy score \eqref{eq:candidate_scoring}. If no candidate passes the validator, the method reports this explicitly and the query is treated as not attained under the declared operational constraints. This makes failure modes interpretable: the method may fail because candidates do not match the target, because they violate user constraints, or because they fall outside the learned operational domain.

The threshold $\tau$ is selected in advance on held-out data to control the strictness of the validator under the training regime. This keeps ODD decisions reproducible and prevents post hoc threshold tuning that would compromise comparability across experiments. The exact ODD model class used (e.g., one-class classification, energy-based scoring, or density-based scoring) is treated as part of the evaluation configuration and is reported alongside the discrepancy definition. \cite{Liu2020EnergyOOD}


\section{Data sources}\label{sec:data_ethics}
We use benchmarked or archival pairs $(\vect{x},\vect{y})$ originating from simulators or standard suites (e.g., COCO/BBOB for black-box optimisation) as well as domain archives when available. For each source, we document provenance, licensing, storage, and disposal. No personally identifiable or sensitive data are involved.



\section{Offline Phase: Data Generation and Model Training}

\subsection{Generation of Pareto Samples}

The first step is to obtain a representative set of Pareto‑optimal or
near‑optimal solutions.  Any suitable multi‑objective optimisation
algorithm can be employed; in our experiments we use the Non‑dominated
Sorting Genetic Algorithm II (NSGA‑II) and the Multi‑Objective
Evolutionary Algorithm based on Decomposition (MOEA/D) as baselines.  The
resulting archive \(\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^N\)
contains decision vectors \(\vect{x}_i\in\calX\) and their objective
values \(\vect{y}_i=\vect{f}(\vect{x}_i)\).  To ensure diversity and
coverage of the Pareto front the optimisation is run with standard
population sizes and termination criteria, and the union of final
non‑dominated solutions over multiple runs is stored.

\subsection{Data Preprocessing}

Prior to model training we normalise the objective vectors to lie in
\([0,1]^m\) using min–max scaling.  This step ensures that objectives
with different units or ranges contribute equally to the inverse model.
Similarly, decision variables are scaled to \([0,1]^n\) to avoid biasing
the regressors.  Outliers are removed using interquartile range
thresholding.  The processed dataset is then randomly split into
training, validation and test sets.



\subsection{Model Selection and Validation}

For each modelling strategy we perform hyperparameter tuning via
k‑fold cross‑validation on the training set.  The loss function combines
the mean squared error between predicted and true decision variables and a
penalty on predicted values outside the feasible range.  Models are
compared using performance metrics such as root mean squared error
(RMSE), coefficient of determination \(R^2\) and the generational
distance between the reconstructed Pareto front and the true front.  We
select the model with the best validation performance and retain it for
the online phase.

\subsection{Forward Surrogate Model}

In addition to the inverse model we train a surrogate forward model
\(\hat{f}\colon\calX\to\calY\) on the same data.  This model is used
during the online phase to rapidly evaluate candidate decisions without
incurring the cost of running the underlying simulation or experiment.  A
Gaussian process is employed due to its good performance on small data;
alternatively, a neural network or polynomial regression can be used.

\section{Online Phase: Interactive Exploration}

During the online phase the trained inverse and forward models are used
to support interactive design exploration.  The process comprises the
following steps:

\paragraph{Target specification.}  The user specifies a desired objective
vector \(\vect{y}^\star\) either numerically or by selecting a point in a
visualisation of the objective space.  Input widgets allow users to
adjust individual objectives and observe how the target moves relative
to the sampled Pareto front.

\paragraph{Plausibility check.}  The system computes the distance
\(d(\vect{y}^\star, \mathcal{D})\) to the nearest sample in the objective
space and derives a plausibility score.  If the score is below a
user‑defined threshold, a warning is displayed indicating that the
desired trade‑off may not be achievable.

\paragraph{Inverse prediction.}  The inverse model \(\hat{g}\) is queried
with \(\vect{y}^\star\) to produce a set of candidate decisions
\(\{\hat{\vect{x}}^{(k)}\}_{k=1}^K\).  For probabilistic models such as
GPR we draw multiple samples from the posterior distribution; for
deterministic models such as RBFs and FFNNs we sample by adding small
Gaussian noise to the input.

\paragraph{Forward evaluation.}  Each candidate is evaluated using the
forward surrogate \(\hat{f}\) to compute the predicted objective vector
\(\hat{\vect{y}}^{(k)}=\hat{f}(\hat{\vect{x}}^{(k)})\).  The forward
error \(e_f^{(k)}=\|\hat{\vect{y}}^{(k)}-\vect{y}^\star\|\) and distance
to the sampled Pareto front are computed.

\paragraph{Ranking and selection.}  Candidates are ranked according to a
multi‑criteria score combining forward error, diversity and plausibility.
The top‑\(M\) candidates are presented to the user along with their
predicted objective values and a visual indication of their proximity to
the Pareto front.

\paragraph{User feedback.}  The user can select a candidate and, if
desired, refine the target or adjust weights in the ranking.  Selected
candidates can be saved for further simulation or fabrication.  User
interactions can be logged to adapt the inverse model over time.


\section{Training Pipeline}

Given a dataset \(\mathcal{D}\) we perform the following steps for each
problem:

\begin{enumerate}
  \item \textbf{Normalisation.}  Compute the minimum and maximum of each
    objective and decision dimension and scale the data to \([0,1]\).
  \item \textbf{Train–validation split.}  Use an 80/20 split for
    training and validation.  When data is scarce we employ a leave‑one‑out
    scheme.
  \item \textbf{Hyperparameter search.}  For each inverse model type
    (GPR, RBF, RF, FFNN) perform 50 random trials over predefined
    hyperparameter ranges.  For GPR we search over squared exponential
    kernels with length scales in \([0.1,10]\); for RBF we vary the
    number of centres between 10 and 200; for RF we vary the number of
    trees (50–300) and maximum depth (3–20); for FFNN we vary the number
    of hidden layers (1–3) and neurons (50–200).
  \item \textbf{Model training.}  Fit each candidate model on the
    training data and evaluate performance on the validation set using
    RMSE and generational distance.  Select the best hyperparameter
    configuration.
  \item \textbf{Forward surrogate.}  Train a Gaussian process regression
    model on the same training data to act as the forward surrogate.  A
    Matérn kernel is used with noise estimated via maximum likelihood.
  \item \textbf{Testing.}  Evaluate the selected inverse and forward
    models on the held‑out test data.  Report RMSE, \(R^2\) and the
    proportion of predictions within a tolerance of the true decision
    values.
\end{enumerate}



\section{Algorithmic Summary}

Algorithm~\ref{alg:offline} summarises the offline training procedure and
Algorithm~\ref{alg:online} details the online exploration loop.  The
pseudocode abstracts away implementation details to highlight the main
steps.

\begin{algorithm}[h]
  \caption{Offline Training of Inverse and Forward Models}
  \label{alg:offline}
  \begin{algorithmic}[1]
    \Require multi‑objective function \(\vect{f}\), decision space \(\calX\)
    \Ensure trained inverse model \(\hat{g}\) and forward surrogate \(\hat{f}\)
    \State \textbf{Generate Pareto samples} \(\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}\)
       by running a MOO algorithm (e.g. NSGA‑II)
    \State \textbf{Normalise} decision and objective vectors
    \State \textbf{Split} \(\mathcal{D}\) into training/validation/test sets
    \State \textbf{Select model type} (GP, RBF, RF, FFNN)
    \State \textbf{Tune hyperparameters} via cross‑validation
    \State \textbf{Train inverse model} \(\hat{g}\) on training set
    \State \textbf{Train forward model} \(\hat{f}\) on training set
    \State \textbf{Validate} on test set; select best model
    \State \Return \(\hat{g},\hat{f}\)
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
  \caption{Online Inverse Decision Exploration}
  \label{alg:online}
  \begin{algorithmic}[1]
    \Require trained models \(\hat{g},\hat{f}\), dataset \(\mathcal{D}\)
    \Repeat
      \State \textbf{Get target} objectives \(\vect{y}^\star\) from user
      \State \textbf{Compute plausibility score} using distance to \(\mathcal{D}\)
      \If{score $<\tau$}
        \State display warning and optionally project \(\vect{y}^\star\) onto front
      \EndIf
      \State \textbf{Generate candidates} \(\{\hat{\vect{x}}^{(k)}\}\) via \(\hat{g}\)
      \For{each candidate \(k\)}
        \State evaluate \(\hat{\vect{y}}^{(k)}=\hat{f}(\hat{\vect{x}}^{(k)})\)
        \State compute forward error and distance metrics
      \EndFor
      \State \textbf{Rank} candidates by error, diversity and plausibility
      \State \textbf{Display} top candidates and solicit user feedback
    \Until{user terminates}
  \end{algorithmic}
\end{algorithm}
