%-------------------------------------------------------------------------------
% Introduction
%-------------------------------------------------------------------------------

\chapter{Introduction}

Across science and engineering, we often specify or observe effects and then ask what causes or controllable settings could have produced them. Inverse problems formalise this reversal: the forward direction from parameters to outcomes may be clear through modelling or simulation, yet the reverse direction from outcomes back to plausible parameters can be ambiguous, unstable, or computationally demanding. This tension motivates a large body of work on regularisation and probabilistic formulations that restore robustness and represent uncertainty when direct inversion is not available or not meaningful.

This inverse viewpoint shows up across engineered systems wherever performance is easier to measure than to prescribe directly in terms of underlying parameters. In signal-processing workflows, for example, designers commonly start from a performance template and work backward: one may prescribe a frequency-response shape and seek filter coefficients, specify a spatial beam pattern and seek array weights, target a rate--distortion operating point and seek encoder parameters, or work from desired detection behaviour back to system settings. In these cases, forward evaluation is straightforward, while inverse design becomes the practical bottleneck.


To make this setting precise, let $\calX$ denote the space of controllable decisions $\vect{x}$ and let $\calY$ denote the space of measurable or computable performance descriptors $\vect{y}$. A forward model provides a mapping
\begin{equation}
  f:\calX \to \calY,\qquad \vect{y}=f(\vect{x}),
\end{equation}
implemented by analysis, simulation, or experiment. Inverse design asks the complementary question: given a target pattern $\vect{y}^{\star}\in\calY$, identify decisions $\vect{x}\in\calX$ whose forward outcomes match $\vect{y}^{\star}$ closely enough for the application.


This inverse relationship is rarely a straightforward inversion. Hadamard characterises a problem as well posed when a solution exists, is unique, and depends continuously on the data; inverse problems frequently violate one or more of these conditions~\cite{KabanikhinSurvey}. Instability is especially common: small errors in measured or simulated outcomes, or small specification changes in $\vect{y}^{\star}$, can correspond to large changes in inferred decisions. Practical inverse design therefore requires additional structure to restore robustness, typically through regularisation that encodes preferences and stabilises inference, or through statistical formulations that represent uncertainty and non-uniqueness directly~\cite{Stuart2010}.


Forward-only workflows address inverse questions indirectly by iteratively perturbing $\vect{x}$, re-evaluating the forward model, and repeating until the observed outcomes satisfy the specification. While effective in low-dimensional and cheap-to-evaluate settings, this strategy becomes expensive when forward evaluations are costly and provides limited insight into the inverse relationship itself. In particular, it does not naturally reveal when a target is unattainable, when multiple distinct designs satisfy essentially the same target, or how sensitive a design is to small changes in the specification.

This thesis studies an alternative approach that uses data to make inverse exploration more direct. Given representative examples $\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^N$ generated by the forward model, we learn a decision-mapping surrogate
\begin{equation}
  \hat{g}:\calY \to \calX,
\end{equation}
that can be queried with a target $\vect{y}^{\star}$ to produce candidate decisions intended to realise that target. Modern machine learning models, including neural networks, are well suited to this role because they can approximate complex, high-dimensional nonlinear relationships from data and provide fast proposals at query time. When the inverse relationship is inherently multi-valued, probabilistic and generative modelling ideas provide a natural way to represent families of plausible decisions and to express ambiguity arising from noise, modelling mismatch, or limited data, rather than collapsing the problem to a single design point.



\begin{figure}[h]
  \centering
  \input{figures/inverse_mapper_figure}
  \caption{Forward vs.\ inverse viewpoint. Forward mapping evaluates how decisions $\vect{x}$ produce outcomes $\vect{y}$; the inverse task seeks decisions that realise a desired outcome pattern.}
  \label{fig:forward-inverse}
\end{figure}


Inverse questions arise throughout signal-processing practice. A designer may specify a frequency-response template and seek filter parameters, prescribe a spatial beam pattern and seek array weights, target a rate--distortion operating point and seek encoder settings, or work backward from desired detection performance to system parameters. These examples share the same core difficulty: forward evaluation is typically straightforward, while the inverse mapping from a target outcome to feasible decisions can be ambiguous, sensitive, or even infeasible. This makes data-driven models, particularly modern machine learning methods, a natural tool for proposing candidate designs quickly and supporting interactive exploration under uncertainty.

\begin{figure}[t]
  \centering
  \input{figures/decision_objective_mapping}
  \caption{Illustration of the mapping between the decision space (design parameters $\mathbf{x}$) and the objective space (performance metrics $\mathbf{f}$). The forward problem maps design parameters to properties, while material inverse design seeks the set of optimal parameters $\mathbf{x}$ that satisfy target requirements on the Pareto front.}
  \label{fig:decision_objective_mapping}
\end{figure}


\section{Problem statement and purpose}

We consider inverse decision mapping for multi-criteria signal-processing systems. Assume access to representative examples $\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^N$ with $\vect{x}_i\in\calX$, $\vect{y}_i\in\calY$, and $\vect{y}_i=\vect{f}(\vect{x}_i)$ for a forward map $\vect{f}:\calX\to\calY$. The problem is to learn a data-driven rule that, given a user-specified target $\vect{y}^{\star}\in\calY$, returns one or more candidate decisions $\widehat{\vect{x}}\in\calX$ whose forward evaluations $\vect{f}(\widehat{\vect{x}})$ are close to $\vect{y}^{\star}$ under task-relevant discrepancy measures in objective space. This inverse relationship is generally ill-posed: multiple distinct decisions may correspond to similar outcomes, some targets may have no feasible preimage, and small perturbations in $\vect{y}^{\star}$ or in the observed outcomes can yield large changes in the inferred decision~\cite{KabanikhinSurvey,BenningBurger2018,Stuart2010}.

The purpose of this thesis is to develop and assess a principled approach for inverse decision mapping that is transparent, reproducible, and suitable for interactive use across application domains. The approach uses machine learning models as fast proposal mechanisms for candidate decisions, while treating forward evaluation as the reference for verification and for application-aligned assessment. This combination reflects a broader direction in inverse problems research that leverages data-driven models to accelerate inverse querying without abandoning forward-model grounding.



\section{Research questions}

\begin{enumerate}
  \item Inverse querying. How can a new target $\vect{y}^{\star}$ be converted into one or more concrete candidate decisions $\widehat{\vect{x}}$ using available examples and forward evaluations, while accounting for non-uniqueness and potential infeasibility?
  \item Quality of generated decisions. How should the quality of $\widehat{\vect{x}}$ be assessed against $\vect{y}^{\star}$ using forward verification in a way that is explicit, application-aligned, and reproducible across experiments?
  \item Modelling strategies and generalisation. Which modelling choices most effectively learn mappings from outcomes to decisions, and under what data conditions and problem characteristics do they generalise reliably to unseen targets?
\end{enumerate}



\section{Assumptions, limitations, and delimitations}
\label{sec:assumptions-limitations-and-delimitaions}

We assume access to a finite, publicly available collection of representative pairs
$\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^N$ generated by a forward process
$\vect{y}=\vect{f}(\vect{x})$ (or by a trusted proxy for $\vect{f}$), and we assume that this dataset provides meaningful coverage of the outcome region in which inverse queries are expected. We further assume that outcomes can be compared with a task-relevant discrepancy measure $d_{\calY}(\cdot,\cdot)$, and that candidate decisions can be restricted to the admissible domain through simple parameter bounds or projection when needed.

The primary limitation is coverage: inverse queries that lie outside, or near the boundary of, the outcome region represented in $\mathcal{D}$ may induce extrapolation and unreliable proposals. A related limitation is distribution shift between the published data used for training and the targets posed at deployment, which can degrade inverse accuracy even when the forward relationship itself is unchanged. Finally, the inverse relationship is inherently non-unique in many problems, so any learned inverse model may return only a subset of the feasible decision set and may miss rare or disconnected solution families; generating multiple candidates mitigates this but does not remove the underlying ambiguity.

We delimit the empirical scope to a small set of benchmarks with two-dimensional outcome descriptions and to publicly released datasets of manageable size, chosen to support controlled comparison and reproducible evaluation. The proposed modelling and evaluation principles are stated to extend beyond these delimitations, but claims in this thesis are supported only within the studied settings.

\section{Methodological overview}

The study uses publicly available decision--outcome examples to train a model that supports inverse querying. From a dataset $\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^N$ with $\vect{y}_i=\vect{f}(\vect{x}_i)$, we apply consistent preprocessing to $\calX$ and $\calY$ (including scaling where appropriate) and learn a mapping from target outcomes to candidate decisions. Since the inverse relationship is commonly one-to-many, the model is used to generate a set of candidates for a single target: for a query $\vect{y}^{\star}$ it returns $\{\widehat{\vect{x}}^{(k)}\}_{k=1}^K$, where $K$ controls the number of proposals and diversity arises from model stochasticity or sampling.

Candidate quality is established through forward verification. For each proposal, we compute or approximate $\vect{f}(\widehat{\vect{x}}^{(k)})$ and measure discrepancy to the target using a task-relevant outcome-space distance $d_{\calY}(\vect{f}(\widehat{\vect{x}}^{(k)}),\vect{y}^{\star})$. The method returns one or more candidates according to an explicit selection rule (for example, the lowest-discrepancy proposal), and evaluation on held-out targets reports both discrepancy and tolerance-based success. Because the output is a set, primary performance summaries are reported in set form, such as the best-achieved discrepancy over $K$ candidates per target, together with statistics that show how performance varies with $K$ and with target location relative to the outcome region covered by the data.



%-------------------------------------------------------------------------------
% Significance and contributions (suggested addition)
% Place after "Problem statement and purpose" (or after "Research questions")
%-------------------------------------------------------------------------------
\section{Significance and contributions}
Inverse design turns a familiar engineering workflow inside out: instead of asking what a given decision produces, we ask which decisions can realise a desired outcome. In signal-processing systems, that ``reverse'' question often dominates the real cost of design because it forces repeated forward evaluations, manual tuning, and ad hoc compromises when objectives conflict. The result is not only computational expense but also reduced transparency: it becomes difficult to tell whether a target is feasible, how many qualitatively different designs meet it, and how sensitive any proposed design is to small specification changes. A methodology that makes inverse exploration faster and more explicit therefore matters for both practical iteration and scientific understanding of the design space.

This thesis argues that data-driven inverse decision mapping can support that goal when it is paired with forward-model verification and evaluation protocols that respect non-uniqueness. Rather than treating the inverse map as a single ``answer,'' we treat it as a mechanism for proposing sets of candidate decisions, and we judge those candidates using the same forward process that defines the task. Concretely, the thesis makes the following contributions within the scope described in Section~\ref{sec:assumptions-limitations-and-delimitaions}:
\begin{itemize}
  \item A clear formulation of inverse decision mapping for multi-criteria targets.
  We cast inverse design as learning a mapping from outcome space to decision space under ill-posedness, making feasibility, ambiguity, and sensitivity first-class concerns rather than afterthoughts.

  \item A reproducible inverse-querying workflow grounded in forward verification.
  We separate offline model fitting from online querying, generate multiple candidate decisions for a single target, and verify candidate quality through forward evaluation using explicit discrepancy measures in outcome space.

  \item Set-based evaluation principles for one-to-many inverse mappings.
  Because inverse solutions are typically not unique, we report performance in forms that reflect what a user actually receives (a candidate set), including best-achieved discrepancy over $K$ proposals and tolerance-based success rates as functions of $K$ and target location relative to data coverage.

  \item An empirical study of modelling choices and generalisation behaviour.
  We compare modelling strategies for learning outcome-to-decision relationships and identify practical conditions under which they generalise reliably to unseen targets, highlighting failure modes tied to coverage and distribution shift.
\end{itemize}
Together, these contributions aim to make inverse design more interactive and more falsifiable: the model proposes, the forward process verifies, and the evaluation makes clear what worked, what failed, and why that matters for deploying inverse querying in practice.

%-------------------------------------------------------------------------------
% Dissertation structure (suggested addition)
% Place at the end of Chapter 1
%-------------------------------------------------------------------------------
\section{Thesis structure}
The remainder of this dissertation is organised as follows. Chapter~2 reviews background on inverse problems, ill-posedness, and data-driven approaches to inverse design, with emphasis on methods that represent non-uniqueness and uncertainty. Chapter~3 describes the proposed inverse-querying pipeline, including data preparation, model training, candidate generation, and the forward-verification protocol used for assessment. Chapter~4 presents experimental results on the selected benchmarks, reporting both discrepancy-based and tolerance-based metrics and analysing how performance varies with the number of generated candidates and with target location relative to the observed outcome region. Chapter~5 discusses the implications of these findings, including limitations, practical guidance for model selection, and open challenges for extending inverse querying beyond the controlled settings studied here. Chapter~6 concludes by summarising the main contributions and outlining directions for future work.

