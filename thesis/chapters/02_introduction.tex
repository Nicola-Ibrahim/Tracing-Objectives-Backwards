%-------------------------------------------------------------------------------
% Introduction
%-------------------------------------------------------------------------------

\chapter{Introduction}

Real-world decision making involves navigating a web of trade-offs. In many
signal-processing and systems contexts—filter design, beamforming, detection,
compression, and sensing—practitioners must balance several objectives that
often conflict. This is the remit of multi-objective optimisation (MOO), where
multiple goals are considered simultaneously rather than collapsed into a
single metric~\cite{Tan2023}. Unlike single-objective optimisation, MOO
typically yields a \emph{set of trade-off designs} in both decision and
objective spaces, from which a decision maker must choose according to
preferences and constraints. For a broad overview of MOO concepts and
state-of-the-art algorithmic families, see Deb’s classic introduction and
more recent tutorials. 

A natural way to analyse such problems is through the \emph{forward} process:
a model maps hidden or controllable parameters $\vect{x}$ (decisions) to
measurable or evaluable quantities $\vect{y}$ (objectives), reflecting how
simulation tools are usually specified. In many applications the forward map
is well understood, but the complementary \emph{inverse} task—inferring
$\vect{x}$ from a desired pattern of objective values $\vect{y}^\star$—is
substantially harder. Information is typically lost in the forward direction,
so the inverse problem can violate one or more of Hadamard’s criteria
(existence, uniqueness, stability), i.e., it is \emph{ill-posed}; different
decisions may yield indistinguishable outcomes, and small perturbations in
data can lead to large changes in inferred parameters. Regularisation and
statistical formulations are therefore required to make inference meaningful
and robust~\cite{KabanikhinSurvey,BenningBurger2018,Stuart2010}. 

From a design perspective, the limitations of purely forward workflows are
well known. When engineers wish to realise a particular objective pattern
(e.g., a specified error–rate profile, resolution–noise compromise, or
spectral mask), they must repeatedly perturb $\vect{x}$ and re-evaluate the
forward model, or they resort to scalarisation tricks that can be sensitive to
weights and may miss diverse solutions. Such procedures are compute-intensive
and offer limited insight into the \emph{structure} of the inverse map,
especially when many distinct decisions produce similar outcomes. In practice,
useful inverse exploration should: (i) acknowledge that solutions may be
non-unique and sometimes unattainable; (ii) expose families of viable designs
rather than a single point; and (iii) integrate smoothly with forward
evaluation when refinement is needed.

\begin{figure}[t]
  \centering
  \input{figures/inverse_mapper_figure}
  \caption{Forward vs.\ inverse viewpoint. Forward mapping evaluates how decisions $\vect{x}$ produce outcomes $\vect{y}$; the inverse task seeks decisions that realise a desired outcome pattern.}
  \label{fig:forward-inverse}
  \label{fig:forward-inverse}
\end{figure}

This thesis addresses the inverse decision–mapping problem in multi-objective
settings. The central idea is to learn a data-driven mapping
$\hat{g}\colon \calY \to \calX$ from representative examples of decisions and
their corresponding outcomes. At query time, a user specifies a desired
objective pattern $\vect{y}^{\star}$; the model proposes candidate $\vect{x}$
values intended to realise $\vect{y}^{\star}$, which can then be verified or
fine-tuned through a forward evaluation step. By shifting effort into model
construction, the workflow enables responsive, interactive exploration of
complex design spaces while remaining agnostic to the source of examples
(e.g., historical runs, prior designs, or samples obtained by exploratory
optimisation).

In addressing these challenges, we consider both deterministic and
probabilistic modelling strategies as complementary tools for inverse
exploration. Deterministic models aim to produce a single representative
decision for a given target, often aided by regularisation to counteract
ill-posedness and to encourage stable, interpretable mappings. They are
appealing when fast, repeatable proposals are required. Probabilistic models,
in contrast, represent uncertainty explicitly by learning distributions over
decisions (or functions). This yields predictive variability that can capture
multi-valued relationships between decisions and objectives, quantify
ambiguity due to noise or limited data, and support diversity in candidate
solutions. In the broader literature, probabilistic formulations (e.g., Bayesian perspectives on inverse problems) provide a principled language for reasoning about non-uniqueness and data
uncertainty, while modern regularisation connects statistical and variational
viewpoints~\cite{Stuart2010,BenningBurger2018}. 


Inverse questions arise naturally across signal-processing tasks. In filter
design, engineers may target a specific passband/stopband pattern subject to
resource limits and then seek parameterisations that realise it. In adaptive
beamforming, one might prescribe mainlobe width and sidelobe levels compatible
with interference constraints and search for array weights that achieve these
objectives. In compression, a target operating point in the rate–distortion
plane motivates encoders and transforms that satisfy both fidelity and bitrate
budget. In detection and estimation, practitioners often work backward from
desired receiver operating characteristics to decision rules and system
settings. These scenarios share the same core difficulty: the forward map is
easier to evaluate, whereas the inverse map can be ambiguous or unstable, so
algorithms that \emph{learn to invert} from data are attractive.


\section{Contributions}

The contributions of this thesis are as follows:

\begin{itemize}
  \item \textbf{Problem formulation.} We formally define the decision
  mapping problem in the context of MOO and highlight its ill‑posed
  nature.  Building on recent work in Pareto estimation and inverse
  machine learning~\cite{Tan2023}, we specify conditions under
  which a target objective vector is admissible and propose criteria for
  assessing the quality of inverse solutions.

  \item \textbf{Comprehensive background.} A detailed survey of
  multi‑objective optimisation methods, performance indicators and
  surrogate‑assisted algorithms is presented.  We review evolutionary
  algorithms, decomposition techniques and quality indicators such as
  generational distance and hypervolume~\cite{Blank2020}.  We
  further survey inverse modelling approaches, including radial basis
  function networks, Gaussian processes and transfer learning methods
  that map objective vectors back to decision space~\cite{Liu2024}.

  \item \textbf{Modular framework.} We design a modular methodology with
  offline and online phases.  In the offline phase an existing MOO
  algorithm approximates the Pareto front, and a surrogate inverse model is
  trained.  In the online phase the user queries the model with a target
  objective vector, a plausibility check determines proximity to the Pareto
  region and candidate decisions are produced and validated.  The framework
  accommodates different surrogate models and incorporates ranking metrics
  to guide the user.

  \item \textbf{Implementation and prototype.} A reproducible software
  prototype is implemented, including routines for data generation, normal-
  isation, model training, hyperparameter optimisation, interactive
  visualisation and ranking.  The code is modular, making it easy to plug
  in alternative surrogate models or optimisation backends.

  \item \textbf{Empirical evaluation.} Extensive experiments on synthetic
  benchmark functions and a real‑world case study demonstrate that the
  learned inverse models yield decision vectors close to the true Pareto
  set.  We compare Gaussian processes, radial basis functions and
  neural networks under varying training set sizes and analyse trade‑offs
  between accuracy and computation time.  Results indicate that the
  proposed framework significantly reduces the number of forward evaluations
  required to identify satisfactory solutions.

\end{itemize}

\section{Structure of the Thesis}

The remainder of this thesis is organised as follows.  Chapter~\ref{chap:motivation}
provides background on multi‑objective optimisation, introduces definitions of
dominance and Pareto optimality, reviews key algorithms and metrics, and
motivates the need for inverse mapping.  Chapter~\ref{chap:related}
surveys related work on inverse modelling, surrogate‑assisted optimisation,
transfer learning and interactive decision support.  Chapter~\ref{chap:problem}
formalises the inverse decision mapping problem, introduces feasibility and
quality criteria and poses research questions.  Chapter~\ref{chap:method}
describes the proposed methodology in detail, including data generation,
surrogate training and online exploration.  Chapter~\ref{chap:implementation}
presents implementation details and algorithmic components.  Chapter~\ref{chap:experiments}
contains experimental results on synthetic and real problems.  Chapter
\ref{chap:discussion} discusses the insights gained from the experiments and
outlines limitations.  Finally, Chapter~\ref{chap:conclusion} concludes the
thesis and proposes future directions.