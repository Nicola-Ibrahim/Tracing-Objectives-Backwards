%-------------------------------------------------------------------------------
% Background and Motivation
%-------------------------------------------------------------------------------

\chapter{Background and Motivation}\label{chap:motivation}

Inverse decision mapping builds on ideas from multi-objective optimisation (MOO)
and inverse problems. This chapter introduces (i) a compact view of MOO and
trade-off sets, (ii) the forward–inverse perspective and why inverse problems
are often ill-posed, and (iii) practical ways to obtain representative
\((\vect{x},\vect{y})\) pairs for learning an inverse map. We keep the
discussion agnostic to any specific optimiser; Pareto-based tools are one
useful option among several. For broad MOO fundamentals and assessment, see
Deb~\cite{Deb2011Intro} and the tutorial by Emmerich \& Deutz~\cite{Emmerich2018Tutorial}.

\section{Multi-Objective Optimisation in Brief}

A multi-objective optimisation problem (MOP) is defined by a vector of decision variables \(\vect{x}=(x_1,\dots,x_n)\in\calX\subset \R^n\) and a vector of objective functions \(\vect{f}=(f_1,\dots,f_m)\colon \calX\to \calY\subset \R^m\). Without loss of generality we consider all objectives to be minimised. A MOP can be written compactly as

\begin{align} 
\min_{\vect{x}\in\calX}\; \vect{f}(\vect{x}) &= \big(f_1(\vect{x}),\dots,f_m(\vect{x})\big),\\ \text{s.t.}\; g_j(\vect{x}) &\le 0,\quad j=1,\dots,J,\\ h_k(\vect{x}) &= 0,\quad k=1,\dots,K, 
\label{eq:mop}
\end{align}


where \(g_j\) and \(h_k\) denote inequality and equality constraints, respectively. The feasible region \(\calX\) is assumed to be a compact, connected subset of \(\R^n\). Each objective \(f_i\) may be non-convex, nonlinear and expensive to evaluate. Because objectives typically conflict, the goal is a \emph{set of trade-off
designs} rather than a single optimum. A standard partial order uses
\emph{Pareto dominance}: for \(\vect{u},\vect{v}\in\R^m\),
\(\vect{u}\preceq\vect{v}\) iff \(u_i\le v_i\ \forall i\) and \(u_j<v_j\) for
some \(j\). A feasible \(\vect{x}^\star\) is Pareto-optimal if no other
\(\vect{x}\) satisfies \(\vect{f}(\vect{x})\preceq \vect{f}(\vect{x}^\star)\).
See the references above for a concise survey of algorithmic families and
assessment practices.
MOPs arise in a broad spectrum of application domains, including engineering, economics, logistics, manufacturing and science~\cite{Tan2023}. In practice, objective functions often conflict: improving one performance metric deteriorates another. 



\section{Forward and Inverse Viewpoints}

The \emph{forward} process evaluates how decisions produce outcomes:
\begin{equation}
  \vect{y} \;=\; \vect{f}(\vect{x}) \;+\; \boldsymbol{\varepsilon},
  \qquad \vect{x}\in\calX,\ \vect{y}\in\calY,
  \label{eq:forward}
\end{equation}
where \(\boldsymbol{\varepsilon}\) represents measurement/model error. Inverse
exploration asks for decisions that realise a desired outcome pattern
\(\vect{y}^{\star}\): find \(\vect{x}\) with \(\vect{f}(\vect{x})\approx
\vect{y}^{\star}\). Inverse problems often fail Hadamard’s well-posedness
criteria (existence, uniqueness, stability) because many distinct decisions
can map to similar outcomes and small output errors can induce large changes
in the inferred input—i.e., they are \emph{ill-posed}. \cite{KabanikhinSurvey,Stuart2010}

Two complementary formalisms are widely used to make the inverse task
meaningful and robust:

\paragraph{Regularised optimisation.}
Seek \(\vect{x}\) by solving
\begin{equation}
  \widehat{\vect{x}}
  \;\in\; \arg\min_{\vect{x}\in\calX}
  \underbrace{\mathcal{L}\big(\vect{f}(\vect{x}),\,\vect{y}^{\star}\big)}_{\text{data-fit}}
  \;+\;
  \lambda\,\underbrace{R(\vect{x})}_{\text{regulariser}},
  \label{eq:tikhonov}
\end{equation}
where \(\mathcal{L}\) measures mismatch (e.g., squared or Mahalanobis distance)
and \(R\) encodes prior structure (e.g., smoothness, sparsity, bounded
complexity). Regularisation restores stability to ill-posed inverses by
penalising implausible solutions. \cite{BenningBurger2018}

\paragraph{Probabilistic/Bayesian formulation.}
Treat \(\vect{x}\) as random and infer the posterior
\begin{equation}
  p(\vect{x}\mid \vect{y}^{\star})
  \;\propto\;
  \underbrace{p(\vect{y}^{\star}\mid \vect{x})}_{\text{likelihood}}
  \;\underbrace{p(\vect{x})}_{\text{prior}},
  \label{eq:posterior}
\end{equation}
which quantifies uncertainty and captures multi-valued relationships between
decisions and outcomes. Point estimates (MAP, posterior mean) and samples
provide candidate designs and uncertainty summaries. \cite{Stuart2010}

In this thesis we adopt both \emph{deterministic} (single-solution, typically
regularised) and \emph{probabilistic} (distributional) models as complementary
tools for inverse exploration, in line with standard inverse-problems practice.
\cite{BenningBurger2018,Stuart2010}

\section{Obtaining \texorpdfstring{$(\vect{x},\vect{y})$}{(x,y)} Examples for Learning}

Learning a data-driven inverse map \(\hat{g}:\calY\!\to\!\calX\) requires a set
\(\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^{N}\) with \(\vect{y}_i=
\vect{f}(\vect{x}_i)\). In practice, \(\mathcal{D}\) can be built from:
\begin{itemize}
  \item \textbf{Historical runs and prior designs} (e.g., archives, logs).
  \item \textbf{Simulation sweeps/DoE} that span \(\calX\) and reveal trade-offs.
  \item \textbf{Exploratory optimisation}, including evolutionary, decomposition-
        based, or surrogate-assisted methods that generate diverse samples.
\end{itemize}
Our approach is \emph{agnostic} to how \(\mathcal{D}\) is obtained; it consumes
input–output examples without relying on a single optimisation family. For
surveys covering these algorithmic paradigms and assessment, see
Deb; Emmerich \& Deutz. \cite{Deb2011Intro,Emmerich2018Tutorial}



\section{Why Inverse Exploration?}

Forward workflows excel at mapping \(\vect{x}\!\to\!\vect{y}\) and sketching
trade-offs, but they do not directly solve the inverse query “which decisions
are likely to realise this outcome pattern \(\vect{y}^{\star}\)?” Framing the
task as an ill-posed inverse problem (Eqs.~\ref{eq:tikhonov}–\ref{eq:posterior})
supports both fast, regularised point proposals and uncertainty-aware,
distributional solutions. This viewpoint motivates the data-driven inverse
methodology developed in the following chapters. \cite{BenningBurger2018,Stuart2010}



\section{Modeling Background}\label{sec:modelling_background}

The data-driven models address inverse exploration in multi-objective settings. We therefore outline two complementary and widely used modelling perspectives: a \emph{deterministic} view that learns a single mapping and a \emph{probabilistic} view that models full conditional distributions. Both are standard in machine learning and statistical learning, and both can be framed within risk minimisation and decision-theoretic principles.

\subsection{Learning as Risk Minimisation}

Let $\mathcal{D}=\{(\vect{y}_i,\vect{x}_i)\}_{i=1}^{N}$ denote examples of outcomes and decisions. In inverse exploration we seek a function $g$ that maps outcomes to decisions, $g:\mathcal{Y}\to\mathcal{X}$, such that $g(\vect{y})$ produces decisions aligned with the target outcome pattern. Learning $g$ can be cast as minimising the (unknown) expected risk
\begin{equation}
  \mathcal{R}(g)
  \;=\;
  \mathbb{E}_{(\vect{y},\vect{x})}\!\left[\,
    \ell\big(\,g(\vect{y})\,,\,\vect{x}\,\big)
  \right],
  \label{eq:true_risk}
\end{equation}
with a task-appropriate loss $\ell(\cdot,\cdot)$ (e.g., squared error for regression‐type inverse mappings or constraint-aware penalties when feasibility must be respected)~\cite{Hastie2009ESL,Bishop2006PRML}.

In practice, $\mathcal{R}(g)$ is approximated by the \emph{empirical risk} over $\mathcal{D}$, augmented by a regulariser to promote stability and encode prior structure:
\begin{equation}
  \widehat{g}
  \;\in\;
  \arg\min_{g\in\mathcal{G}}
  \underbrace{\frac{1}{N}\sum_{i=1}^{N}\ell\!\left(g(\vect{y}_i),\,\vect{x}_i\right)}_{\text{empirical risk}}
  \;+\;
  \lambda\,\underbrace{\Omega(g)}_{\text{regularisation}}.
  \label{eq:erm}
\end{equation}
Here $\mathcal{G}$ is a hypothesis space (e.g., linear models, kernels, neural networks), $\Omega$ encodes smoothness, sparsity, or complexity control, and $\lambda\!\ge\!0$ balances data fit and model complexity~\cite{Hastie2009ESL,Bishop2006PRML}. Regularisation curbs overfitting and mitigates instability typical of ill-posed inverse problems.

\paragraph{Connection to the forward model.}
When prior knowledge about the forward map $\vect{f}$ is available, one can incorporate it via a data-fit term that aligns $g(\vect{y})$ with decisions whose forward evaluation is consistent with $\vect{y}$. A generic formulation is
\begin{equation}
  \widehat{g}
  \;\in\;
  \arg\min_{g\in\mathcal{G}}
  \frac{1}{N}\sum_{i=1}^{N}
  \underbrace{\Big[
    \alpha\,\ell\!\left(g(\vect{y}_i),\,\vect{x}_i\right)
    \;+\;
    (1-\alpha)\,\mathcal{L}\!\left(\vect{f}\!\big(g(\vect{y}_i)\big),\,\vect{y}_i\right)
  \Big]}_{\text{inverse consistency + forward consistency}}
  \;+\; \lambda\,\Omega(g),
  \qquad \alpha\in[0,1],
  \label{eq:hybrid}
\end{equation}
where $\mathcal{L}$ penalises discrepancy in outcome space. This blends purely data-driven inverse learning with physics‐ or simulation-aware consistency~\cite{Bishop2006PRML,Murphy2022PML1}.


\paragraph{Representation, inductive bias, and constraints.}
Our objective is to learn an inverse mapping $g:\mathcal{Y}\!\to\!\mathcal{X}$ that proposes
decisions aligned with a target outcome pattern. The central design choices are \emph{representation}
(the hypothesis class $\mathcal{G}$), \emph{inductive bias} (regularisation and architectural
constraints that stabilise ill-posed inverses), and \emph{feasibility handling} (how domain
constraints are respected). These choices determine not only data fit but also robustness,
generalisation beyond the training outcomes, and the diversity/credibility of proposed
decisions~\cite{Bishop2006PRML,Hastie2009ESL,Murphy2022PML1}. In all cases, learning can be
framed as principled risk minimisation or (for probabilistic models) likelihood-based estimation,
with evaluation grounded in predictive performance and, when distributions are produced,
proper scoring rules and decision-theoretic criteria.

\subsection{Deterministic Modelling}\label{sec:deterministic_modelling}
In the deterministic view, the inverse model returns a \emph{single} representative decision
for each target outcome. Learning $g:\mathcal{Y}\!\to\!\mathcal{X}$ is posed as regularised
empirical risk minimisation
\begin{equation}
  \widehat{g}
  \;\in\;
  \arg\min_{g\in\mathcal{G}}
  \frac{1}{N}\sum_{i=1}^{N}\ell\!\big(g(\vect{y}_i),\,\vect{x}_i\big)
  \;+\; \lambda\,\Omega(g),
  \label{eq:erm_det}
\end{equation}
where $\ell$ is a task loss (e.g., $\ell(\hat{\vect{x}},\vect{x})=\|\hat{\vect{x}}-\vect{x}\|_2^2$)
and $\Omega(g)$ encodes inductive bias (e.g., weight decay, Lipschitz/Jacobian penalties, or
early stopping) to curb overfitting and to stabilise the estimate in the presence of
non-uniqueness and no

isy data. Hyperparameters (including $\lambda$) are tuned by validation
or cross-validation to balance bias–variance and ensure generalisation to unseen queries
$\vect{y}^\star$.

\begin{figure}[tbp]
  \centering
  \begin{adjustbox}{max width=\linewidth}
    \input{figures/deterministic_modeling} 
  \end{adjustbox}
  \caption{Deterministic modelling pipeline with regularised ERM and feasibility handling.}
  \label{fig:deterministic-model}
\end{figure}



When decisions must satisfy feasibility constraints $g_j(\vect{x})\!\le\!0$ and $h_k(\vect{x})\!=\!0$
(cf.\ Eq.~\eqref{eq:mop}), training can promote feasibility either by projection onto a feasible set
$\Pi_{\calX}$ or via soft penalties:
\begin{equation}
  \widehat{g}
  \;\in\;
  \arg\min_{g}
  \frac{1}{N}\sum_{i=1}^{N}
  \Big[
     \ell\!\big(\Pi_{\calX}(g(\vect{y}_i)),\,\vect{x}_i\big)
     \;+\;
     \beta\!\sum_{j}\![\,g_j(g(\vect{y}_i))\,]_+^{\!2}
     \;+\;
     \beta'\!\sum_{k}\! h_k(g(\vect{y}_i))^{2}
  \Big]
  \;+\; \lambda\,\Omega(g),
  \label{eq:constraints_det}
\end{equation}
with $[t]_+=\max\{t,0\}$. Such Tikhonov-style penalties stabilise ill-posed inverses by discouraging
implausible or infeasible proposals while keeping the output single-valued.

Deterministic outputs are well-suited for fast, repeatable proposals—e.g., as starting points for
expert refinement or subsequent verification—yet they \emph{collapse} potentially multi-valued inverse
relations to one point and do not natively quantify uncertainty or capture diversity in candidate
decisions. This motivates the complementary probabilistic treatment presented next.



\subsection{Probabilistic Modelling}

The probabilistic view represents \emph{distributions} over outputs conditional on inputs, thereby quantifying uncertainty and naturally handling multi-valued input--output relationships~\cite{Murphy2022PML1,Gelman2013BDA3}. A conditional model $p_\theta(\vect{y}\mid\vect{x})$ is fit by maximising the (regularised) conditional log-likelihood:
\begin{equation}
  \widehat{\theta}
  \;\in\;
  \arg\max_{\theta}
  \bigg[
  \sum_{i=1}^{N}\log p_{\theta}\!\big(\vect{y}_i \mid \vect{x}_i\big)
  \;-\; \gamma\,\mathcal{R}(\theta)
  \bigg],
  \label{eq:mle}
\end{equation}
where $\mathcal{R}(\theta)$ controls complexity. Given a new input $\vect{x}^{\star}$, one may form \emph{predictive summaries}:
\begin{align}
  \text{(point)}\quad
  \widehat{\vect{y}}_{\text{MAP}}(\vect{x}^{\star}) 
  &= \arg\max_{\vect{y}}\, p_{\widehat{\theta}}(\vect{y}\mid \vect{x}^{\star}),
  \label{eq:map_point}\\[2pt]
  \text{(predictive)}\quad
  p(\vect{y}\mid \vect{x}^{\star},\mathcal{D})
  &= \int p_{\theta}(\vect{y}\mid \vect{x}^{\star})\,
     p(\theta\mid\mathcal{D})\,\mathrm{d}\theta
     \;\approx\; p_{\widehat{\theta}}(\vect{y}\mid \vect{x}^{\star}),
  \label{eq:predictive}
\end{align}
where the Bayesian predictive integrates parameter uncertainty whereas the plug-in predictive uses $\widehat{\theta}$~\cite{Gelman2013BDA3,Murphy2022PML1}. Such distributions support sampling diverse candidate outputs, credible regions in output space, and uncertainty-aware ranking.

\begin{figure}[tbp]
  \centering
  \begin{adjustbox}{max width=\linewidth}
    \input{figures/probabilistic_modeling}
  \end{adjustbox}
  \caption{Probabilistic modelling via conditional density $p_\theta(\vect{y}\mid\vect{x})$, sampling, and decision rule.}
  \label{fig:probabilistic-model}
\end{figure}

\paragraph{Aleatoric vs.\ epistemic uncertainty.}
Probabilistic models distinguish data (aleatoric) uncertainty from model (epistemic) uncertainty; the former reflects intrinsic variability/noise in $\vect{y}\mid\vect{x}$, while the latter arises from limited data or model misspecification. This distinction is key when communicating confidence in recommendations~\cite{Murphy2022PML1,Bishop2006PRML}.

\subsection{Decision-Theoretic Perspective}

Whether one uses a deterministic point estimate or a full predictive distribution, choosing a \emph{reported} output $\tilde{\vect{y}}$ for an input $\vect{x}^{\star}$ is a decision problem. Under a utility (negative loss) $u(\tilde{\vect{y}},\vect{y})$, the Bayes decision rule selects
\begin{equation}
  \tilde{\vect{y}}^{\star}
  \;\in\;
  \arg\max_{\tilde{\vect{y}}\in\calY}
  \mathbb{E}_{\vect{y}\sim p(\vect{y}\mid \vect{x}^{\star},\mathcal{D})}
  \big[u(\tilde{\vect{y}},\vect{y})\big],
  \label{eq:bayes_decision}
\end{equation}
or, equivalently, minimises expected loss. For quadratic loss, the posterior \emph{mean} is optimal; for absolute deviation, the posterior \emph{median} is optimal; for zero–one loss, a \emph{MAP} decision is natural~\cite{Gelman2013BDA3,Murphy2022PML1}. This connects predictive distributions (probabilistic view) and point-valued recommendations (deterministic outputs) through a common principle.

In summary, deterministic models (Eq.~\ref{eq:erm_det}) provide \emph{fast, regularised} proposals that are easy to deploy and refine with forward evaluation, while probabilistic models (Eqs.~\ref{eq:mle}–\ref{eq:predictive}) provide \emph{uncertainty-aware} recommendations and support sampling of multiple plausible outputs when the mapping is multi-valued. Both viewpoints will be used to evaluate inverse exploration in multi-objective settings, guided by the principles above and without committing to a specific algorithmic family~\cite{Bishop2006PRML,Murphy2022PML1,Hastie2009ESL,Gelman2013BDA3}.



\section{Inverse Exploration: Data, Assumptions, and Evaluation}\label{sec:inverse_data_eval}

\subsection{Inverse models in context}
An inverse model aims to propose decisions $\vect{x}\in\mathcal{X}$ that are likely to realise
a desired outcome pattern $\vect{y}^{\star}\in\mathcal{Y}$. Rather than tying the learning
process to a particular way of generating examples (e.g., forward Pareto sweeps), we treat
inverse exploration as a data-driven task that uses any representative set
$\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^{N}$—from historical logs, simulation sweeps,
or exploratory optimisation—together with principled modelling (deterministic or
probabilistic) to deal with ill-posedness (non-uniqueness, instability) and to quantify
uncertainty where appropriate. This view is consistent with modern treatments of
multi-objective optimisation and inverse problems. \cite{Emmerich2018Tutorial,Miettinen1999,Stuart2010,BenningBurger2018}

\subsection{Assumptions and identifiability}
For clarity, we separate modelling assumptions into: (i) \emph{structural} assumptions on the
inverse map $g:\mathcal{Y}\!\to\!\mathcal{X}$ (smoothness, sparsity, Lipschitz bounds),
(ii) \emph{feasibility} constraints $g_j(\vect{x})\!\le\!0$, $h_k(\vect{x})\!=\!0$ inherited from
the design domain (cf.\ Eq.~\eqref{eq:mop}), and (iii) \emph{data} assumptions (coverage,
noise properties). When many $\vect{x}$ produce similar $\vect{y}$, the inverse is
non-unique; well-posedness is recovered by regularisation or by adopting a probabilistic
posterior/predictive view that represents distributions over $\vect{x}$ conditional on
$\vect{y}$.\footnote{See \cite{Stuart2010,KaipioSomersalo2005,Tarantola2005} for detailed
treatments of existence/uniqueness/stability and statistical formulations.} In practice, the
training and query distributions may differ (dataset/covariate shift), which motivates
validation protocols and uncertainty reporting that are explicit about the deployment
setting. \cite{QuinoneroCandela2009}

\subsection{Deterministic vs.\ probabilistic objectives (summary)}
Deterministic models estimate a single decision $\widehat{\vect{x}}=g(\vect{y}^{\star})$ via
regularised empirical risk (Eq.~\eqref{eq:erm_det}). They are attractive for fast, repeatable
proposals, but collapse multi-valued relations to a point. Probabilistic models learn
$p_{\theta}(\vect{x}\mid\vect{y})$ by maximising conditional log-likelihood (Eq.~\eqref{eq:nll}),
enabling uncertainty quantification, multimodality and decision-theoretic recommendations
(Eqs.~\eqref{eq:map}–\eqref{eq:pred}). \cite{Bishop2006PRML,Murphy2022PML1,Gelman2013BDA3}

\subsection{Inverse-centric evaluation metrics}
Evaluation should reflect the inverse goal “propose decisions that realise $\vect{y}^{\star}$”
and the nature of the model (point vs.\ distribution). We therefore use complementary,
task-aligned criteria:

\paragraph{Target realisation (outcome space).}
For a reported decision $\widehat{\vect{x}}$, measure the discrepancy in $\mathcal{Y}$:
\begin{equation}
  \mathcal{E}_{\mathcal{Y}}(\widehat{\vect{x}},\vect{y}^{\star})
  \;=\;
  d\!\left(\,\vect{f}(\widehat{\vect{x}})\,,\,\vect{y}^{\star}\right),
  \qquad
  d(\cdot,\cdot)\ \text{a task-appropriate distance (e.g., $\ell_2$ or Mahalanobis)}.
  \label{eq:target_error}
\end{equation}
When $\vect{f}$ is expensive, a cheaper proxy $d_{\text{surf}}(\widehat{\vect{x}},\vect{y}^{\star})$
based on a validated surrogate of $\vect{f}$ may be used, with explicit caveats.

\paragraph{Feasibility.}
Report the fraction of proposed decisions satisfying domain constraints:
\begin{equation}
  \mathrm{Feas} \;=\; \frac{1}{M}\sum_{m=1}^{M}
  \mathbb{I}\!\Big[\,g_j(\widehat{\vect{x}}^{(m)})\!\le\!0\ \forall j,\ \ h_k(\widehat{\vect{x}}^{(m)})\!=\!0\ \forall k\,\Big],
  \label{eq:feas}
\end{equation}
for $M$ queries $\{\vect{y}^{\star (m)}\}$.

\paragraph{Decision-space diversity.}
When multiple candidates $\{\widehat{\vect{x}}^{(m,r)}\}_{r=1}^{R}$ are proposed per target
(e.g., via stochastic sampling), quantify coverage via the average pairwise distance:
\begin{equation}
  \mathrm{Div}
  \;=\;
  \frac{2}{R(R-1)}\sum_{r<s}\|\widehat{\vect{x}}^{(m,r)}-\widehat{\vect{x}}^{(m,s)}\|_2,
  \label{eq:diversity}
\end{equation}
or a problem-specific dispersion index.

\paragraph{Uncertainty quality (probabilistic models).}
Use proper scoring rules on held-out pairs $(\vect{y}_i,\vect{x}_i)$, e.g.\ negative
log-likelihood (NLL) $\;-\!\sum_i\log p_{\widehat{\theta}}(\vect{x}_i\mid\vect{y}_i)$ or
CRPS for continuous targets, which jointly reward calibration and sharpness of predictive
distributions. \cite{Gneiting2007JASA}

\paragraph{Compute/queries.}
Report forward evaluations (or wall-clock) per query $\vect{y}^{\star}$, to make efficiency
comparisons transparent across models and datasets.

\subsection{Practical data considerations}
To support robust inverse learning, (i) normalise or standardise variables in $\mathcal{X}$ and
$\mathcal{Y}$; (ii) stratify train/validation/test splits across the range of outcomes of
interest; (iii) document how $\mathcal{D}$ was obtained (historical logs, DoE sweeps, exploratory
runs) and any domain shifts between training and deployment; and (iv) report sensitivity to the
size and composition of $\mathcal{D}$. These practices reflect common ML and inverse-problems
guidelines and ensure that inverse results are interpretable and reproducible. \cite{Emmerich2018Tutorial,Miettinen1999,Stuart2010,QuinoneroCandela2009}
