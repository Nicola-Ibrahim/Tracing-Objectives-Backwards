%-------------------------------------------------------------------------------
% Related work
%-------------------------------------------------------------------------------

\chapter{Related Work}\label{chap:related}


This chapter reviews research that tackles the reverse direction of design: starting from a desired pattern of objectives and reasoning back to actionable decision suggestions. We organize the discussion around approaches that embed inverse reasoning within evolutionary search, learn inverse surrogates from data for post-hoc or interactive use, adopt probabilistic or generative views to reflect non-uniqueness and uncertainty, and formalize inverse multi-objective inference to address robustness and identifiability. The goal is to clarify the assumptions these works make about data, feasibility, and uncertainty, and to distill what kind of guidance they ultimately offer to a practitioner who sets targets and seeks viable designs.

Early work in evolutionary multi-objective optimization (EMO) relied on variation operators and selection applied directly in decision space. Subsequent model-based approaches are commonly grouped into three strands: estimation-of-distribution algorithms (EDAs), surrogate (forward) modeling, and inverse modeling, where a mapping is learned from objective space back to decision space \cite{Gholamnezhad2022}. Inverse modeling within the optimization loop was established by IM-MOEA, which builds Gaussian-process (GP) inverse models on non-dominated solutions and uses a random grouping strategy to decompose the multivariate inverse into univariate components before sampling objectives to generate new decisions \cite{Cheng2015IMMOEA}. Decomposition-based and constrained extensions (IM-MOEA/D and IM-C-MOEA/D) adapt this idea to structured subproblems and constraint handling \cite{Farias2024IMCMOEAD}. For many-objective problems (MaOPs), recent work replaces GP with random-forest (RF) regression and introduces uniform reference points to map all non-dominated points from the Pareto front (PF) to the decision space (PS), reporting gains in both convergence and diversity on standard benchmarks \cite{Gholamnezhad2022}.


A complementary direction uses deep generative models to learn conditional distributions over decisions given target outcomes, enabling the sampling of diverse candidates rather than regressing a single solution. Variational autoencoders and conditional generative adversarial networks capture the one-to-many relation and can produce novel, target-consistent proposals. Representative demonstrations include conditional generation for high-entropy refractory alloys \cite{Debnath2021JMI}, guidance on reversible/periodic encodings for inorganic solids \cite{Chen2021JMI}, encoder–decoder models for photonic and mechanical metamaterials with semi-supervised training \cite{Ma2019AdvMater}, rapid pipelines that translate prescribed mechanical responses into printable architectures \cite{Ha2023NatComm}, and frameworks that generate bulk metallic glasses aligned with target properties \cite{Zhou2023NPJCM}. These studies highlight the appeal of generative inverse modeling—diversity and novelty—while underscoring practical cautions such as representation invertibility, mode collapse, and the need to vet samples with predictive or experimental checks.



Inverse multi-objective optimization (IMOP) addresses a distinct inverse task: given observed (approximate) Pareto‐optimal decisions attributed to a decision maker, infer the parameters $\theta$ of a parametric multiobjective decision–making problem (DMP). A distributionally robust formulation, WRO–IMOP, places a Wasserstein ball around the empirical distribution of observed decisions and minimizes the worst–case expected loss over that ambiguity set. This yields finite–sample excess–risk guarantees with sublinear convergence rates, admits semi–infinite reformulations, and is solved via a cutting–plane algorithm with finite–iteration convergence; demonstrations include a synthetic multiobjective quadratic program and a real portfolio selection task \cite{DongZeng2020ICML,DongZeng2021AAAI}. While IMOP does not map a new target outcome $\mathbf{y}^\ast$ directly to a decision, it informs preference inference and robustness under model misspecification and limited data.


Beyond point regressors, probabilistic and deep generative models address the inherently multi-valued nature of inverse mappings by learning conditional distributions over decisions. This supports multimodality and sample diversity, while tandem forward–inverse training mitigates non-uniqueness by validating each inverse proposal against a forward model. Demonstrations in photonics and materials show that these strategies yield diverse, target-consistent candidates when representations and training are chosen carefully.

In summary, inverse-in-loop EMO leverages inverse models during search but remains coupled to population and iteration budgets; offline inverse surrogates enable interactive, post-hoc queries provided they are paired with forward validation; and probabilistic/generative methods contribute principled diversity at greater training cost. We adopt the offline, interactive path: train an inverse model once, answer real-time inverse queries, and close the loop with feasibility and forward-model checks—bridging optimization-centric algorithms with user-centric decision support. 
