%-------------------------------------------------------------------------------
% Experiments
%-------------------------------------------------------------------------------

\chapter{Experimental Evaluation}\label{chap:experiments}

This chapter presents the empirical findings for two experiments. Results are organised by experiment, reported with descriptive statistics and pre-specified indicators, and shown without interpretation.

\section{bi-objective benchmarks case}\label{sec:exp_coco}


\subsection*{Multi-Objective Optimisation}

A multi-objective optimisation problem (MOP) is defined by a vector of decision variables \(\vect{x}=(x_1,\dots,x_n)\in\calX\subset \R^n\) and a vector of objective functions \(\vect{f}=(f_1,\dots,f_m)\colon \calX\to \calY\subset \R^m\). Without loss of generality we consider all objectives to be minimised. A MOP can be written compactly as

\begin{align} 
\min_{\vect{x}\in\calX}\; \vect{f}(\vect{x}) &= \big(f_1(\vect{x}),\dots,f_m(\vect{x})\big),\\ \text{s.t.}\; g_j(\vect{x}) &\le 0,\quad j=1,\dots,J,\\ h_k(\vect{x}) &= 0,\quad k=1,\dots,K, 
\label{eq:mop}
\end{align}


where \(g_j\) and \(h_k\) denote inequality and equality constraints, respectively. The feasible region \(\calX\) is assumed to be a compact, connected subset of \(\R^n\). Each objective \(f_i\) may be non-convex, nonlinear and expensive to evaluate. Because objectives typically conflict, the goal is a \emph{set of trade-off
designs} rather than a single optimum. A standard partial order uses
\emph{Pareto dominance}: for \(\vect{u},\vect{v}\in\R^m\),
\(\vect{u}\preceq\vect{v}\) iff \(u_i\le v_i\ \forall i\) and \(u_j<v_j\) for
some \(j\). A feasible \(\vect{x}^\star\) is Pareto-optimal if no other
\(\vect{x}\) satisfies \(\vect{f}(\vect{x})\preceq \vect{f}(\vect{x}^\star)\).
See the references above for a concise survey of algorithmic families and
assessment practices.
MOPs arise in a broad spectrum of application domains, including engineering, economics, logistics, manufacturing and science~\cite{Tan2023}. In practice, objective functions often conflict: improving one performance metric deteriorates another. 


\subsection*{Problem and data}
We evaluate inverse mapping on the COCO \emph{bbob-biobj} test suite using the \texttt{cocoex} Python API. We select representative bi-objective problems that include the Linear Slope component from the single-objective suite (bbob $f_5$), such as F61 Ellipsoid/Linear Slope, and additional separable and multimodal combinations (e.g., F62 Rastrigin/B{\"u}che-Rastrigin, F63 Rastrigin/Linear Slope). For each chosen instance, we construct $\mathcal{D}=\{(\mathbf{x}_i,\mathbf{y}_i)\}$ by running a reference multi-objective optimiser (NSGA-II or MOEA/D) and logging decision–outcome pairs from the approximation set \cite{Blank2020, FariasA21}. Data are split into train/validation/test (80/10/10). To quantify data-size effects, we repeat training with $\{100,200,500\}$ training pairs. A forward surrogate $\widehat{\mathbf{f}}$ is trained on the same training split and used only where explicitly stated. Targets for inverse querying are drawn per problem by uniform sampling of 200 points inside the convex hull of training outcomes.

We compare three inverse models—CVAE, MDN, and RBF—tuned by cross-validation. When a model can propose multiple decisions, we sample $k{=}10$ candidates per target (for CVAE/MDN) and compute headline metrics on the top-ranked candidate for consistency, while also logging candidate-set diversity and plausibility flags from the forward check.

\subsection*{Outcome-space discrepancy (primary)}
\textit{Definition.}
\[
  \mathcal{E}_{\mathcal{Y}}(\widehat{\mathbf{x}},\mathbf{y}^{\ast})
  \;=\;
  d\!\big(\mathbf{f}(\widehat{\mathbf{x}}),\,\mathbf{y}^{\ast}\big),
  \qquad
  d(\mathbf{a},\mathbf{b})\in\left\{\ \|\mathbf{a}-\mathbf{b}\|_2,\ \sqrt{(\mathbf{a}-\mathbf{b})^\top\mathbf{\Sigma}^{-1}(\mathbf{a}-\mathbf{b})}\ \right\},
\]
where $\mathbf{\Sigma}$ is the outcome covariance estimated from a calibration split (Mahalanobis). We report median and IQR over targets.

\textit{Findings.} Table~\ref{tab:coco_outcome_error} reports $\mathcal{E}_{\mathcal{Y}}$ (median~[IQR]) across representative problems and training sizes; Figure~\ref{fig:coco_ecdf} shows empirical CDFs of $\mathcal{E}_{\mathcal{Y}}$ over targets.

\begin{table}[t]
  \centering
  \caption{COCO/bbob-biobj: outcome-space error $\mathcal{E}_{\mathcal{Y}}$ (median~[IQR]) across selected problems and training sizes. Best per column in bold.}
  \label{tab:coco_outcome_error}
  \rarray{1.3}
  \begin{tabular}{L{3.8cm}C{2.6cm}C{2.6cm}C{2.6cm}}
    \toprule
    Problem (dim / inst) & CVAE & MDN & INN \\
    \midrule
    F61 Ellipsoid/Linear Slope (100) & \textbf{--} & -- & -- \\
    F61 Ellipsoid/Linear Slope (200) & \textbf{--} & -- & -- \\
    F61 Ellipsoid/Linear Slope (500) & \textbf{--} & -- & -- \\
    \addlinespace
    F62 Rastrigin/B{\"u}che-Rastrigin (100) & -- & \textbf{--} & -- \\
    F62 Rastrigin/B{\"u}che-Rastrigin (200) & -- & \textbf{--} & -- \\
    F62 Rastrigin/B{\"u}che-Rastrigin (500) & -- & \textbf{--} & -- \\
    \addlinespace
    F63 Rastrigin/Linear Slope (100) & -- & -- & \textbf{--} \\
    F63 Rastrigin/Linear Slope (200) & -- & -- & \textbf{--} \\
    F63 Rastrigin/Linear Slope (500) & -- & -- & \textbf{--} \\
    \bottomrule
  \end{tabular}
\end{table}

\placeholderfigure{COCO/bbob-biobj: empirical CDFs of $\mathcal{E}_{\mathcal{Y}}$ per model.}
\label{fig:coco_ecdf}

\subsection*{Tolerance success (primary)}
\textit{Definition.}
\[
  \mathrm{Succ}_\epsilon(\widehat{\mathbf{x}},\mathbf{y}^{\ast})
  \;=\;
  \mathbb{I}\!\left[\, d\!\big(\mathbf{f}(\widehat{\mathbf{x}}),\,\mathbf{y}^{\ast}\big) \le \epsilon \right],
\]
with $\epsilon$ set per benchmark tolerance band (reported with the table).

\textit{Findings.} Table~\ref{tab:coco_success} lists $\mathrm{Succ}_\epsilon$ (median across targets; IQR in parentheses).

\begin{table}[t]
  \centering
  \caption{COCO/bbob-biobj: tolerance success $\mathrm{Succ}_\epsilon$ (median across targets; IQR in parentheses).}
  \label{tab:coco_success}
  \rarray{1.3}
  \begin{tabular}{L{3.8cm}C{2.6cm}C{2.6cm}C{2.6cm}}
    \toprule
    Problem & CVAE & MDN & INN \\
    \midrule
    F61 Ellipsoid/Linear Slope & -- & -- & -- \\
    F62 Rastrigin/B{\"u}che-Rastrigin & -- & -- & -- \\
    F63 Rastrigin/Linear Slope & -- & -- & -- \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection*{Decision-space error (secondary)}
\textit{Definition.}
\[
  \mathrm{RMSE}_{\mathcal{X}}
  \;=\;
  \sqrt{\frac{1}{|\mathcal{T}|\,n}\sum_{i\in\mathcal{T}}\big\|g(\mathbf{y}_i)-\mathbf{x}_i\big\|_2^2},
\]
where $\mathcal{T}$ indexes test pairs and $n=\dim(\mathcal{X})$.

\textit{Findings.} Table~\ref{tab:coco_rmse} summarises $\mathrm{RMSE}_{\mathcal{X}}$ (mean~$\pm$~std over five runs).

\begin{table}[t]
  \centering
  \caption{COCO/bbob-biobj: decision-space RMSE ($\times 10^{-2}$), mean~$\pm$~std over five runs.}
  \label{tab:coco_rmse}
  \rarray{1.3}
  \begin{tabular}{L{3.8cm}C{2.6cm}C{2.6cm}C{2.6cm}}
    \toprule
    Problem & CVAE & MDN & RBF \\
    \midrule
    F61 Ellipsoid/Linear Slope & -- & -- & -- \\
    F62 Rastrigin/B{\"u}che-Rastrigin & -- & -- & -- \\
    F63 Rastrigin/Linear Slope & -- & -- & -- \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection*{Set-quality indicators (secondary)}
\textit{Definition.} For achieved outcomes $S=\{\mathbf{f}(\widehat{\mathbf{x}})\}$ and a high-resolution reference front $P$:
\[
  \mathrm{GD}(S,P)
  \;=\;
  \left(\frac{1}{|S|}\sum_{\mathbf{s}\in S}\min_{\mathbf{p}\in P}\|\mathbf{s}-\mathbf{p}\|_2^p\right)^{\!\!1/p},
  \qquad
  \mathrm{IGD}(S,P)
  \;=\;
  \frac{1}{|P|}\sum_{\mathbf{p}\in P}\min_{\mathbf{s}\in S}\|\mathbf{s}-\mathbf{p}\|_2,
\]
with $p\in\{1,2\}$.

\textit{Findings.} Figure~\ref{fig:coco_gd_igd} presents GD/IGD (means with error bars) versus a reference front.

\placeholderfigure{COCO/bbob-biobj: GD and IGD relative to reference fronts (means with error bars).}
\label{fig:coco_gd_igd}

\subsection*{Data-size sensitivity and cross-problem ranking}
\textit{Definition.} We compare training sizes $\{100,200,500\}$ by aggregating $\mathcal{E}_{\mathcal{Y}}$ across problems; for cross-problem comparisons, we compute average ranks per model and apply the Friedman test with Nemenyi post-hoc at $\alpha=0.05$.

\textit{Findings.} Figure~\ref{fig:coco_size} shows median $\mathcal{E}_{\mathcal{Y}}$ versus size; Figure~\ref{fig:coco_cd} shows the corresponding critical-difference diagram.

\placeholderfigure{COCO/bbob-biobj: median $\mathcal{E}_{\mathcal{Y}}$ by training size (aggregated over problems).}
\label{fig:coco_size}

\placeholderfigure{COCO/bbob-biobj: critical-difference diagram (Friedman + Nemenyi) for $\mathcal{E}_{\mathcal{Y}}$ across problems.}
\label{fig:coco_cd}


\section{Real-world signal-processing case}\label{sec:exp_real}

\subsection*{Protocol}
A two-objective signal-processing dataset with domain constraints was prepared analogously to Experiment~\ref{sec:exp_coco}. Representative decision–outcome pairs $\mathcal{D}$ were assembled from historical configurations and measurements. Data were split into train/validation/test (80/10/10). Targets were sampled within the observed outcome hull; each inverse model produced one decision per target under its selection policy. Models (CVAE, MDN, RBF) were tuned by cross-validation on the training partition. Primary indicators ($\mathcal{E}_{\mathcal{Y}}$, $\mathrm{Succ}_\epsilon$) and secondary indicators (RMSE, GD/IGD) followed the same definitions and reporting conventions.

\subsection*{Findings (signal processing)}
Table~\ref{tab:sp_primary} reports $\mathcal{E}_{\mathcal{Y}}$ and $\mathrm{Succ}_\epsilon$ on held-out targets. Table~\ref{tab:sp_secondary} lists RMSE and GD for completeness. Figure~\ref{fig:sp_scatter} plots $\mathbf{f}(\widehat{\mathbf{x}})$ against targets.

\begin{table}[t]
  \centering
  \caption{Signal-processing case: outcome-space error and tolerance success (median~[IQR]).}
  \label{tab:sp_primary}
  \rarray{1.3}
  \begin{tabular}{L{2.8cm}C{2.6cm}C{2.6cm}}
    \toprule
    Model & $\mathcal{E}_{\mathcal{Y}}$ & $\mathrm{Succ}_\epsilon$ \\
    \midrule
    CVAE  & -- & -- \\
    MDN   & -- & -- \\
    RBF   & -- & -- \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[t]
  \centering
  \caption{Signal-processing case: secondary indicators (RMSE, GD).}
  \label{tab:sp_secondary}
  \rarray{1.3}
  \begin{tabular}{L{2.8cm}C{2.6cm}C{2.6cm}}
    \toprule
    Model & RMSE ($\times 10^{-2}$) & GD ($\times 10^{-2}$)\\
    \midrule
    CVAE & -- & --\\
    MDN  & -- & --\\
    RBF  & -- & --\\
    \bottomrule
  \end{tabular}
\end{table}

\placeholderfigure{Signal-processing case: scatter of forward-evaluated outcomes $\mathbf{f}(\widehat{\mathbf{x}})$ versus targets $\mathbf{y}^{\ast}$.}
\label{fig:sp_scatter}


\section{Supplementary figures and tables}
ECDFs, per-problem success curves, and statistical test tables are provided in Appendix~A.



This chapter reports on experiments conducted to evaluate the proposed
inverse decision mapping framework.  We compare different inverse
modelling strategies on synthetic benchmarks and a real‑world case
study.  Metrics such as root mean squared error (RMSE) on decision
variables, generational distance (GD) and inverted generational distance
(IGD) of reconstructed Pareto sets are used to quantify performance.



\section{Discussion of Results}

Across benchmarks, Gaussian process regression emerges as the most
effective inverse modelling strategy.  Its probabilistic nature allows
uncertainty quantification and smooth interpolation, resulting in low
forward error and generational distance.  Random forests are competitive,
especially when the Pareto front is discontinuous or when training data
is limited, thanks to their ability to capture local patterns and
produce diverse candidates.  Radial basis function networks perform
reasonably well but are sensitive to the choice of centres and require
tuning; they scale poorly with high‑dimensional objectives.  Neural
networks benefit from larger training sets and yield competitive results
once overfitting is controlled.

Data scarcity has a significant impact on inverse model accuracy.
Increasing the number of training samples from 100 to 500 reduces the
RMSE by approximately 40\% across models.  Transfer learning by pooling
  samples from related problems improves performance when tasks share
  similar objective definitions~\cite{Tan2023}.  The plausibility
check effectively warns users when targets lie far from the sampled
Pareto front; in our experiments less than 5\% of queries fell into this
category, highlighting the quality of the data and the models.

Overall, the experiments demonstrate that the proposed framework can
produce high‑quality inverse predictions with modest computational
resources and enables interactive trade‑off exploration.