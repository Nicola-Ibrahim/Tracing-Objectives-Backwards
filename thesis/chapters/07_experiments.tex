%-------------------------------------------------------------------------------
% Experiments
%-------------------------------------------------------------------------------

\chapter{Experiments}\label{chap:experiments}

% This section follows a two-stage evaluation pipeline. First, we tune each inverse-model family with an intra-model ablation study, using outcome-space target matching to identify a best-performing configuration within that family. We then freeze these best-in-class configurations and compare models head-to-head under identical data splits, target sets, and sampling protocol.

\section{bi-objective case}\label{sec:exp_coco}

\subsection{Multi-objective optimization}
A multi-objective optimisation problem (MOP) is defined by decision variables
$\vect{x}=(x_1,\dots,x_n)\in\calX\subset\R^n$ and an objective vector
$\vect{f}=(f_1,\dots,f_m)\colon \calX\to\calY\subset\R^m$. We consider minimization of all objectives.
A general constrained MOP is written as
\begin{align}
\min_{\vect{x}\in\calX}\; \vect{f}(\vect{x}) &= \big(f_1(\vect{x}),\dots,f_m(\vect{x})\big),\\
\text{s.t.}\; g_j(\vect{x}) &\le 0,\quad j=1,\dots,J,\\
h_k(\vect{x}) &= 0,\quad k=1,\dots,K.
\label{eq:mop}
\end{align}
The feasible region $\calX$ is assumed compact. Objectives may be nonlinear, non-convex, and costly to evaluate, which motivates data-driven surrogates and inverse models in later sections.

Trade-offs are formalized by Pareto dominance. For $\vect{u},\vect{v}\in\R^m$,
$\vect{u}\preceq\vect{v}$ holds if $u_i\le v_i$ for all $i$ and $u_j<v_j$ for at least one index $j$.
A feasible point $\vect{x}^\star$ is Pareto-optimal if no feasible $\vect{x}$ satisfies
$\vect{f}(\vect{x})\preceq\vect{f}(\vect{x}^\star)$. The corresponding image in objective space is the Pareto front.
Figure~\ref{fig:pareto_front} illustrates the dominated region and a typical non-dominated approximation set returned by the optimizer.


\begin{figure}[h]
  \centering
  \input{figures/pareto_front}
  \caption{Illustration of Pareto dominance and an example Pareto front (non-dominated set) for a bi-objective problem.}
  \label{fig:pareto_front}
\end{figure}


\subsection{Problem and data}
We generate bi-objective data from the COCO BBOB test functions using the \texttt{cocoex} Python interface
(which provides benchmark suites via \texttt{Suite} and standardized function instances) \cite{cocoexDocs, cocoBBOBOverview}.
We focus on four BBOB functions: $f_5$ (Linear Slope), $f_6$ (Attractive Sector), $f_{10}$ (Ellipsoid), and $f_{20}$ (Schwefel $x\sin(x)$).
These functions cover boundary optima and strong directionality ($f_5$), pronounced asymmetry ($f_6$),
ill-conditioning ($f_{10}$), and highly multi-modal structure ($f_{20}$).

To obtain a two-objective problem from each selected function, we pair two COCO instances of the same analytic form:
\[
\vect{f}(\vect{x})=\big(f^{(\mathrm{inst}=a)}(\vect{x}),\; f^{(\mathrm{inst}=b)}(\vect{x})\big),
\]
with $(a,b)$ fixed per function (e.g., $a=1,b=2$). Instances implement deterministic transformations and shifts while preserving the function family, enabling reproducible variation across runs.

For each function-based bi-objective problem, we run NSGA-II with a fixed evaluation budget and random seeds \cite{deb2002nsga2}.
The optimization objective is to obtain Pareto-relevant data by approximating the Pareto-optimal set and its image in objective space.
At the same time, we log a larger set of decision--outcome pairs than the final non-dominated set by recording evaluations throughout the run.
This produces broad coverage of the outcome region while keeping the optimization signal centered on Pareto performance.

For each selected function, we collect $N=8000$ records $\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^{8000}$, where $\vect{y}_i=\vect{f}(\vect{x}_i)$.
Each record stores a dominance flag indicating whether $\vect{y}_i$ is non-dominated within the final archive.
This supports training on the full logged set and also allows Pareto-only filtering when needed.

Data are split into train/validation/test (80/10/10) using a fixed seed per function.
Decision variables are scaled to a common range using the COCO domain bounds; outcomes are standardised using training-set statistics.
Targets for inverse querying are sampled inside the convex hull of training outcomes (200 targets per function).


\section{Experimental Results and Discussion}
\label{sec:results}

We evaluate the inverse models across two dimensions: first, an intra-model ablation study to identify optimal hyperparameters; second, a cross-model comparison of the best-performing configurations.


\subsection{Hyperparameter Ablation and Model Selection}
\label{subsec:ablation}

Inverse decision mapping is sensitive to model capacity: for MDNs this is governed primarily by the number of mixture components $M$, for CVAEs by the latent dimensionality $z$, and for INNs by the number of coupling layers (and corresponding subnet capacity). To avoid attributing performance differences to under- or over-parameterized settings, we first run an ablation study within each model family and select a single configuration for downstream comparison.

Table~\ref{tab:ablation_results} summarizes the mean outcome-space discrepancy $\mathcal{E}_{\mathcal{Y}}$ across a representative subset of configurations. The overall pattern is that additional capacity can improve target matching up to a saturation point, after which gains diminish and the model may begin to oversmooth the inverse relation. Figure~\ref{fig:ablation_scaling} visualizes this scaling behaviour and helps identify where extra capacity no longer translates into materially better discrepancy.

\begin{table}[h]
    \centering
    \caption{Ablation Study: Mean outcome discrepancy $\mathcal{E}_{\mathcal{Y}}$ for varying configurations. The best-in-class setting per model family is marked with $\star$ and carried forward to the head-to-head comparison.}
    \label{tab:ablation_results}
    \rarray{1.2}
    \begin{tabular}{ll cccc}
        \toprule
        \textbf{Model} & \textbf{Configuration} & \textbf{$f_5$} & \textbf{$f_6$} & \textbf{$f_{10}$} & \textbf{$f_{20}$} \\
        \midrule
        \multirow{3}{*}{CVAE} & $z=2, \text{layers}=3$ & -- & -- & -- & -- \\
                              & $z=8, \text{layers}=4$ ($\star$) & -- & -- & -- & -- \\
                              & $z=16, \text{layers}=4$ & -- & -- & -- & -- \\
        \midrule
        \multirow{4}{*}{MDN}  & $M=1, H=64$ & -- & -- & -- & -- \\
                              & $M=5, H=128$ & -- & -- & -- & -- \\
                              & $M=10, H=256$ ($\star$) & -- & -- & -- & -- \\
                              & $M=20, H=256$ & -- & -- & -- & -- \\
        \midrule
        \multirow{3}{*}{INN}  & $\text{layers}=4, \text{subnet}=64$ & -- & -- & -- & -- \\
                              & $\text{layers}=8, \text{subnet}=128$ ($\star$) & -- & -- & -- & -- \\
                              & $\text{layers}=12, \text{subnet}=256$ & -- & -- & -- & -- \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[h]
    \centering
    \placeholderfigure{Scaling behavior: $\mathcal{E}_{\mathcal{Y}}$ vs. model complexity (e.g., number of mixtures $M$ for MDN, latent dimensionality $z$ for CVAE, coupling layers for INN).}
    \caption{Performance scaling across architectures. The figure highlights where additional capacity continues to improve target matching and where returns begin to diminish.}
    \label{fig:ablation_scaling}
\end{figure}

\subsection{Comparative Performance (Best-in-Class)}
\label{subsec:comparison}

After model selection (Section~\ref{subsec:ablation}), we compare CVAE, MDN, and INN using the same train/validation/test splits and the same set of target outcomes.
Given a target $\vect{y}^\star$, each model proposes $K=10$ candidates $\{\widehat{\vect{x}}^{(k)}\}_{k=1}^{K}$.
Because inverse exploration is inherently one-to-many, we report best-of-$K$ performance: for each target, we evaluate all candidates via the forward map and retain the candidate with the smallest outcome discrepancy.

The primary indicator measures how closely a returned candidate reproduces the desired outcome when mapped forward:
\[
  \mathcal{E}_{\mathcal{Y}}(\widehat{\mathbf{x}},\mathbf{y}^{\ast})
  \;=\;
  d\!\big(\mathbf{f}(\widehat{\mathbf{x}}),\,\mathbf{y}^{\ast}\big),
  \qquad
  d(\mathbf{a},\mathbf{b})\in\left\{\ \|\mathbf{a}-\mathbf{b}\|_2,\ \sqrt{(\mathbf{a}-\mathbf{b})^\top\mathbf{\Sigma}^{-1}(\mathbf{a}-\mathbf{b})}\ \right\},
\]
where $\mathbf{\Sigma}$ is estimated from a calibration split (Mahalanobis variant).
We report median and IQR across targets, which provides a robust summary when the distribution of errors is skewed.

Table~\ref{tab:bbob_outcome_error} reports $\mathcal{E}_{\mathcal{Y}}$ (median~[IQR]) for each COCO function and each best-in-class model.
These results support a like-for-like comparison of outcome-space target matching across different landscape characteristics; in particular, they allow us to inspect whether explicit mixture modelling improves performance on multi-modal tasks and whether invertible architectures remain precise on ill-conditioned objectives.

\begin{table}[h]
  \centering
  \caption{COCO BBOB-derived bi-objective tasks: outcome-space discrepancy $\mathcal{E}_{\mathcal{Y}}$ (median~[IQR]) over 200 targets; best-of-$K$ with $K=10$. Best per row in bold.}
  \label{tab:bbob_outcome_error}
  \rarray{1.25}
  \begin{tabular}{L{3.8cm}C{2.6cm}C{2.6cm}C{2.6cm}}
    \toprule
    Function (two-instance bi-objective) & CVAE & MDN & INN \\
    \midrule
    $f_5$ Linear Slope & -- & -- & -- \\
    $f_6$ Attractive Sector & -- & -- & -- \\
    $f_{10}$ Ellipsoid & -- & -- & -- \\
    $f_{20}$ Schwefel $x\sin(x)$ & -- & -- & -- \\
    \bottomrule
  \end{tabular}
\end{table}




\subsection{Calibration curve and CRPS}
\label{subsec:cal_crps}

Outcome-space discrepancy tells us how closely the best candidate matches the target outcomes, but inverse exploration also depends on the quality of the candidate set as a representation of uncertainty.
Because each inverse model returns $K$ feasible candidates per query, we interpret these candidates as samples from an implicit conditional distribution over decisions given an outcome target.
We therefore evaluate distributional behaviour on held-out test pairs $(\vect{x}_i,\vect{y}_i)$ by conditioning on $\vect{y}_i$ and comparing the sampled candidates against the known ground-truth decision $\vect{x}_i$.
Two complementary questions guide the diagnostics: (i) can the model's uncertainty be trusted (reliability), and (ii) is the predicted distribution useful (close to the truth without being unnecessarily diffuse).

To assess reliability, we use the probability integral transform (PIT).
For each test case $i$ and decision dimension $d$, we compute the PIT value from the candidate samples $\{\widehat{x}^{(k)}_{i,d}\}_{k=1}^{K}$ as
\[
  z_{i,d}
  \;=\;
  \frac{1}{K}\sum_{k=1}^{K}\mathbb{I}\!\left[\widehat{x}^{(k)}_{i,d}\le x_{i,d}\right].
\]
The quantity $z_{i,d}$ is the empirical percentile position of the true component $x_{i,d}$ within the model's sampled candidates for that query.
If the sampled conditional distribution is calibrated component-wise, then the aggregated PIT values are approximately uniform on $[0,1]$.
We visualize this by plotting the empirical CDF of $\{z_{i,d}\}$ against the diagonal $u\mapsto u$.
Systematic departures from the diagonal reveal how uncertainty is distorted: concentration of PIT mass near 0 and 1 indicates overconfidence (samples too tight, truth falls in the tails too often), while PIT mass concentrated near 0.5 indicates underconfidence (samples too wide, truth falls near the centre too often).
To summarize the plot with a single number, we quantify deviation from the diagonal using the calibration error implemented in our pipeline:
\[
  \mathrm{CalErr}
  \;=\;
  \frac{1}{L}\sum_{\ell=1}^{L}\left|z_{(\ell)}-\frac{\ell}{L}\right|,
\]
where $\{z_{(\ell)}\}$ are the sorted PIT values and $L$ is the total number of PIT values aggregated over test cases and dimensions.
Lower values indicate closer agreement with the diagonal and hence more reliable uncertainty.

Reliability alone, however, does not guarantee a practically useful candidate set: a model can be well-calibrated yet overly diffuse.
We therefore also report the continuous ranked probability score (CRPS), a strictly proper scoring rule that evaluates the full predictive distribution and rewards distributions that are both accurate and sharp.
For each test case $i$ and dimension $d$, we compute a sample-based CRPS from the same candidate samples:
\[
  \mathrm{CRPS}_{i,d}
  \;=\;
  \mathbb{E}\,|X-x_{i,d}|
  \;-\;
  \frac{1}{2}\,\mathbb{E}\,|X-X'|,
\]
where $X$ and $X'$ are independent draws from the predictive distribution represented by the candidate samples.
The first term measures how far the samples are, on average, from the truth (distributional accuracy), while the second term penalizes excessive spread by measuring the typical distance between samples (distributional sharpness).
A low CRPS therefore corresponds to candidate sets that concentrate near the ground truth without inflating uncertainty.

Taken together, PIT/CalErr and CRPS separate behaviours that outcome-space error alone can hide.
For example, a model may achieve strong best-of-$K$ target matching yet be systematically overconfident (sharp but miscalibrated), or it may be reliable but too diffuse to support decisive exploration.
Reporting both diagnostics makes these trade-offs explicit and supports the interpretation of the comparative results.

\subsubsection{Same Dataset against Multiple Models}
\label{subsubsec:same_data_multi_models}

We first compare models on the same held-out dataset (fixed function instance and split), so differences in PIT shape, calibration error, and CRPS can be attributed to the inverse model rather than to changes in the data distribution.
Figure~\ref{fig:bbob_calibration_curve} provides the visual reliability check through PIT curves, while Table~\ref{tab:bbob_cal_crps} reports the corresponding scalar summaries (lower is better for both metrics).
In the discussion, we read these results jointly: PIT proximity to the diagonal indicates trustworthy uncertainty, and CRPS indicates whether that uncertainty remains informative rather than overly conservative.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{images/bbob_calibration_curve.png}
    \caption{Calibration curve (PIT empirical CDF vs.\ diagonal) for each model; optionally one panel per function.}
  \label{fig:bbob_calibration_curve}
\end{figure}


\subsubsection{Multiple Dataset against Same Models}
\label{subsubsec:multi_data_same_models}

We next hold the best-in-class models fixed and evaluate them across multiple datasets (here: across COCO functions), keeping the sampling protocol unchanged.
This view matters because uncertainty behaviour can shift with landscape properties such as conditioning, multi-modality, and the effective dimensionality of the inverse mapping.
Stable calibration error and CRPS across functions indicate robust distributional behaviour, whereas large shifts identify where a model becomes systematically overconfident, underconfident, or overly diffuse.
These per-function patterns are then used to contextualize the manual target-level inspection that follows, where users care about both hitting the target and understanding how much to trust the candidate set.

\begin{table}[H]
  \centering
  \caption{Cross-function robustness of decision-space distribution diagnostics. Calibration error (PIT-based) and mean CRPS are reported per COCO function for the same fixed best-in-class models (CVAE, MDN, INN) under an unchanged sampling protocol; lower is better for both metrics.}
  \label{tab:bbob_cal_crps_multi_dataset}
  \begin{adjustbox}{width=\linewidth}
      \rarray{1.25}
      \begin{tabular}{L{3.6cm}C{1.9cm}C{1.9cm}C{1.9cm}C{1.9cm}C{1.9cm}C{1.9cm}}
        \toprule
        & \multicolumn{3}{c}{Calibration error (PIT)} & \multicolumn{3}{c}{CRPS} \\
        \cmidrule(lr){2-4}\cmidrule(lr){5-7}
        COCO function (dataset) & CVAE & MDN & INN & CVAE & MDN & INN \\
        \midrule
        $f_5$ Linear Slope         & -- & -- & -- & -- & -- & -- \\
        $f_6$ Attractive Sector    & -- & -- & -- & -- & -- & -- \\
        $f_{10}$ Ellipsoid         & -- & -- & -- & -- & -- & -- \\
        $f_{20}$ Schwefel          & -- & -- & -- & -- & -- & -- \\
        \bottomrule
      \end{tabular}
  \end{adjustbox}
\end{table}


\subsection{Qualitative Analysis of User Targets}
To contextualize the errors, we list specific target outcomes $\vect{y}^\star$ and the achieved results for a representative function.

\begin{table}[h]
  \centering
  \caption{Target-specific performance for $f_{10}$ (Ellipsoid). Achieved $\vect{f}(\widehat{\vect{x}})$ and Euclidean distance $d$ for $K=10$ samples.}
  \label{tab:bbob_target_table}
  \begin{tabular}{llll}
    \toprule
    Target $\vect{y}^\star$ & CVAE: $\vect{f}(\widehat{\vect{x}})$ / $d$ & MDN: $\vect{f}(\widehat{\vect{x}})$ / $d$ & INN: $\vect{f}(\widehat{\vect{x}})$ / $d$ \\
    \midrule
    $(10.5, 12.0)$ & $(10.6, 11.9)$ / 0.14 & $(10.5, 12.1)$ / 0.10 & $(10.5, 12.0)$ / 0.02 \\
    \bottomrule
  \end{tabular}
\end{table}




% \subsubsection{Tolerance success (primary)}
% Definition.
% \[
%   \mathrm{Succ}_\epsilon(\widehat{\mathbf{x}},\mathbf{y}^{\ast})
%   \;=\;
%   \mathbb{I}\!\left[\, d\!\big(\mathbf{f}(\widehat{\mathbf{x}}),\,\mathbf{y}^{\ast}\big) \le \epsilon \right].
% \]
% The tolerance $\epsilon$ is set per function using a fixed rule (reported in the final table caption).

% Findings.
% Table~\ref{tab:bbob_success} reports $\mathrm{Succ}_\epsilon$ across targets.

% \begin{table}[t]
%   \centering
%   \caption{COCO BBOB-derived bi-objective tasks: tolerance success $\mathrm{Succ}_\epsilon$ (median across targets; IQR in parentheses); best-of-$K$ with $K=10$.}
%   \label{tab:bbob_success}
%   \rarray{1.25}
%   \begin{tabular}{L{3.8cm}C{2.6cm}C{2.6cm}C{2.6cm}}
%     \toprule
%     Function & CVAE & MDN & INN \\
%     \midrule
%     $f_5$ & -- & -- & -- \\
%     $f_6$ & -- & -- & -- \\
%     $f_{10}$ & -- & -- & -- \\
%     $f_{20}$ & -- & -- & -- \\
%     \bottomrule
%   \end{tabular}
% \end{table}



% \section{Real-world signal-processing case}\label{sec:exp_real}

% \subsection{Dataset}
% This experiment uses the dEchorate database, which provides measured multichannel room impulse responses and related recordings together with metadata describing the acoustic scene (room surface configuration, source and microphone geometry, and acquisition conditions) \cite{DiCarlo2021dechorate}. The aim is inverse design from an embedded voice representation to room shape/configuration variables: given a user-selected embedding tensor $\vect{y}^\star$, an inverse model proposes feasible candidates $\widehat{\vect{x}}$ describing the room configuration (decision variables). The forward association needed for evaluation is obtained by pairing each metadata entry with its corresponding tensor record from the HDF5 files using the shared filename key.

% \placeholderfigure{dEchorate overview: measurement setup and an example of a source--receiver geometry under one room configuration.}
% \label{fig:sp_setup}

% \subsection{Data representation and preprocessing}
% The metadata are loaded from the provided CSV table (dEchorate\_database.csv). Each row corresponds to one recording/measurement and includes a filename plus configuration fields such as room surface flags and geometry descriptors (source and microphone positions, array identifiers, and related acquisition attributes). The outcome $\vect{y}$ is a precomputed embedded voice tensor extracted from the corresponding multichannel signal stored in the HDF5 files (e.g., speech recordings), and indexed by filename.

% We construct a paired dataset $\mathcal{D}=\{(\vect{x}_i,\vect{y}_i)\}_{i=1}^{N}$ by retaining rows for which both the selected configuration columns and the tensor embedding are available. Decision variables mix continuous and discrete components; continuous fields are standardised using training-set statistics, and discrete fields are encoded using a fixed scheme (binary for surface flags; integer IDs for categorical labels).

% \begin{table}[t]
%   \centering
%   \caption{Selected decision variables $\vect{x}$ from dEchorate\_database.csv used to represent room shape/configuration. Column selection is fixed across models.}
%   \label{tab:sp_x_cols}
%   \rarray{1.15}
%   \begin{tabular}{L{4.8cm}L{8.2cm}}
%     \toprule
%     CSV column(s) & Role in $\vect{x}$ (room configuration) \\
%     \midrule
%     room\_rfl\_floor, room\_rfl\_ceiling & Surface configuration (binary) \\
%     room\_rfl\_west, room\_rfl\_south, room\_rfl\_east, room\_rfl\_north & Surface configuration (binary) \\
%     room\_code, room\_fornitures & Condition label(s) / context (categorical/binary) \\
%     src\_pos\_x, src\_pos\_y, src\_pos\_z & Source position (continuous) \\
%     mic\_pos\_x, mic\_pos\_y, mic\_pos\_z (or array\_* fields) & Receiver/array geometry (continuous) \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% Data are split into train/validation/test (80/10/10) with a fixed seed. Targets for inverse querying are taken from held-out embeddings: we sample 200 targets per split from the test partition, so each target corresponds to at least one feasible ground-truth configuration in $\mathcal{D}$.

% \subsection{Models and evaluation protocol}
% We compare three inverse estimators for $p(\vect{x}\mid\vect{y})$: CVAE, MDN, and INN \cite{DiCarloEtAl2022dechorateZenodo6576203,Ardizzone2019INN}. For each target embedding $\vect{y}^\star$, each model generates $K$ candidate configurations $\{\widehat{\vect{x}}^{(k)}\}_{k=1}^{K}$ by sampling. All candidates are projected to the feasible domain by construction (encoding and bounds), and any categorical fields are mapped to valid codes.

% \subsubsection{Embedding-space discrepancy (primary)}
% The primary indicator measures how closely a candidate’s associated embedding matches the target:
% \[
% \mathcal{E}_{\mathcal{Y}}(\widehat{\vect{x}},\vect{y}^{\ast})
% \;=\;
% d\!\big(\vect{y}(\widehat{\vect{x}}),\,\vect{y}^{\ast}\big),
% \qquad
% d(\mathbf{a},\mathbf{b})\in\{\|\mathbf{a}-\mathbf{b}\|_2,\; 1-\cos(\mathbf{a},\mathbf{b})\}.
% \]
% We report median and IQR over targets. For multi-candidate models, we report best-of-$K$:
% $\min_{k\le K}\mathcal{E}_{\mathcal{Y}}(\widehat{\vect{x}}^{(k)},\vect{y}^{\ast})$.

% \subsubsection{Tolerance success (primary)}
% Tolerance success records whether at least one candidate falls within an embedding tolerance band:
% \[
% \mathrm{Succ}_\epsilon(\vect{y}^{\ast})
% \;=\;
% \mathbb{I}\!\left[\min_{k\le K} d\!\big(\vect{y}(\widehat{\vect{x}}^{(k)}),\,\vect{y}^{\ast}\big)\le \epsilon\right].
% \]
% The tolerance $\epsilon$ is set once using validation statistics and reused for all models.

% \begin{table}[t]
%   \centering
%   \caption{dEchorate inverse design: embedding-space discrepancy $\mathcal{E}_{\mathcal{Y}}$ and tolerance success $\mathrm{Succ}_\epsilon$ over 200 test targets (median [IQR]); best-of-$K$ with $K=\,$--.}
%   \label{tab:sp_primary_updated}
%   \rarray{1.25}
%   \begin{tabular}{L{3.2cm}C{3.3cm}C{3.3cm}}
%     \toprule
%     Model & $\mathcal{E}_{\mathcal{Y}}$ & $\mathrm{Succ}_\epsilon$ \\
%     \midrule
%     CVAE & -- & -- \\
%     MDN  & -- & -- \\
%     INN  & -- & -- \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \subsubsection{Room-parameter accuracy (secondary)}
% When a unique ground-truth configuration $\vect{x}$ is available for a target, we report decision-space accuracy using component-aware metrics. For continuous geometry fields $\vect{x}_{\mathrm{cont}}$ we report RMSE; for binary/categorical configuration fields $\vect{x}_{\mathrm{disc}}$ we report Hamming error (binary) and accuracy (categorical). For multi-candidate models we compute these on the selected best-of-$K$ candidate.

% \begin{table}[t]
%   \centering
%   \caption{dEchorate inverse design: decision-space errors for geometry and configuration fields on the test set (mean $\pm$ std over runs).}
%   \label{tab:sp_secondary_updated}
%   \rarray{1.25}
%   \begin{tabular}{L{3.2cm}C{3.3cm}C{3.3cm}}
%     \toprule
%     Model & RMSE (continuous) & Discrete error (Hamming / Acc.) \\
%     \midrule
%     CVAE & -- & -- \\
%     MDN  & -- & -- \\
%     INN  & -- & -- \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \subsubsection{Calibration curve and CRPS (secondary)}
% Because CVAE, MDN, and INN return candidate sets that approximate a conditional distribution over $\vect{x}$, we assess probabilistic quality in decision space. For each test pair $(\vect{x}_i,\vect{y}_i)$ and each decision dimension $d$, we compute a probability integral transform (PIT) value as the empirical fraction of sampled candidates not exceeding the true component $x_{i,d}$. Aggregating PIT values across test items yields a calibration curve via the empirical CDF; deviations from the diagonal indicate miscalibration \cite{Dawid1984}.

% We also report the continuous ranked probability score (CRPS), averaged over dimensions, using a sample-based approximation:
% \[
% \mathrm{CRPS} \approx
% \mathbb{E}\,|X-y| \;-\; \tfrac{1}{2}\mathbb{E}\,|X-X'|,
% \]
% where expectations are taken over candidate samples (and an independent copy) for the same target. Lower CRPS indicates better calibrated and sharper predictive distributions \cite{GneitingRaftery2007,Hersbach2000}.

% \placeholderfigure{Calibration curve (PIT CDF): empirical CDF of PIT values versus the uniform diagonal; one curve per inverse model.}
% \label{fig:sp_calibration}

% \placeholderfigure{CRPS comparison: mean CRPS (with variability bars over runs) per inverse model.}
% \label{fig:sp_crps}

% \begin{table}[t]
%   \centering
%   \caption{dEchorate inverse design: calibration error (mean absolute deviation of PIT CDF from the diagonal) and mean CRPS in decision space.}
%   \label{tab:sp_calib_crps}
%   \rarray{1.25}
%   \begin{tabular}{L{3.2cm}C{3.8cm}C{3.8cm}}
%     \toprule
%     Model & Calibration error & CRPS \\
%     \midrule
%     CVAE & -- & -- \\
%     MDN  & -- & -- \\
%     INN  & -- & -- \\
%     \bottomrule
%   \end{tabular}
% \end{table}

% \subsubsection{Figures (placeholders)}
% \placeholderfigure{Embedding-space visualisation: 2D projection (e.g., PCA/UMAP) of $\{\vect{y}_i\}$ with sampled targets $\{\vect{y}^\star\}$.}
% \label{fig:sp_embed_scatter}

% \placeholderfigure{Outcomes of selected candidates: $\vect{y}(\widehat{\vect{x}})$ versus target $\vect{y}^{\ast}$ for each model (one panel per model).}
% \label{fig:sp_targets}



\section{Discussion of Results}

Across benchmarks, Gaussian process regression emerges as the most
effective inverse modelling strategy.  Its probabilistic nature allows
uncertainty quantification and smooth interpolation, resulting in low
forward error and generational distance.  Random forests are competitive,
especially when the Pareto front is discontinuous or when training data
is limited, thanks to their ability to capture local patterns and
produce diverse candidates.  Radial basis function networks perform
reasonably well but are sensitive to the choice of centres and require
tuning; they scale poorly with high‑dimensional objectives.  Neural
networks benefit from larger training sets and yield competitive results
once overfitting is controlled.

Data scarcity has a significant impact on inverse model accuracy.
Increasing the number of training samples from 100 to 500 reduces the
RMSE by approximately 40\% across models.  Transfer learning by pooling
  samples from related problems improves performance when tasks share
  similar objective definitions~\cite{Tan2023}.  The plausibility
check effectively warns users when targets lie far from the sampled
Pareto front; in our experiments less than 5\% of queries fell into this
category, highlighting the quality of the data and the models.

Overall, the experiments demonstrate that the proposed framework can
produce high‑quality inverse predictions with modest computational
resources and enables interactive trade‑off exploration.