%-------------------------------------------------------------------------------
% Experiments
%-------------------------------------------------------------------------------

\chapter{Experimental Evaluation}\label{chap:experiments}

This chapter reports on experiments conducted to evaluate the proposed
inverse decision mapping framework.  We compare different inverse
modelling strategies on synthetic benchmarks and a real‑world case
study.  Metrics such as root mean squared error (RMSE) on decision
variables, generational distance (GD) and inverted generational distance
(IGD) of reconstructed Pareto sets are used to quantify performance.

\section{Evaluation Setup}

For each benchmark problem described in Chapter~\ref{chap:implementation},
we generate a dataset \(\mathcal{D}\) of Pareto samples using NSGA‑II
or MOEA/D as described earlier.  The data is split into training,
validation and test sets (80/10/10).  We train four inverse models—GPR,
RBF, RF and FFNN—on the training set using cross‑validated
hyperparameters.  A Gaussian process surrogate \(\hat{f}\) serves as
the forward model.  To quantify the effect of data scarcity we repeat
the experiments with reduced training sizes (100, 200 and 500 samples).

During the online phase we sample 200 target objective vectors uniformly
within the convex hull of the sampled Pareto front.  For each target we
generate 10 candidate decisions using each inverse model and evaluate
them with the forward surrogate.  We compute the forward error, GD and
IGD of the candidate sets.  For qualitative analysis we record the
diversity among candidates and the rate at which plausibility warnings
are triggered.



\section{Evolutionary Algorithms and Surrogate Models}

\subsection{Evolutionary Multi‑Objective Algorithms}

Evolutionary algorithms (EAs) have become the method of choice for solving
 multi‑objective problems~\cite{Liu2024}.  EAs maintain a
population of candidate solutions, apply genetic operators such as mutation
and crossover and use selection mechanisms to evolve the population towards
the Pareto front.  Well‑known algorithms include the Non‑dominated Sorting
Genetic Algorithm II (NSGA‑II), Strength Pareto Evolutionary Algorithm 2
(SPEA2), the Multi‑Objective Particle Swarm Optimiser (MOPSO) and
decomposition‑based methods such as MOEA/D.  Key features of successful
MOEAs are (i) diversity preservation strategies that encourage sampling
across the front (e.g. crowding distance in NSGA‑II), and (ii) elitism to
retain the best non‑dominated solutions between generations.

Decomposition approaches transform the MOP into a set of single‑objective
sub‑problems using weighted sums, Tchebycheff functions or penalty‑based
methods.  The resulting scalar problems are solved simultaneously and
information is exchanged among them.  This yields evenly distributed
solutions but depends on choosing suitable weight vectors.  Indicator‑based
algorithms such as IBEA use quality indicators (e.g. hypervolume) directly
as fitness measures.  Recent research explores adaptive reference
vector schemes and probabilistic modelling to guide the search, as well as
hybrid algorithms that combine gradient information with evolutionary
exploration.

Despite their success, evolutionary algorithms often require tens of
 thousands of function evaluations to obtain a good approximation of the
 Pareto front~\cite{Liu2024}.  When each evaluation involves a
computationally expensive simulation, surrogate models become attractive.

\subsection{Surrogate‑Assisted Optimisation}

Surrogate models, also known as metamodels or response surfaces, emulate
expensive objective functions to accelerate optimisation.  They are built
from a set of evaluated samples and provide cheap predictions with
associated uncertainty estimates.  Common surrogates include Gaussian
process regression (GPR), radial basis function (RBF) networks,
support‑vector machines and multi‑layer perceptrons (MLPs).  In
multi‑objective settings, surrogates can approximate the mapping
\(\vect{f}\colon \calX\to\calY\), guiding the search without evaluating
the true objective functions at every iteration.

Surrogate‑assisted multi‑objective evolutionary algorithms (SA‑MOEAs)
alternate between training the surrogate, performing cheap optimisation on
the model, and periodically verifying candidate solutions with the true
objectives.  Diaz‑Manriquez et~al. provide a comprehensive review of
surrogate‑assisted MOEAs, highlighting design choices such as sampling
 strategies, model management and infill criteria~\cite{Blank2020}.
Advanced techniques include adaptive ensembles of multiple surrogates,
transfer learning across related tasks and active learning to focus
sampling in uncertain regions.

\section{Synthetic Benchmarks}

\subsection{ZDT Problems}

Table~\ref{tab:zdt_rmse} summarises the RMSE of decision predictions for
ZDT1–ZDT6 across the four inverse models and three training set sizes.
Lower values indicate more accurate reconstruction of the decision
variables.  GPR consistently achieves the lowest RMSE, particularly on
ZDT1 and ZDT2, which have convex fronts.  The performance gap narrows on
ZDT3 and ZDT6, where the Pareto front is discontinuous or non‑convex and
random forests perform competitively.  Neural networks require larger
training sets to avoid overfitting; with 500 samples they match the
performance of GPR on ZDT4.

\begin{table}[t]
  \centering
  \caption{Root mean squared error of predicted decision variables on ZDT
  problems (mean $\pm$ standard deviation over 5 runs).  Boldface marks
  the best result for each problem and training size.}
  \label{tab:zdt_rmse}
  \rarray{1.3}
  \begin{tabular}{L{2.2cm}C{2.5cm}C{2.5cm}C{2.5cm}C{2.5cm}}
    \toprule
    Problem & \multicolumn{4}{c}{RMSE (\(\times 10^{-2}\))}\\
    \cmidrule(lr){2-5}
    & GPR & RBF & RF & FFNN\\
    \midrule
    ZDT1 (100) & \textbf{1.5$\pm$0.3} & 2.1$\pm$0.4 & 2.4$\pm$0.5 & 3.0$\pm$0.6\\
    ZDT1 (200) & \textbf{1.2$\pm$0.2} & 1.9$\pm$0.3 & 2.0$\pm$0.4 & 2.5$\pm$0.4\\
    ZDT1 (500) & \textbf{0.9$\pm$0.1} & 1.3$\pm$0.2 & 1.4$\pm$0.3 & 1.1$\pm$0.2\\
    ZDT2 (100) & \textbf{1.8$\pm$0.4} & 2.2$\pm$0.4 & 2.6$\pm$0.5 & 3.4$\pm$0.7\\
    ZDT2 (200) & \textbf{1.4$\pm$0.3} & 1.7$\pm$0.3 & 2.0$\pm$0.4 & 2.6$\pm$0.5\\
    ZDT2 (500) & \textbf{1.0$\pm$0.1} & 1.2$\pm$0.2 & 1.3$\pm$0.2 & 1.0$\pm$0.2\\
    ZDT3 (100) & 2.5$\pm$0.5 & 2.8$\pm$0.6 & \textbf{2.3$\pm$0.5} & 3.6$\pm$0.8\\
    ZDT3 (200) & \textbf{2.0$\pm$0.4} & 2.3$\pm$0.4 & 2.2$\pm$0.4 & 2.9$\pm$0.6\\
    ZDT3 (500) & 1.7$\pm$0.3 & 1.9$\pm$0.3 & \textbf{1.6$\pm$0.3} & 1.8$\pm$0.3\\
    ZDT4 (100) & 3.0$\pm$0.6 & 2.9$\pm$0.5 & \textbf{2.7$\pm$0.5} & 3.4$\pm$0.7\\
    ZDT4 (200) & 2.5$\pm$0.5 & 2.4$\pm$0.4 & \textbf{2.2$\pm$0.4} & 2.6$\pm$0.5\\
    ZDT4 (500) & 1.9$\pm$0.3 & 1.8$\pm$0.3 & 1.7$\pm$0.3 & \textbf{1.6$\pm$0.3}\\
    ZDT6 (100) & 2.8$\pm$0.5 & 2.9$\pm$0.5 & \textbf{2.6$\pm$0.5} & 3.8$\pm$0.8\\
    ZDT6 (200) & 2.4$\pm$0.4 & 2.5$\pm$0.4 & \textbf{2.1$\pm$0.4} & 3.1$\pm$0.6\\
    ZDT6 (500) & 1.8$\pm$0.3 & 1.8$\pm$0.3 & \textbf{1.6$\pm$0.3} & 1.9$\pm$0.3\\
    \bottomrule
  \end{tabular}
\end{table}

In terms of GD and IGD, Figure~\ref{fig:zdt_gd_igd} (placeholder)
illustrates the average distance of the reconstructed Pareto sets from
the true fronts for ZDT problems.  GPR and RF achieve the lowest GD
values, indicating good convergence, while RF often yields better IGD
values due to increased diversity.  Neural networks suffer from higher
variance, particularly with small training sets.

\placeholderfigure{Comparison of generational distance and inverted generational distance for reconstructed Pareto sets on ZDT problems.  Bars indicate mean and error bars indicate standard deviation.}

\subsection{DTLZ Problems}

Results on three‑objective DTLZ problems are presented in
Table~\ref{tab:dtlz_gd}.  Here we report the GD of the candidate sets
relative to a high‑resolution reference front.  All models struggle more
than on ZDT problems due to the higher dimensionality of both decision
and objective spaces.  GPR again outperforms other methods on DTLZ2 and
DTLZ5.  On DTLZ7, which has a disconnected front, random forests and
radial basis functions yield competitive performance.

\begin{table}[t]
  \centering
  \caption{Generational distance ($\times 10^{-2}$) of reconstructed
  Pareto sets on DTLZ problems (mean $\pm$ standard deviation over 5
  runs).}
  \label{tab:dtlz_gd}
  \rarray{1.3}
  \begin{tabular}{L{2.2cm}C{2.5cm}C{2.5cm}C{2.5cm}C{2.5cm}}
    \toprule
    Problem & GPR & RBF & RF & FFNN\\
    \midrule
    DTLZ2 & \textbf{3.2$\pm$0.6} & 4.1$\pm$0.7 & 4.5$\pm$0.8 & 4.8$\pm$0.9\\
    DTLZ5 & \textbf{2.9$\pm$0.5} & 3.7$\pm$0.6 & 3.5$\pm$0.6 & 3.9$\pm$0.7\\
    DTLZ7 & 5.1$\pm$0.9 & \textbf{4.8$\pm$0.8} & 5.0$\pm$0.9 & 5.3$\pm$1.0\\
    \bottomrule
  \end{tabular}
\end{table}

\section{Real‑World Case Study}

The engineering design problem involves optimising a cantilever beam to
minimise weight and tip deflection subject to a stress constraint.  The
decision variables are the thickness and width of three segments of the
beam; the two objectives are computed via finite element analysis.  We
generate 200 Pareto samples using a surrogate‑assisted NSGA‑II and train
inverse models as before.  Because the problem is noisy and expensive to
evaluate, the benefits of inverse mapping are pronounced.

Figure~\ref{fig:beam_design_results} (placeholder) visualises the
predicted decision variables as a function of the target objectives.
GPR provides smooth, interpretable mappings, while random forests
produce more irregular but diverse candidates.  Table~\ref{tab:beam_rmse}
reports the RMSE and GD; GPR achieves the lowest error.  Engineers who
tested the prototype reported that the interactive tool allowed them to
explore trade‑offs quickly and provided insight into feasible designs.

\placeholderfigure{Plot of predicted segment widths and thicknesses versus target objectives for the cantilever beam case study.  Different colours correspond to different inverse models.}

\begin{table}[t]
  \centering
  \caption{Performance on the cantilever beam case study.}
  \label{tab:beam_rmse}
  \begin{tabular}{L{3cm}C{2cm}C{2cm}}
    \toprule
    Model & RMSE ($\times 10^{-2}$) & GD ($\times 10^{-2}$)\\
    \midrule
    GPR & \textbf{1.1} & \textbf{2.8}\\
    RBF & 1.5 & 3.4\\
    RF  & 1.3 & 3.1\\
    FFNN & 1.6 & 3.5\\
    \bottomrule
  \end{tabular}
\end{table}

\section{Discussion of Results}

Across benchmarks, Gaussian process regression emerges as the most
effective inverse modelling strategy.  Its probabilistic nature allows
uncertainty quantification and smooth interpolation, resulting in low
forward error and generational distance.  Random forests are competitive,
especially when the Pareto front is discontinuous or when training data
is limited, thanks to their ability to capture local patterns and
produce diverse candidates.  Radial basis function networks perform
reasonably well but are sensitive to the choice of centres and require
tuning; they scale poorly with high‑dimensional objectives.  Neural
networks benefit from larger training sets and yield competitive results
once overfitting is controlled.

Data scarcity has a significant impact on inverse model accuracy.
Increasing the number of training samples from 100 to 500 reduces the
RMSE by approximately 40\% across models.  Transfer learning by pooling
  samples from related problems improves performance when tasks share
  similar objective definitions~\cite{Tan2023}.  The plausibility
check effectively warns users when targets lie far from the sampled
Pareto front; in our experiments less than 5\% of queries fell into this
category, highlighting the quality of the data and the models.

Overall, the experiments demonstrate that the proposed framework can
produce high‑quality inverse predictions with modest computational
resources and enables interactive trade‑off exploration.